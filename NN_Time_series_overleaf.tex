\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, amsthm, mathtools}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}

% ---------- basic macros ----------
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\KL}{\mathrm{KL}}
\newcommand{\Cat}{\mathrm{Cat}}
\newcommand{\Normal}{\mathcal{N}}
\newcommand{\Ind}{1_{\{\cdot\}}}

\DeclareMathOperator{\softmax}{softmax}
\DeclareMathOperator{\LRU}{LRU}
\DeclareMathOperator{\TTL}{TTL}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\Gam}{Gamma} % distribution shorthand

% ---------- theorem envs ----------
\theoremstyle{plain}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\theoremstyle{remark}
\newtheorem{remark}{Remark}

\title{A Neural, Uncertainty-Aware, Similarity-Exploiting Router for Dynamic Expert Sets\\
(Neural Replacement of Switching State-Space Routing)}
\author{}
\date{}

\begin{document}
\maketitle

\begin{abstract}
We propose a neural strategy that retains the desirable features of switching state-space model (SSM) routers---non-stationarity via regimes, information transfer across experts, principled uncertainty for exploration/risk-aware routing, expert birth/staleness handling, and dynamic availability---while improving scalability and expressivity. The approach is a switching latent-variable sequence model with amortized filtering, ensemble-based uncertainty decomposition, and a sparse similarity-driven broadcast update. We specialize the observation model to the case where the router observes \emph{mean squared error} (MSE): the loss is nonnegative and often skewed, so we use a positive likelihood (Gamma / scaled-$\chi^2$) rather than a Gaussian. We state all variables and assumptions explicitly and provide short proofs for (i) the Gamma MSE likelihood under Gaussian residuals, (ii) the masked-ELBO objective under bandit feedback, and (iii) the score-based innovation signal used for broadcast.
\end{abstract}

\tableofcontents

% =========================================================
\section{Notation and Objects (Complete Definitions)}
\label{sec:notation}

Time index: $t\in\{1,\dots,T\}$.

\paragraph{Observed exogenous signals.}
\begin{itemize}
  \item Context $x_t\in\mathcal{X}$.
  \item Registry (all experts that exist at time $t$): $U_t$.
  \item Available set (experts that can be consulted at time $t$): $K_t\subseteq U_t$.
\end{itemize}

\paragraph{Decision and feedback (bandit).}
The router chooses an expert index $r_t\in K_t$ and only observes that expert's loss $\ell_{r_t,t}$.

Each expert $j$ has consultation cost $\beta_j\ge 0$ and we incur
\begin{equation}
C_t(r_t) = \ell_{r_t,t} + \beta_{r_t}.
\end{equation}
Goal: minimize $\E[\sum_{t=1}^T C_t(r_t)]$.

\paragraph{History (information state).}
\begin{equation}
H_t := \big(x_{1:t},K_{1:t},r_{1:t-1},\ell_{r_{1:t-1},1:t-1}\big).
\end{equation}

\paragraph{Trainable representations.}
We learn a feature map (encoder) $\phi_\eta:\mathcal{X}\to\R^{d_x}$ and write
\[
\phi_t := \phi_\eta(x_t).
\]
Unless stated otherwise, $\eta$ is learned end-to-end because the training objective depends on $\phi_t$ through the predictive distribution of $\ell_{r_t,t}$.

\paragraph{Expert embeddings.}
Each expert $j$ has metadata $\mathrm{meta}_j$ and a stable identifier $j$; we define an embedding
\[
e_j := \mathrm{Enc}_\omega(\mathrm{meta}_j,\mathrm{hash}(j)) \in \R^{d_e},
\]
with $\omega$ learned.

\paragraph{Bounded registry cache.}
We maintain a cache $\mathcal{M}_t\subseteq U_t$ with $|\mathcal{M}_t|\le N_{\max}$ containing for each cached expert $j$:
\[
(e_j,\ v_{j,t},\ n_{j,t},\ \mathrm{lastSeen}_{j,t},\ \beta_j),
\]
where (i) $v_{j,t}\in\R^{d_v}$ is a learnable \emph{expert memory state}, (ii) $n_{j,t}$ is the number of times $j$ has been chosen up to $t$, and (iii) $\mathrm{lastSeen}_{j,t}$ is the last time $j$ was in $K_{\tau}$ for $\tau\le t$. Eviction is by $\TTL$ or $\LRU$ and is an implementation constraint (not a probabilistic assumption).

\paragraph{Latent variables (non-stationarity).}
We use:
\begin{itemize}
  \item regime $z_t \in \{1,\dots,M\}$ (discrete),
  \item global factor $g_t \in \R^{d_g}$ (continuous).
\end{itemize}

\paragraph{Deterministic belief state.}
We maintain a deterministic summary $h_t\in\R^{d_h}$ computed by a temporal encoder:
\begin{equation}
h_t = f_\psi\!\left(h_{t-1},\, \phi_t,\, \bar{s}_t,\, e_{r_{t-1}},\, \ell_{r_{t-1},t-1}\right),
\label{eq:temprec}
\end{equation}
where $\bar{s}_t$ is a permutation-invariant encoding of the available set (defined in Section~\ref{sec:inference}). Parameters $\psi$ are learned.

% =========================================================
\section{Problem Setting and Challenges}
At each $t$: observe $(x_t,K_t)$, choose $r_t\in K_t$, observe $\ell_{r_t,t}$ only.

Key challenges:
(i) non-stationarity (regime shifts), (ii) censored observations (bandit feedback), (iii) variable-size $K_t$ and open-world experts, (iv) exploiting similarity: observing one expert should update predictions for correlated experts.

% =========================================================
\section{Model: Switching Latent Router with MSE Likelihood}
\label{sec:model}

We specify a conditional generative model for the observed loss sequence \emph{given the exogenous process} $(x_{1:T},K_{1:T})$ and actions $(r_{1:T})$.

\subsection{Prior dynamics for $(z_t,g_t)$}
We use a switching prior conditioned on the deterministic belief state:
\begin{align}
p_\theta(z_t \mid z_{t-1}, h_{t-1}) &= \Cat\!\big(\pi_\theta(z_{t-1},h_{t-1})\big), \label{eq:prior_z}\\
p_\theta(g_t \mid z_t, h_{t-1}) &= \Normal\!\big(\mu_\theta(z_t,h_{t-1}),\ \Sigma_\theta(z_t,h_{t-1})\big), \label{eq:prior_g}
\end{align}
where $\Sigma_\theta$ is constrained positive definite (diagonal or low-rank).

\subsection{Observation model for MSE (Gamma / scaled-$\chi^2$)}
\label{sec:mse}

The router observes an MSE for the chosen expert. We model
\begin{equation}
\ell_{j,t} = \frac{1}{d}\|\varepsilon_{j,t}\|_2^2,\qquad \varepsilon_{j,t}\in\R^d.
\label{eq:mse_def}
\end{equation}
We predict a conditional mean $\mu_{j,t}>0$ via a neural head:
\begin{equation}
\mu_{j,t} = \mu_\theta\big(g_t,h_t,e_j,\mathrm{stat}_{j,t},v_{j,t}\big),\qquad \mu_{j,t}>0,
\label{eq:mean_head}
\end{equation}
where $\mathrm{stat}_{j,t}$ can include $(n_{j,t}, t-\mathrm{lastSeen}_{j,t})$ and other cheap features.

We then define a Gamma likelihood with shape $k>0$ and scale $\theta_{j,t}=\mu_{j,t}/k$:
\begin{equation}
p_\theta(\ell_{j,t}\mid g_t,h_t,e_j,\mathrm{stat}_{j,t},v_{j,t})
=
\Gam\!\left(k,\ \theta_{j,t}=\frac{\mu_{j,t}}{k}\right).
\label{eq:gamma_like}
\end{equation}
This implies $\E[\ell_{j,t}\mid\cdot]=\mu_{j,t}$ and $\Var(\ell_{j,t}\mid\cdot)=\mu_{j,t}^2/k$.
In the isotropic Gaussian residual case, one can set $k=d/2$ (Proposition~\ref{prop:gamma}).

\begin{proposition}[MSE under isotropic Gaussian residuals is Gamma]
\label{prop:gamma}
Assume $\varepsilon_{j,t}\mid(\cdot)\sim\Normal(0,\sigma^2 I_d)$. Then $\ell_{j,t}$ in \eqref{eq:mse_def} satisfies
\[
\ell_{j,t}\mid(\cdot)\sim \Gam\!\left(k=\frac d2,\ \theta=\frac{2\sigma^2}{d}\right)
\quad\text{and}\quad
\E[\ell_{j,t}\mid(\cdot)]=\sigma^2.
\]
\end{proposition}
\begin{proof}
If $\varepsilon\sim\Normal(0,\sigma^2 I_d)$, then $\|\varepsilon\|_2^2/\sigma^2\sim\chi^2_d$.
Hence $\|\varepsilon\|_2^2\sim \sigma^2 \chi^2_d$ and
\[
\ell=\frac1d\|\varepsilon\|_2^2 \sim \frac{\sigma^2}{d}\chi^2_d.
\]
Since $\chi^2_d$ is Gamma with shape $d/2$ and scale $2$, scaling by $\sigma^2/d$ yields a Gamma with shape $d/2$ and scale $2\sigma^2/d$.
\end{proof}

\remark
If isotropy is questionable, keep the Gamma mean parametrization \eqref{eq:gamma_like} but treat $k$ as a learned (or expert-dependent) positive output, ensuring $\mu_{j,t}^2/k$ captures heteroscedasticity.

% =========================================================
\section{Amortized Inference (Online Filtering)}
\label{sec:inference}

We define a permutation-invariant summary of the available set:
\begin{equation}
\bar{s}_t = \mathrm{SetEnc}\Big(\{\, e_j \oplus \mathrm{stat}_{j,t} \oplus v_{j,t} : j\in K_t \cap \mathcal{M}_t \,\}\Big)\in\R^{d_s},
\end{equation}
where $\mathrm{SetEnc}$ is any permutation-invariant function (e.g., DeepSets:
$\rho(\sum_{j\in K_t}\varphi(\cdot))$; or a Set Transformer).

Using \eqref{eq:temprec} we obtain $h_t$, and define a variational posterior:
\begin{equation}
q_\phi(z_t,g_t\mid H_t)=q_\phi(z_t\mid h_t)\,q_\phi(g_t\mid z_t,h_t),
\label{eq:q}
\end{equation}
with
\[
q_\phi(z_t\mid h_t)=\Cat(\alpha_\phi(h_t)),
\qquad
q_\phi(g_t\mid z_t,h_t)=\Normal(\nu_\phi(z_t,h_t),\Lambda_\phi(z_t,h_t)).
\]
This is standard amortized filtering: $h_t$ summarizes $H_t$, and the posterior heads map $h_t$ to parameters.

% =========================================================
\section{Learning Objective Under Bandit Feedback (Masked ELBO)}
\label{sec:elbo}

Only $\ell_{r_t,t}$ is observed. Let $\ell^{\mathrm{obs}}_t:=\ell_{r_t,t}$.

\begin{lemma}[Masked ELBO is a valid lower bound]
\label{lem:elbo}
Under the model in Section~\ref{sec:model}, for any variational distribution $q_\phi(z_t,g_t\mid H_t)$,
\begin{align}
\log p_\theta(\ell^{\mathrm{obs}}_{1:T}\mid x_{1:T},K_{1:T},r_{1:T})
\;\ge\;
\sum_{t=1}^T
\E_{q_\phi(z_t,g_t\mid H_t)}\!\Big[\log p_\theta(\ell^{\mathrm{obs}}_{t}\mid z_t,g_t,h_t,r_t)\Big]
-
\sum_{t=1}^T
\KL\!\Big(q_\phi(z_t,g_t\mid H_t)\,\big\|\,p_\theta(z_t,g_t\mid z_{t-1},h_{t-1})\Big),
\label{eq:elbo}
\end{align}
where the likelihood term uses only the observed loss for the selected expert.
\end{lemma}
\begin{proof}
This is the standard variational bound applied to the latent sequence $(z_{1:T},g_{1:T})$ with observations restricted to the actually observed emissions $\ell^{\mathrm{obs}}_t$.
At each $t$, Jensen's inequality yields
\[
\log p(\ell^{\mathrm{obs}}_t\mid\cdot)\ge
\E_q[\log p(\ell^{\mathrm{obs}}_t\mid\cdot)]-\KL(q\|p),
\]
and summing over $t$ gives \eqref{eq:elbo}. The masking is not a modification: the generative model simply defines an emission density for any $(t,j)$, but the dataset contains only the selected emissions.
\end{proof}

\remark
The bound is on the likelihood of observed losses \emph{conditional on the action sequence} $r_{1:T}$. If training from logged data produced by a different policy, selection bias appears; see Section~\ref{sec:offpolicy}.

% =========================================================
\section{Similarity Exploitation: Sparse Graph Broadcast as Approximate Inference}
\label{sec:graph}

\subsection{Graph on available experts}
At time $t$ we build a sparse graph on $K_t$:
\[
G_t=(K_t,E_t),\qquad w_{ij,t}\ge 0,\ (i,j)\in E_t,
\]
e.g., $E_t$ from top-$k$ nearest neighbors in embedding space, so $|E_t|=O(|K_t|k)$.

\subsection{A justified innovation signal (score of the Gamma likelihood)}
We want an innovation $\delta_t$ that is (i) centered under the model and (ii) directionally aligned with ``surprise''.

For Gamma with fixed shape $k$ and mean $\mu_{r_t,t}$, define the \emph{score} with respect to $\log \mu_{r_t,t}$:
\begin{equation}
\delta_t
:=\left.\frac{\partial}{\partial \log \mu}\log p(\ell^{\mathrm{obs}}_t\mid \mu)\right|_{\mu=\mu_{r_t,t}}
= k\left(\frac{\ell^{\mathrm{obs}}_t}{\mu_{r_t,t}}-1\right).
\label{eq:score_delta}
\end{equation}

\begin{proposition}[Centering of the score]
\label{prop:score_centered}
If $\ell^{\mathrm{obs}}_t\mid\mu_{r_t,t}\sim \Gam(k,\mu_{r_t,t}/k)$ then $\E[\delta_t\mid \mu_{r_t,t}]=0$.
\end{proposition}
\begin{proof}
Under the Gamma mean parametrization, $\E[\ell^{\mathrm{obs}}_t\mid\mu]=\mu$. Plugging into \eqref{eq:score_delta} gives $\E[\delta_t\mid\mu]=k(\mu/\mu-1)=0$.
\end{proof}

\remark
The log-innovation in your previous draft can still be used, but \eqref{eq:score_delta} is strictly more justified: it is the exact likelihood score for the chosen positive MSE model.

\subsection{Deterministic memory updates (chosen + broadcast)}
We update \emph{expert memory states} $v_{j,t}$ deterministically (these are part of the inference machinery and enter the mean head \eqref{eq:mean_head}).

\paragraph{Local update (chosen expert).}
\begin{equation}
v_{r_t,t+1} = v_{r_t,t} + \Delta_{\psi}\big(v_{r_t,t},h_t,e_{r_t},\mathrm{stat}_{r_t,t},\delta_t\big).
\end{equation}

\paragraph{Broadcast update (neighbors).}
For $j\in K_t\setminus\{r_t\}$:
\begin{equation}
v_{j,t+1} = v_{j,t} + w_{j r_t,t}\cdot \Gamma_\psi\big(h_t,e_j,e_{r_t}\big)\,\delta_t,
\label{eq:broadcast}
\end{equation}
where $\Gamma_\psi(\cdot)$ is a learned gain (scalar or low-rank) and is gated by confidence (e.g., suppress if $\mu_{r_t,t}$ is highly uncertain).

\paragraph{Justification.}
In linear-Gaussian latent residual models, the exact posterior mean update has the form ``innovation $\times$ gain'' (Kalman conditioning). Here we use the same algebraic structure but (i) sparsify the dependency using $G_t$ and (ii) learn the gain for a non-Gaussian MSE emission. This is best interpreted as \emph{amortized message passing} rather than an exact posterior update outside the linear-Gaussian special case.

% =========================================================
\section{Uncertainty: Aleatoric, Epistemic, and Staleness}
\label{sec:uncertainty}

\subsection{Gamma moments (aleatoric)}
Under \eqref{eq:gamma_like}, conditional variance is $\mu_{j,t}^2/k$. If $k$ is learned or expert-dependent, this directly expresses heteroscedasticity.

\subsection{Ensembles (epistemic)}
Use an ensemble of $K$ independently initialized models indexed by $a=1,\dots,K$. Each outputs $\mu^{(a)}_{j,t}$ and (if applicable) $k^{(a)}_{j,t}$, hence variance
\[
\sigma^{2,(a)}_{j,t} =
\frac{\left(\mu^{(a)}_{j,t}\right)^2}{k^{(a)}_{j,t}}.
\]
Define
\begin{align}
\bar{\mu}_{j,t} &= \frac{1}{K}\sum_{a=1}^K \mu^{(a)}_{j,t},\\
\widehat{\sigma}^2_{j,t}
&=
\underbrace{\frac{1}{K}\sum_{a=1}^K \sigma^{2,(a)}_{j,t}}_{\text{aleatoric}}
+
\underbrace{\frac{1}{K}\sum_{a=1}^K \Big(\mu^{(a)}_{j,t}-\bar{\mu}_{j,t}\Big)^2}_{\text{epistemic}}.
\end{align}

\subsection{Staleness/newness inflation}
Because experts can be new or rarely selected, we inflate uncertainty:
\begin{equation}
\widetilde{\sigma}^2_{j,t}
=
\widehat{\sigma}^2_{j,t}
+
\frac{c_1}{\sqrt{n_{j,t}+1}} + c_2\,(t-\mathrm{lastSeen}_{j,t}),
\end{equation}
with $c_1,c_2\ge 0$. This is a conservative heuristic (not a probabilistic identity) that corrects for unobserved drift and low-data experts.

% =========================================================
\section{Routing (Decision-Theoretic + Exploration)}
\label{sec:routing}

Risk-neutral myopic Bayes decision:
\[
r_t \in \arg\min_{j\in K_t}\ \bar{\mu}_{j,t} + \beta_j.
\]

Risk-aware (mean--std conservative proxy):
\begin{equation}
r_t \in \arg\min_{j\in K_t}\ \bar{\mu}_{j,t} + \beta_j + \lambda_t\,\widetilde{\sigma}_{j,t},
\label{eq:route_rule}
\end{equation}
where $\lambda_t\ge 0$ can be fixed or learned.

\paragraph{Exploration is necessary.}
Under bandit feedback, without exploration the model may stop observing certain experts, making their predictions unidentifiable. A principled approach is Thompson sampling over the ensemble:
\[
a\sim \mathrm{Unif}(\{1,\dots,K\}),\qquad
r_t = \arg\min_{j\in K_t}\ \mu^{(a)}_{j,t} + \beta_j.
\]

% =========================================================
\section{Off-Policy / Logged Data Caveat}
\label{sec:offpolicy}

If training from logs generated by a behavior policy $b(r_t\mid H_t)$ different from the current policy, then naive maximization of \eqref{eq:elbo} is biased because the training distribution over $(H_t,r_t)$ is policy-dependent.

A standard fix is inverse propensity weighting (IPS) for the likelihood term:
\[
\E\Big[\log p_\theta(\ell^{\mathrm{obs}}_t\mid\cdot)\Big]
\quad\leadsto\quad
\E\Big[\frac{\Ind\{r_t=j\}}{b(j\mid H_t)}\log p_\theta(\ell_{j,t}\mid\cdot)\Big],
\]
or a doubly-robust variant. In online training, using exploration (TS/UCB) and training on-policy is typically the simplest.

% =========================================================
\section{Online Algorithm (Fully Specified)}
\begin{algorithm}[h]
\caption{Switching Neural Latent Router with Gamma-MSE Likelihood and Graph Broadcast}
\begin{algorithmic}[1]
\State Initialize ensemble parameters $\{(\theta_a,\phi_a,\eta_a,\omega_a,\psi_a)\}_{a=1}^K$
\State Initialize cache $\mathcal{M}_1$ (empty or with known experts) with $|\mathcal{M}_1|\le N_{\max}$
\For{$t=1,2,\dots$}
  \State Observe $(x_t,K_t)$; compute $\phi_t=\phi_\eta(x_t)$
  \State Build $\bar{s}_t=\mathrm{SetEnc}(\{e_j\oplus \mathrm{stat}_{j,t}\oplus v_{j,t} : j\in K_t\cap \mathcal{M}_t\})$
  \For{$a=1,\dots,K$}
    \State Update $h_t^{(a)}$ via \eqref{eq:temprec}
    \State Compute $q_{\phi_a}(z_t,g_t\mid H_t)$ via \eqref{eq:q} and predictive $\mu^{(a)}_{j,t}$ for all $j\in K_t$
  \EndFor
  \State Aggregate $(\bar{\mu}_{j,t},\widetilde{\sigma}_{j,t})$ and select $r_t$ via \eqref{eq:route_rule} (or Thompson sampling)
  \State Query $r_t$ and observe $\ell^{\mathrm{obs}}_t=\ell_{r_t,t}$
  \State Compute innovation $\delta_t$ by the Gamma score \eqref{eq:score_delta} (per ensemble member or using $\bar{\mu}_{r_t,t}$)
  \State Update memory: local update for $v_{r_t,t}$; broadcast to neighbors using \eqref{eq:broadcast}
  \State Update counters $n_{r_t,t+1}\leftarrow n_{r_t,t}+1$, $\mathrm{lastSeen}_{j,t+1}\leftarrow t$ for $j\in K_t$
  \State Cache maintenance: add births; evict by $\TTL$/$\LRU$ to keep $|\mathcal{M}_{t+1}|\le N_{\max}$
  \State SGD step(s) on \eqref{eq:elbo} using the observed $(H_t,r_t,\ell^{\mathrm{obs}}_t)$ for each ensemble member
\EndFor
\end{algorithmic}
\end{algorithm}

% =========================================================
\section{Complexity}
Let $|K_t|$ be the availability size and $|E_t|$ the sparse graph edges (e.g., $O(|K_t|k)$ for $k$-NN).
Per-step cost is
\[
O(|K_t|\,d) \;+\; O(|E_t|\,d) \;+\; O(d^2),
\]
where $d$ is a representative hidden dimension. Crucially, runtime depends on $|K_t|$ and $N_{\max}$, not on the full registry size $|U_t|$.

% =========================================================
\section{What Is Exact vs Approximate (No Over-Claiming)}
\begin{itemize}
  \item The Gamma MSE likelihood is \textbf{exact} under isotropic Gaussian residuals (Proposition~\ref{prop:gamma}); otherwise it is a \textbf{well-posed positive} parametric model.
  \item The masked ELBO \eqref{eq:elbo} is a \textbf{valid} lower bound on the conditional log-likelihood of observed losses (Lemma~\ref{lem:elbo}).
  \item The broadcast update \eqref{eq:broadcast} is \textbf{exact} in linear-Gaussian special cases (innovation$\times$gain) and otherwise a \textbf{controlled amortized inference heuristic} designed to exploit similarity under censoring.
  \item Learning $\phi_\eta(x_t)$ is \textbf{end-to-end} by default, but is policy-dependent under bandit feedback (hence exploration or IPS is required for unbiasedness under logging).
\end{itemize}

% =========================================================
\section{Full-Feedback Setting (All Expert Losses Observed)}
\label{sec:fullfeedback}

So far we assumed \emph{partial feedback} (bandit): at time $t$ we only observe $\ell_{r_t,t}$ for the chosen expert.
In some applications, however, one may log the losses of \emph{all} available experts, i.e.,
\[
\{\ell_{j,t} : j\in K_t\}\quad \text{is observed at time }t.
\]
We call this the \emph{full-feedback} setting. This section states precisely what changes in the model, training objective, and routing.

\subsection{Observation model and likelihood factorization}

The conditional emission model in Section~\ref{sec:mse} defines a density for every $(t,j)$:
\[
p_\theta(\ell_{j,t}\mid g_t,h_t,e_j,\mathrm{stat}_{j,t},v_{j,t})
=
\Gam\!\left(k,\theta_{j,t}=\frac{\mu_{j,t}}{k}\right),
\qquad \mu_{j,t}>0.
\]
Under full feedback, we observe the vector of losses
\[
\ell_{K_t,t}:=\{\ell_{j,t}\}_{j\in K_t}.
\]

\paragraph{Conditional independence assumption (standard).}
A natural assumption---and the one implicitly used in the bandit derivations---is that \emph{conditional on the shared latents $(z_t,g_t)$ and the expert states}, the losses of different experts are independent:
\begin{equation}
p_\theta(\ell_{K_t,t}\mid z_t,g_t,h_t,K_t)
=
\prod_{j\in K_t} p_\theta(\ell_{j,t}\mid g_t,h_t,e_j,\mathrm{stat}_{j,t},v_{j,t}).
\label{eq:full_like_factor}
\end{equation}
This assumption is exact if all cross-expert dependence is mediated through $(z_t,g_t)$ and the expert states $(v_{j,t})_{j\in K_t}$. If there remain residual correlations beyond these latents, \eqref{eq:full_like_factor} is an approximation; the graph/broadcast update in Section~\ref{sec:graph} partially compensates for this in practice.

\subsection{Full-feedback ELBO (no masking)}

Define the observed data under full feedback as
\[
\ell^{\mathrm{obs}}_t := \ell_{K_t,t}=\{\ell_{j,t}\}_{j\in K_t}.
\]
Then the same variational argument as Lemma~\ref{lem:elbo} yields the \emph{unmasked} ELBO:
\begin{align}
\log p_\theta(\ell^{\mathrm{obs}}_{1:T}\mid x_{1:T},K_{1:T})
\;\ge\;
\sum_{t=1}^T
\E_{q_\phi(z_t,g_t\mid H_t)}\Bigg[
\sum_{j\in K_t}\log p_\theta(\ell_{j,t}\mid g_t,h_t,e_j,\mathrm{stat}_{j,t},v_{j,t})
\Bigg]
-
\sum_{t=1}^T
\KL\!\Big(q_\phi(z_t,g_t\mid H_t)\,\big\|\,p_\theta(z_t,g_t\mid z_{t-1},h_{t-1})\Big).
\label{eq:elbo_full}
\end{align}

\paragraph{Interpretation.}
Compared to \eqref{eq:elbo}, full feedback simply replaces the single log-likelihood term for $r_t$ by a sum over all $j\in K_t$.
As a consequence:
\begin{itemize}
  \item learning is \emph{supervised} over the entire available set each round;
  \item selection bias from the routing policy disappears (no IPS is needed);
  \item exploration is no longer required for identifiability (though it can still be useful for other goals like robustness).
\end{itemize}

\subsection{Routing becomes a pure decision problem}

When all losses are observed, the routing decision $r_t$ no longer affects the training signal at time $t$ (the model sees all losses anyway). Thus $r_t$ can be chosen purely to minimize expected cost:
\[
r_t \in \arg\min_{j\in K_t}\ \bar{\mu}_{j,t}+\beta_j,
\]
or with risk control as in \eqref{eq:route_rule}. There is no exploration--exploitation tradeoff induced by feedback censoring.

\subsection{What happens to similarity broadcast?}

Under full feedback, one can compute \emph{exact per-expert innovations} for all $j\in K_t$:
\[
\delta_{j,t}
:=
\left.\frac{\partial}{\partial \log \mu}\log p(\ell_{j,t}\mid \mu)\right|_{\mu=\mu_{j,t}}
= k\left(\frac{\ell_{j,t}}{\mu_{j,t}}-1\right).
\]
Then the memory update can be done directly for each expert:
\[
v_{j,t+1} = v_{j,t} + \Delta_\psi(v_{j,t},h_t,e_j,\mathrm{stat}_{j,t},\delta_{j,t}),\qquad \forall j\in K_t.
\]
In this regime, broadcast \eqref{eq:broadcast} is \emph{not needed} to propagate information to unobserved experts, since no expert is unobserved within $K_t$. However, it can still be used in two situations:
\begin{itemize}
  \item \textbf{Propagation to non-available experts:} experts in the registry but not in $K_t$ at time $t$ still have missing losses; broadcast can propagate signals through a persistent graph on the full registry (or through cached neighbors).
  \item \textbf{Regularization / multi-task inductive bias:} message passing can enforce that similar experts have similar latent states, improving sample efficiency and stability, even if all losses are observed.
\end{itemize}

\subsection{Uncertainty and ensembles under full feedback}

Under full feedback, epistemic uncertainty typically shrinks faster because each time step yields $|K_t|$ supervised observations rather than $1$.
Ensembles remain useful:
\begin{itemize}
  \item to quantify epistemic uncertainty under non-stationarity (distribution shift),
  \item to obtain a robust risk-aware routing rule (e.g., mean--std),
  \item to diagnose model misspecification.
\end{itemize}
But their role for \emph{exploration} is no longer fundamental.

\subsection{Summary of differences (partial vs full feedback)}
\begin{itemize}
  \item \textbf{Objective:} \eqref{eq:elbo} (masked) $\rightarrow$ \eqref{eq:elbo_full} (sum over $j\in K_t$).
  \item \textbf{Bias/IPS:} policy-induced selection bias present in bandit logs $\rightarrow$ absent under full feedback.
  \item \textbf{Exploration:} necessary for learning under bandit feedback $\rightarrow$ not required for identifiability.
  \item \textbf{Broadcast:} required to transfer from observed expert to unobserved experts in $K_t$ $\rightarrow$ optional/regularizing and mainly relevant for experts not in $K_t$.
\end{itemize}



\section{Summary}
We provide a scalable neural replacement of switching SSM routing with:
(i) explicit regime and global factor for non-stationarity,
(ii) a statistically compatible positive likelihood for MSE (Gamma / scaled-$\chi^2$),
(iii) amortized filtering with set/temporal encoders,
(iv) robust uncertainty (Gamma aleatoric + ensemble epistemic + staleness inflation),
(v) similarity-driven cross-updates via sparse graph broadcast using a likelihood-score innovation.

\end{document}
