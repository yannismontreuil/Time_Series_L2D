% Standalone replacement text for the exploration subsection in main.tex.
% Intended to be included via \input{exploration}.

\subsubsection{Exploration via \((z_t,\mathbf{g}_t)\)-information}
\label{sec:exploration}

Bandit feedback reveals only the queried expert's residual, so the router must trade off
\emph{exploitation} (low immediate cost) against \emph{learning} (reducing posterior uncertainty to
improve future decisions).
In our IMM-factorized SLDS, two latent objects drive both non-stationarity and cross-expert transfer:
the regime \(z_t\in\{1,\dots,M\}\) and the shared factor \(\mathbf{g}_t\) (Proposition~\ref{prop:cross_update}).
We therefore score exploration by the information gained about the \emph{joint} latent state
\((z_t,\mathbf{g}_t)\) from the (potential) queried residual.
Throughout, logarithms are natural unless stated otherwise, so mutual information is measured in nats
(replace \(\log\) by \(\log_2\) to obtain bits).
We reuse the core SLDS/IMM notation from the main text: \(\phi(\mathbf{x}_t)\), \(\mathbf{B}_k\),
\(\bar{w}_t^{(m)}=\mathbb{P}(z_t=m\mid\mathcal{F}_t)\), and the predictive moments
\((\mu^{(m)}_{g,t\mid t-1},\Sigma^{(m)}_{g,t\mid t-1})\), \((\mu^{(m)}_{u,k,t\mid t-1},\Sigma^{(m)}_{u,k,t\mid t-1})\), and \(R_{m,k}\).
For Monte Carlo, we use \(\tilde{\cdot}\) to denote sampled quantities and write
\(\tilde z\sim \mathrm{Cat}((\bar{w}_t^{(m)})_{m=1}^M)\) for a categorical draw from the mode weights.

\paragraph{Decision-time predictive random variables.}
At round \(t\), the decision-time sigma-algebra is
\(
\mathcal{F}_t=\sigma(\mathcal{H}_{t-1},\mathbf{x}_t,\mathcal{E}_t)
\)
and the router chooses \(I_t\in\mathcal{E}_t\).
For each available expert \(k\in\mathcal{E}_t\), define the pre-query predictive residual random variable
\begin{equation}
\label{eq:exp_pred_rv}
e_{t,k}^{\mathrm{pred}} \sim p(e_{t,k}\mid \mathcal{F}_t).
\end{equation}
If \(I_t=k\), the realized observation is \(e_t=e_{t,k}\) and
\(
e_t \mid (\mathcal{F}_t,I_t=k)\overset{d}{=} e_{t,k}^{\mathrm{pred}}\mid \mathcal{F}_t.
\)

\paragraph{Per-mode linear-Gaussian predictive parametrization (IMM outputs).}
Fix a regime \(z_t=m\).
The IMM predictive step yields a Gaussian predictive prior for the shared factor:
\begin{equation}
\label{eq:exp_g_prior}
\mathbf{g}_t\mid(\mathcal{F}_t,z_t=m)\sim
\mathcal{N}\!\left(\mu^{(m)}_{g,t\mid t-1},\,\Sigma^{(m)}_{g,t\mid t-1}\right).
\end{equation}
Under the factorized predictive belief, querying expert \(k\) induces the linear-Gaussian observation channel
\begin{equation}
\label{eq:exp_channel}
e_{t,k}^{\mathrm{pred}} \mid (\mathbf{g}_t,\mathcal{F}_t,z_t=m)
\sim
\mathcal{N}\!\big(h_{t,k}^\top \mathbf{g}_t + b^{(m)}_{t,k},\, s^{(m)}_{t,k}\big),
\end{equation}
with mode-specific quantities
\begin{equation}
\label{eq:exp_channel_params}
h_{t,k}\coloneqq \mathbf{B}_k^\top \phi(\mathbf{x}_t)\in\mathbb{R}^{d_g},
\qquad
b^{(m)}_{t,k}\coloneqq \phi(\mathbf{x}_t)^\top \mu^{(m)}_{u,k,t\mid t-1},
\qquad
s^{(m)}_{t,k}\coloneqq
\phi(\mathbf{x}_t)^\top \Sigma^{(m)}_{u,k,t\mid t-1}\phi(\mathbf{x}_t) + R_{m,k}.
\end{equation}

\paragraph{Exploitation score: predictive cost and gap.}
Recall the realized cost \(C_{t,k}=\psi(e_{t,k})+\beta_k\), where \(\beta_k\ge 0\) is the known query
fee.
In practice (and in our experiments), we use squared loss,
\begin{equation}
\label{eq:exp_squared_loss}
\psi(u)=u^2,
\end{equation}
and we will simplify expressions accordingly; nothing in the \((z_t,\mathbf{g}_t)\)-information score
depends on this choice.
Define the predictive (virtual) cost random variable
\begin{equation}
\label{eq:exp_virtual_cost}
C_{t,k}^{\mathrm{pred}}
\coloneqq
\psi(e_{t,k}^{\mathrm{pred}})+\beta_k,
\qquad k\in\mathcal{E}_t,
\end{equation}
with conditional mean
\begin{equation}
\label{eq:exp_mean_cost}
\widehat{c}_t(k)\coloneqq \mathbb{E}\!\left[C_{t,k}^{\mathrm{pred}}\,\middle|\,\mathcal{F}_t\right]
=
\mathbb{E}\!\left[\psi(e_{t,k}^{\mathrm{pred}})\,\middle|\,\mathcal{F}_t\right]+\beta_k.
\end{equation}
Let \(k_t^\star\in\arg\min_{k\in\mathcal{E}_t}\widehat{c}_t(k)\) and define the predictive gap
\begin{equation}
\label{eq:exp_gap}
\Delta_t(k)\coloneqq \widehat{c}_t(k)-\widehat{c}_t(k_t^\star)\ge 0.
\end{equation}

\paragraph{Computing \(\widehat{c}_t(k)\) from per-mode moments.}
From \eqref{eq:exp_g_prior}--\eqref{eq:exp_channel}, the mode-conditioned predictive residual is Gaussian with
\begin{align}
\label{eq:exp_residual_mean}
\mu^{(m)}_{e,t,k}
&\coloneqq
\mathbb{E}\!\left[e_{t,k}^{\mathrm{pred}}\,\middle|\,\mathcal{F}_t,z_t=m\right]
=
h_{t,k}^\top \mu^{(m)}_{g,t\mid t-1}+b^{(m)}_{t,k},\\
\label{eq:exp_residual_var}
v^{(m)}_{e,t,k}
&\coloneqq
\mathrm{Var}\!\left(e_{t,k}^{\mathrm{pred}}\,\middle|\,\mathcal{F}_t,z_t=m\right)
=
h_{t,k}^\top \Sigma^{(m)}_{g,t\mid t-1} h_{t,k}+s^{(m)}_{t,k}.
\end{align}
Let \(\bar{w}_t^{(m)}=\mathbb{P}(z_t=m\mid\mathcal{F}_t)\).
Then \(p(e_{t,k}\mid\mathcal{F}_t)=\sum_{m=1}^M \bar{w}_t^{(m)}\,\mathcal{N}(\mu^{(m)}_{e,t,k},v^{(m)}_{e,t,k})\).
For general \(\psi\),
\begin{equation}
\label{eq:exp_cost_mix}
\mathbb{E}\!\left[\psi(e_{t,k}^{\mathrm{pred}})\,\middle|\,\mathcal{F}_t\right]
=
\sum_{m=1}^M \bar{w}_t^{(m)}\,
\mathbb{E}\!\left[\psi(E)\right]_{E\sim\mathcal{N}(\mu^{(m)}_{e,t,k},v^{(m)}_{e,t,k})}.
\end{equation}
In the squared-loss case \(\psi(e)=e^2\) from \eqref{eq:exp_squared_loss}, we have
\(\mathbb{E}[E^2]=v+\mu^2\), hence
\begin{equation}
\label{eq:exp_square_loss_mix}
\widehat{c}_t(k)
=
\left(\sum_{m=1}^M \bar{w}_t^{(m)}\big(v^{(m)}_{e,t,k}+(\mu^{(m)}_{e,t,k})^2\big)\right)+\beta_k.
\end{equation}

\paragraph{Learning score: information about \((z_t,\mathbf{g}_t)\).}
Define the \((z_t,\mathbf{g}_t)\)-information gain of querying expert \(k\) by
\begin{equation}
\label{eq:exp_ig_zg_def}
\mathrm{IG}^{(z,g)}_t(k)
\coloneqq
\mathcal{I}\!\left((z_t,\mathbf{g}_t);\ e_{t,k}^{\mathrm{pred}}\,\middle|\,\mathcal{F}_t\right).
\end{equation}
By the chain rule,
\begin{align}
\label{eq:exp_ig_zg_chain}
\mathrm{IG}^{(z,g)}_t(k)
&=
\mathcal{I}\!\left(z_t;\ e_{t,k}^{\mathrm{pred}}\,\middle|\,\mathcal{F}_t\right)
+
\mathcal{I}\!\left(\mathbf{g}_t;\ e_{t,k}^{\mathrm{pred}}\,\middle|\,\mathcal{F}_t,z_t\right)\\
&=
\underbrace{\mathcal{I}\!\left(z_t;\ e_{t,k}^{\mathrm{pred}}\,\middle|\,\mathcal{F}_t\right)}_{\text{mode-identification}}
+
\underbrace{\sum_{m=1}^M \bar{w}_t^{(m)}\,
\mathcal{I}\!\left(\mathbf{g}_t;\ e_{t,k}^{\mathrm{pred}}\,\middle|\,\mathcal{F}_t,z_t=m\right)}_{\text{shared-factor refinement}}.
\end{align}
The second term admits a closed form per mode; the first term is a 1D Gaussian-mixture information
quantity that can be computed accurately with light Monte Carlo.

\paragraph{Closed form: \(\mathcal{I}(\mathbf{g}_t;e_{t,k}^{\mathrm{pred}}\mid \mathcal{F}_t,z_t=m)\).}
Fix \(z_t=m\).
Let \(G\coloneqq \mathbf{g}_t\) and \(Y\coloneqq e_{t,k}^{\mathrm{pred}}\).
Equation \eqref{eq:exp_channel} implies the affine Gaussian channel
\(
Y=h_{t,k}^\top G+b^{(m)}_{t,k}+\varepsilon
\)
with \(\varepsilon\sim\mathcal{N}(0,s^{(m)}_{t,k})\) independent of \(G\).
Then
\begin{equation}
\label{eq:exp_ig_g_mode}
\mathcal{I}\!\left(\mathbf{g}_t;\ e_{t,k}^{\mathrm{pred}}\,\middle|\,\mathcal{F}_t,z_t=m\right)
=
\frac12\log\!\left(1+\frac{h_{t,k}^\top \Sigma^{(m)}_{g,t\mid t-1} h_{t,k}}{s^{(m)}_{t,k}}\right).
\end{equation}

\paragraph{Monte Carlo: \(\mathcal{I}(z_t;e_{t,k}^{\mathrm{pred}}\mid\mathcal{F}_t)\) for a Gaussian mixture.}
Let \(p_m(e)\coloneqq p(e_{t,k}^{\mathrm{pred}}=e\mid \mathcal{F}_t,z_t=m)=\mathcal{N}(e;\mu^{(m)}_{e,t,k},v^{(m)}_{e,t,k})\) and
\(
p_{\mathrm{mix}}(e)\coloneqq \sum_{m=1}^M \bar{w}_t^{(m)}p_m(e).
\)
Then
\begin{align}
\label{eq:exp_iz_mc}
\mathcal{I}\!\left(z_t;\ e_{t,k}^{\mathrm{pred}}\,\middle|\,\mathcal{F}_t\right)
&=
\sum_{m=1}^M \bar{w}_t^{(m)}\,\mathrm{KL}\!\left(p_m\ \middle\|\ p_{\mathrm{mix}}\right)\\
&=
\sum_{m=1}^M \bar{w}_t^{(m)}\,
\mathbb{E}_{E\sim p_m}\!\left[\log p_m(E)-\log p_{\mathrm{mix}}(E)\right].
\end{align}
This suggests the estimator (with \(S\) samples per mode):
\begin{equation}
\label{eq:exp_iz_estimator}
\widehat{\mathcal{I}}_t^{(z)}(k)
\coloneqq
\sum_{m=1}^M \bar{w}_t^{(m)}\left(\frac{1}{S}\sum_{s=1}^S \Big[\log p_m(E_{m,s})-\log p_{\mathrm{mix}}(E_{m,s})\Big]\right),
\qquad
E_{m,s}\overset{\text{iid}}{\sim}\mathcal{N}(\mu^{(m)}_{e,t,k},v^{(m)}_{e,t,k}).
\end{equation}

\paragraph{Stable evaluation of \(\log p_{\mathrm{mix}}(e)\).}
Compute Gaussian log-densities via
\begin{equation}
\label{eq:exp_logpdf_gauss}
\log \mathcal{N}(e;\mu,v)
=
-\frac12\log(2\pi v)-\frac{(e-\mu)^2}{2v}.
\end{equation}
Define \(\ell_m(e)\coloneqq \log \bar{w}_t^{(m)}+\log \mathcal{N}(e;\mu^{(m)}_{e,t,k},v^{(m)}_{e,t,k})\).
Then compute \(\log p_{\mathrm{mix}}(e)\) by a stable log-sum-exp:
\begin{equation}
\label{eq:exp_logmix_lse}
\log p_{\mathrm{mix}}(e)
=
\log\!\left(\sum_{m=1}^M e^{\ell_m(e)}\right)
=
a(e)+\log\!\left(\sum_{m=1}^M e^{\ell_m(e)-a(e)}\right),
\qquad
a(e)\coloneqq \max_{m\in\{1,\dots,M\}} \ell_m(e).
\end{equation}

\paragraph{Final \((z_t,\mathbf{g}_t)\)-information gain.}
Combine \eqref{eq:exp_ig_zg_chain}, \eqref{eq:exp_ig_g_mode}, and \eqref{eq:exp_iz_estimator}:
\begin{equation}
\label{eq:exp_ig_zg_final}
\widehat{\mathrm{IG}}^{(z,g)}_t(k)
\coloneqq
\widehat{\mathcal{I}}_t^{(z)}(k)
+
\sum_{m=1}^M \bar{w}_t^{(m)}\,
\frac12\log\!\left(1+\frac{h_{t,k}^\top \Sigma^{(m)}_{g,t\mid t-1} h_{t,k}}{s^{(m)}_{t,k}}\right).
\end{equation}

\paragraph{Action selection rules (direct, non-randomized).}
With \(\Delta_t(k)\) and \(\widehat{\mathrm{IG}}^{(z,g)}_t(k)\) computed for all \(k\in\mathcal{E}_t\), two
simple direct policies are:
\begin{align}
\label{eq:exp_det_ids_zg}
\text{(ratio / IDS-style)}\qquad
I_t &\in \arg\min_{k\in\mathcal{E}_t}\ \frac{\Delta_t(k)^2}{\widehat{\mathrm{IG}}^{(z,g)}_t(k)},\\
\label{eq:exp_additive_zg}
\text{(additive / MI-bonus)}\qquad
I_t &\in \arg\min_{k\in\mathcal{E}_t}\ \widehat{c}_t(k)-\lambda_t\,\widehat{\mathrm{IG}}^{(z,g)}_t(k).
\end{align}
For \eqref{eq:exp_det_ids_zg}, interpret the ratio as \(+\infty\) when \(\widehat{\mathrm{IG}}^{(z,g)}_t(k)=0\) and
\(\Delta_t(k)>0\); if \(\widehat{\mathrm{IG}}^{(z,g)}_t(k)=0=\Delta_t(k)\), treat the ratio as \(0\).
Equation \eqref{eq:exp_additive_zg} is typically easier to tune: \(\lambda_t\) directly encodes how much
expected cost you are willing to pay per nat/bit of information about \((z_t,\mathbf{g}_t)\).

\paragraph{Implementation map (directly translatable to code).}
At round \(t\), a straightforward implementation can be organized into the following pure functions:
\begin{itemize}
\item \texttt{channel\_params(k,m)} \(\to (h_{t,k},b^{(m)}_{t,k},s^{(m)}_{t,k})\) from \eqref{eq:exp_channel_params};
\item \texttt{residual\_moments(k,m)} \(\to (\mu^{(m)}_{e,t,k},v^{(m)}_{e,t,k})\) from \eqref{eq:exp_residual_mean}--\eqref{eq:exp_residual_var};
\item \texttt{logpdf\_normal(e,mu,var)} using \eqref{eq:exp_logpdf_gauss};
\item \texttt{logpdf\_mix(e,weights,mu[1:M],var[1:M])} using the log-sum-exp in \eqref{eq:exp_logmix_lse};
\item \texttt{estimate\_I\_z(k,S)} implementing \eqref{eq:exp_iz_estimator};
\item \texttt{score(k)} computing \(\widehat{\mathrm{IG}}^{(z,g)}_t(k)\) by \eqref{eq:exp_ig_zg_final};
\item \texttt{select\_action(scores,costs)} using \eqref{eq:exp_det_ids_zg} or \eqref{eq:exp_additive_zg}.
\end{itemize}

\paragraph{Alternative baselines: Bayes-UCB and posterior sampling.}
It can be very informative to benchmark against exploration rules that are widely used in Bayesian
bandits and do not require an explicit information-gain computation.
In our setting, they plug directly into the predictive cost random variable \(C^{\mathrm{pred}}_{t,k}\)
from \eqref{eq:exp_virtual_cost}.

\subparagraph{Bayes-UCB (for costs: a Bayesian LCB rule).}
For losses/costs, the natural analogue of UCB is a \emph{lower} credible bound on the cost.
Let \(F_{t,k}(c)\coloneqq \mathbb{P}(C^{\mathrm{pred}}_{t,k}\le c\mid \mathcal{F}_t)\) be the conditional
cdf of the predictive cost and define the \(\alpha\)-quantile
\begin{equation}
\label{eq:exp_bayes_ucb_quantile}
Q_{t}(k;\alpha)
\coloneqq
\inf\left\{c\in\mathbb{R}: F_{t,k}(c)\ge \alpha\right\}.
\end{equation}
Given a confidence schedule \(\alpha_t\in(0,1)\) that decays with \(t\) (e.g., \(\alpha_t=1/t\) or \(\alpha_t=1/(t\log^2 t)\)),
the Bayes-UCB (LCB) decision rule is
\begin{equation}
\label{eq:exp_bayes_ucb_rule}
I_t \in \arg\min_{k\in\mathcal{E}_t}\ Q_t(k;\alpha_t).
\end{equation}
Intuition: \eqref{eq:exp_bayes_ucb_rule} chooses an expert that looks good under an optimistic
assessment of its cost (a low quantile), thereby naturally favoring experts with large posterior
uncertainty when they could plausibly be very good.

\subparagraph{Computing \(Q_t(k;\alpha)\) under the IMM predictive model.}
With squared loss, \(C^{\mathrm{pred}}_{t,k}=(e^{\mathrm{pred}}_{t,k})^2+\beta_k\) is a nonlinear
transformation of a Gaussian mixture, so closed-form quantiles are not available in general.
However, quantiles are easy to estimate by Monte Carlo using the same per-mode predictive objects as
Algorithm~\ref{alg:zg_information_exploration}:
\begin{enumerate}
\item draw a mode \(\tilde z\sim \mathrm{Cat}((\bar{w}_t^{(m)})_{m=1}^M)\);
\item draw \(\tilde{\mathbf{g}}\sim \mathcal{N}(\mu^{(\tilde z)}_{g,t\mid t-1},\Sigma^{(\tilde z)}_{g,t\mid t-1})\);
\item for each \(k\), draw \(\tilde e_k = h_{t,k}^\top \tilde{\mathbf{g}} + b^{(\tilde z)}_{t,k} + \tilde\varepsilon_k\) with \(\tilde\varepsilon_k\sim\mathcal{N}(0,s^{(\tilde z)}_{t,k})\);
\item set \(\tilde C_k=\psi(\tilde e_k)+\beta_k\).
\end{enumerate}
Repeating \(N\) times yields samples \(\{\tilde C_{k}^{(n)}\}_{n=1}^N\), and \(Q_t(k;\alpha)\) is the empirical
\(\alpha\)-quantile.

\subparagraph{Posterior sampling (Thompson sampling).}
Posterior sampling chooses actions by sampling a plausible world from the current posterior and then
acting optimally in that sampled world.
In our setting, a direct and implementation-friendly one-step variant is \emph{posterior sampling on
predictive costs}:
\begin{align}
\label{eq:exp_ps_cost}
\text{(PS on predictive costs)}\qquad
&(\tilde z_t,\tilde{\mathbf{g}}_t)\sim p(z_t,\mathbf{g}_t\mid\mathcal{F}_t),\quad
\tilde e_{t,k}=h_{t,k}^\top \tilde{\mathbf{g}}_t + b^{(\tilde z_t)}_{t,k} + \tilde\varepsilon_{t,k},\ \ \tilde\varepsilon_{t,k}\sim \mathcal{N}(0,s^{(\tilde z_t)}_{t,k}),\\
&I_t \in \arg\min_{k\in\mathcal{E}_t}\ \psi(\tilde e_{t,k})+\beta_k.
\end{align}
This rule preserves the \emph{shared-factor coupling} across experts because all sampled residuals are
generated using the same \((\tilde z_t,\tilde{\mathbf{g}}_t)\).
If one prefers a deterministic decision conditional on sampled latents, one can instead replace
\(\psi(\tilde e_{t,k})\) by the conditional predictive expectation
\(\mathbb{E}[\psi(E)]\) with \(E\sim \mathcal{N}(h_{t,k}^\top\tilde{\mathbf{g}}_t+b^{(\tilde z_t)}_{t,k},s^{(\tilde z_t)}_{t,k})\).
Under squared loss \(\psi(e)=e^2\), this yields the closed form
\begin{equation}
\label{eq:exp_ps_squared_det}
\mathbb{E}\!\left[E^2\right]
=
s^{(\tilde z_t)}_{t,k}+\big(h_{t,k}^\top\tilde{\mathbf{g}}_t+b^{(\tilde z_t)}_{t,k}\big)^2,
\qquad
I_t \in \arg\min_{k\in\mathcal{E}_t}\ \left(s^{(\tilde z_t)}_{t,k}+\big(h_{t,k}^\top\tilde{\mathbf{g}}_t+b^{(\tilde z_t)}_{t,k}\big)^2+\beta_k\right),
\end{equation}
which avoids sampling \(\tilde\varepsilon_{t,k}\) and reduces Monte Carlo variance.

\begin{algorithm}[t]
\caption{Bayes-UCB and posterior sampling at round \(t\) (implementation templates)}
\label{alg:bayes_ucb_ps_templates}
\begin{algorithmic}[1]
\REQUIRE Predictive objects from the IMM-factorized SLDS: \(\bar{w}_t^{(m)}\), \((\mu^{(m)}_{g,t\mid t-1},\Sigma^{(m)}_{g,t\mid t-1})\), and \((h_{t,k},b^{(m)}_{t,k},s^{(m)}_{t,k})\).
\REQUIRE Loss \(\psi\), fees \((\beta_k)_{k\in\mathcal{E}_t}\), Bayes-UCB parameters \((\alpha_t,N)\).
\STATE \textbf{Bayes-UCB (LCB) via Monte Carlo quantiles:}
\FOR{\(n=1,\dots,N\)}
    \STATE Draw \(\tilde z^{(n)}\sim \mathrm{Cat}((\bar{w}_t^{(m)})_{m=1}^M)\) and \(\tilde{\mathbf{g}}^{(n)}\sim \mathcal{N}(\mu^{(\tilde z^{(n)})}_{g,t\mid t-1},\Sigma^{(\tilde z^{(n)})}_{g,t\mid t-1})\).
    \FOR{each \(k\in\mathcal{E}_t\)}
        \STATE Draw \(\tilde e_{k}^{(n)}=h_{t,k}^\top \tilde{\mathbf{g}}^{(n)} + b^{(\tilde z^{(n)})}_{t,k} + \tilde\varepsilon\), with \(\tilde\varepsilon\sim \mathcal{N}(0,s^{(\tilde z^{(n)})}_{t,k})\).
        \STATE Set \(\tilde C_{k}^{(n)}\leftarrow \psi(\tilde e_{k}^{(n)})+\beta_k\).
    \ENDFOR
\ENDFOR
\FOR{each \(k\in\mathcal{E}_t\)}
    \STATE Set \(Q_t(k;\alpha_t)\leftarrow\) empirical \(\alpha_t\)-quantile of \(\{\tilde C_k^{(n)}\}_{n=1}^N\).
\ENDFOR
\STATE Choose \(I_t\in\arg\min_k Q_t(k;\alpha_t)\).
\STATE \textbf{Posterior sampling on predictive costs:}
\STATE Draw \(\tilde z\sim \mathrm{Cat}((\bar{w}_t^{(m)})_{m=1}^M)\) and \(\tilde{\mathbf{g}}\sim \mathcal{N}(\mu^{(\tilde z)}_{g,t\mid t-1},\Sigma^{(\tilde z)}_{g,t\mid t-1})\).
\FOR{each \(k\in\mathcal{E}_t\)}
    \STATE Draw \(\tilde e_{t,k}=h_{t,k}^\top\tilde{\mathbf{g}}+b^{(\tilde z)}_{t,k}+\tilde\varepsilon_{t,k}\), with \(\tilde\varepsilon_{t,k}\sim\mathcal{N}(0,s^{(\tilde z)}_{t,k})\).
    \STATE Set \(\tilde C_{t,k}=\psi(\tilde e_{t,k})+\beta_k\).
\ENDFOR
\STATE Choose \(I_t\in\arg\min_k \tilde C_{t,k}\).
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
\caption{\((z_t,\mathbf{g}_t)\)-information exploration at round \(t\)}
\label{alg:zg_information_exploration}
\begin{algorithmic}[1]
\REQUIRE \(\mathcal{F}_t\), available experts \(\mathcal{E}_t\), feature map \(\phi(\mathbf{x}_t)\), predictive weights \(\bar{w}_t^{(m)}\).
\REQUIRE Per-mode predictive moments for \(\mathbf{g}_t\): \((\mu^{(m)}_{g,t\mid t-1},\Sigma^{(m)}_{g,t\mid t-1})\).
\REQUIRE Per-mode predictive moments for each expert's idiosyncratic state: \((\mu^{(m)}_{u,k,t\mid t-1},\Sigma^{(m)}_{u,k,t\mid t-1})\).
\REQUIRE Fees \((\beta_k)_k\), loss \(\psi\), and Monte Carlo budget \(S\).
\FOR{each \(k\in\mathcal{E}_t\)}
    \FOR{each mode \(m\in\{1,\dots,M\}\)}
        \STATE Compute \(h_{t,k},b^{(m)}_{t,k},s^{(m)}_{t,k}\) from \eqref{eq:exp_channel_params}.
        \STATE Compute \(\mu^{(m)}_{e,t,k},v^{(m)}_{e,t,k}\) from \eqref{eq:exp_residual_mean}--\eqref{eq:exp_residual_var}.
        \STATE Compute \(\mathrm{IG}^{(m)}_{g}(k)=\frac12\log\!\left(1+\frac{h_{t,k}^\top \Sigma^{(m)}_{g,t\mid t-1} h_{t,k}}{s^{(m)}_{t,k}}\right)\) from \eqref{eq:exp_ig_g_mode}.
    \ENDFOR
    \STATE Compute \(\widehat{c}_t(k)\) (e.g., \eqref{eq:exp_cost_mix}, or \eqref{eq:exp_square_loss_mix} if \(\psi(e)=e^2\)).
    \STATE Estimate \(\widehat{\mathcal{I}}^{(z)}_t(k)\) via \eqref{eq:exp_iz_estimator} using \(\log p_{\mathrm{mix}}\) from \eqref{eq:exp_logmix_lse}.
    \STATE Set \(\widehat{\mathrm{IG}}^{(z,g)}_t(k)\) via \eqref{eq:exp_ig_zg_final}.
\ENDFOR
\STATE Compute \(k_t^\star\in\arg\min_{k\in\mathcal{E}_t}\widehat{c}_t(k)\) and \(\Delta_t(k)\) via \eqref{eq:exp_gap}.
\STATE Select \(I_t\) using either \eqref{eq:exp_det_ids_zg} or \eqref{eq:exp_additive_zg}.
\RETURN \(I_t\).
\end{algorithmic}
\end{algorithm}
