# not specify data_source thus uses synthetic data generation in slds_imm_router.py
environment:
  data_source: "synthetic"
  # Main experiment environment (slds_imm_router.py)
  num_experts: 5
  # Tuned number of regimes for correlated router (from best_corr_hyperparams.json)
  num_regimes: 2
  # Dataset setting for the synthetic environment.
  #   - "easy_setting": default, moderate noise, single regime switch.
  #   - "noisy_forgetting": higher noise and regime 0 → 1 → 0 pattern
  #                        (or a multi-regime 0 → 1 → 2 → … → M-1 → 0
  #                        pattern when num_regimes > 2) to highlight
  #                        potential catastrophic forgetting.
  setting: "easy_setting"
  # choose from: 
  # easy_setting,
  # noisy_forgetting,
  #   currently unsupported for M = 5:
  # sidekick_trap,
  #   Note: must have T >= 1000 for sidekick_trap since regime switch at t=1000
  # theoretical_trap,
  # cluster_transfer,
  # division_by_M
  #
  # When setting: noisy_forgetting, slds_imm_router.py will
  # automatically use at least 5 regimes (num_regimes ≥ 5) to create a
  # more challenging Markovian pattern. You can override this by
  # setting num_regimes to a larger value.
  # For "noisy_forgetting", you may also want to increase T
  # (e.g., T: 1000) and optionally override the noise scale:
  noise_scale: 0.6
  # State dimension d = dim φ(x). The current feature map returns 2D features.
  state_dim: 2
  # Time horizon T
  T: 2000
  seed: 42
  # expert 1 is unavailable in these intervals
  unavailable_expert_idx: 1
  unavailable_intervals:
    - [300, 350]
    - [400, 450]
    - [500, 550]
    - [600, 650]
    - [700, 750]
    - [800, 850]
  # expert 4 has arrivals only in these intervals
  arrival_expert_idx: 4
  arrival_intervals:
    - [90, 300]
    - [300, 700]

routers:
  # Risk sensitivity for SLDS-IMM routers.
  # Can be a scalar (regime-independent) or a list
  # of length num_regimes for regime-specific λ_k.
  lambda_risk: 0.0

  # Consultation costs for routers (β_j).
  # Can be a single scalar (broadcast to all experts) or
  # a list of length num_experts.
  # Example: [0.0, 0.0, 0.0, 0.0, 0.0]
  beta: 0.0

  # Optional configuration for the independent-expert SLDS-IMM router.
  slds_imm:
    # Optional: full A_k matrices; default is identity per regime. of size M x d x d
#    A:
#      - [[1.0, 0.0],
#         [0.0, 1.0]]
#      - [[1.0, 0.0],
#         [0.0, 1.0]]
    # Process noise covariances Q_k are built from per-regime scales.
    # Can be:
    #   - a single scalar (same scale for all regimes),
    #   - a list of length num_regimes, or
    #   - for num_regimes > 2, a 2-element list [q_0, q_other] which is
    #     interpreted as one scale for regime 0 and one shared scale for
    #     all other regimes.
    Q_scales: [0.002, 0.02]
    # Observation noise R_{k,j}: single scalar broadcast to all regimes/experts.
    R_scalar: 0.5
    # Regime transition matrix Π (shape [num_regimes, num_regimes]).
    # Some note on specifying Π:
    # - If omitted, defaults to uniform transitions.
    # - If regimes is sticky, Π will be blended with identity
    # - If expect frequent regime switches, lower diagonal values in Π.
    # Pi:
    #   - [1.0, 0.0, 0.0, 0.0, 0.0]
    #   - [0.0, 1.0, 0.0, 0.0, 0.0]
    #   - [0.0, 0.0, 1.0, 0.0, 0.0]
    #   - [0.0, 0.0, 0.0, 1.0, 0.0]
    #   - [0.0, 0.0, 0.0, 0.0, 1.0]
    # Example of more dynamic transitions:
    # Pi:
    #   - [0.7, 0.1, 0.1, 0.05, 0.05]
    #   - [0.1, 0.7, 0.1, 0.05, 0.05]
    #   - [0.1, 0.1, 0.7, 0.05, 0.05]
    #   - [0.05, 0.05, 0.05, 0.7, 0.15]
    #   - [0.05, 0.05, 0.05, 0.15, 0.7]
    # Prior for initial regime distribution π_0
    # Prior on reliability states α_{j,t} (shape [d] and [d,d]).
    pop_mean: [0.0, 0.0]
    pop_cov:
      - [1.0, 0.0]
      - [0.0, 1.0]
    # Numerical stabilizer ε for covariances.
    eps: 1.0e-8

  # Optional configuration for the correlated-expert SLDS-IMM router.
  # If omitted, defaults matching the original example are used.
  slds_imm_corr:
    # Base configuration aligned to best partial-feedback correlated router.
    exploration_mode: "greedy"
    feature_mode: "fixed"
    feature_learning_rate: 0.0
    # Feature architecture: "linear" (no hidden layers in fixed mode).
    feature_arch: "linear"
    # Hidden size for MLP features (only relevant if feature_arch: "mlp").
    feature_hidden_dim: 4
    # Nonlinearity for MLP features: "tanh" or "relu".
    feature_activation: "tanh"
    # Shared factor dimension d_g and idiosyncratic dimension d_u.
    shared_dim: 1
    idiosyncratic_dim: 2
    # Shared-factor dynamics A_gk default to identity; Q_gk built from scales.
    # Hyperparameter-tuned absolute value for shared-factor process noise
    # (best partial: q_g = 0.01).
    Q_g_scales: 0.01
    # Idiosyncratic dynamics A_uk default to identity; Q_uk built from scales.
    # Hyperparameter-tuned absolute value for idiosyncratic process noise
    # (best partial: q_u = 0.001).
    Q_u_scales: 0.001
    # Observation noise R_{k,j} for the correlated router (base/partial mode).
    # Best partial uses r = 1.0.
    R_scalar: 1.0
    # Shared-factor loadings B_j: first feature loads on shared factor with
    # this scalar (same for all experts).
    B_intercept_load: 1.0
    # Priors for shared factor g_t and idiosyncratic states u_{j,t}.
    g_mean0: [0.0]
    g_cov0:
      - [1.0]
    u_mean0: [0.0, 0.0]
    u_cov0:
      - [1.0, 0.0]
      - [0.0, 1.0]
    # Numerical stabilizer ε for joint covariances.
    eps: 1.0e-8

    # Optional mode-specific overrides for the correlated router.
    # These keys allow router_partial_corr and router_full_corr to use
    # *different* hyperparameters while sharing the same base model
    # structure. Any field set here overrides the corresponding field
    # above for that mode only.

    # Best hyperparameters (see param/best_corr_hyperparams.json):
    #   partial: M=2, lambda_risk=0.0, q_g=0.01, q_u=0.001, r=1.0,
    #            feature_mode=fixed, feature_lr=0.0, feature_arch=linear,
    #            shared_dim=1, idiosyncratic_dim=2
    #   full:    M=2, lambda_risk=0.0, q_g=1.0, q_u=0.001, r=0.1,
    #            feature_mode=fixed, feature_lr=0.0, feature_arch=linear,
    #            shared_dim=1, idiosyncratic_dim=2
    partial_overrides:
      feature_mode: "fixed"
      feature_learning_rate: 0.0
      shared_dim: 1
      idiosyncratic_dim: 2
      Q_g_scales: 0.01
      Q_u_scales: 0.001
      feature_arch: "linear"
      R_scalar: 1.0
    full_overrides:
      feature_mode: "fixed"
      feature_learning_rate: 0.0
      shared_dim: 1
      idiosyncratic_dim: 2
      Q_g_scales: 1.0
      Q_u_scales: 0.001
      feature_arch: "linear"
      R_scalar: 0.1

  # Optional configuration for the recurrent SLDS-IMM router.
  # If omitted, defaults matching the non-recurrent example are used.
  slds_imm_recurrent:
    C:
      # shape (M, N, d); example for M=2, N=5, d=2
      - [[0.02, 0.0], [0.02, 0.0], [0.02, 0.0], [0.02, 0.0], [0.02, 0.0]]
      - [[-0.02, 0.0], [-0.02, 0.0], [-0.02, 0.0], [-0.02, 0.0], [-0.02, 0.0]]
    # stick_gamma:
    #   - [0.5, 0.0]
    #   - [-0.5, 0.0]
    # size: (M, d)
    stick_gamma:
      - [0.2, 0.0]
      - [-0.2, 0.0]
    stick_kappa: [0.0, 0.0]
    # Optional row-conditioned stick-breaking params (shape M x M x d and M x M).
    # When set, Π_t rows differ by previous regime. Example for M=2, d=2:
    stick_gamma_rows:
      - [[0.2, 0.0], [0.1, 0.0]]
      - [[-0.2, 0.0], [-0.1, 0.0]]
    stick_kappa_rows:
      - [0.0, 0.0]
      - [0.0, 0.0]

  # Registry pruning threshold for correlated router (in decision epochs).
  # If null, pruning is disabled.
  staleness_threshold: null

baselines:
  l2d:
    # L2D baseline with configurable architecture ("mlp" or "rnn")
    arch: "mlp"
    learning_rate: 0.01
    hidden_dim: 8
    # Window size for context; use 1 for standard L2D.
    window_size: 1

    # Coefficients α_j in μ_j = α_j * RMSE + β_j.
    # Scalar or list of length num_experts.
    alpha: 1.0

    # Consultation costs for L2D baseline. If omitted or null, the
    # router β vector from routers.beta is used.
    beta: null

  l2d_sw:
    # L2D baseline with sliding-window context (L2D_SW).
    # By default we use an RNN policy here to contrast with the MLP L2D.
    arch: "rnn"
    learning_rate: 0.01
    # model's internal  hidden dimension, larger means more capacity
    hidden_dim: 8
    window_size: 5

    alpha: 1.0
    beta: null

  #  Models used in the field of Contextual Multi-Armed Bandits (CMAB)
  #  maximizing cumulative rewards by intelligently balancing
  #  exploration (trying new actions) and
  #  exploitation (choosing the best known action) based on observed context.
  linucb:
    # LinUCB baseline (contextual bandit on losses).
    # UCB_α = x^T θ_hat - α * sqrt(x^T Α_a^-1 x)
    # alpha_ucb controls the optimism bonus; larger values explore more.
    alpha_ucb: 1.0
    # Ridge regularization strength for per-expert linear models.
    lambda_reg: 1.0

  # First transform the context features via a neural network to feature space,
  # then apply LinUCB in that feature space.
  neural_ucb:
    # NeuralUCB baseline (NeuralLinear approximation).
    alpha_ucb: 1.0
    lambda_reg: 1.0
    hidden_dim: 16
    nn_learning_rate: 0.001

horizon_planning:
  # Starting time index for horizon planning in slds_imm_router.py
  t0: 400

  # Planning horizon length
  H: 20
  # Planning method for horizon scheduling:
  #   - "regressive": original router-influenced context planning
  #   - "monte_carlo": scenario-based staffing planning with N=1
  #   - "{N}_monte_carlo": N-scenario Monte Carlo staffing planning
  method: "20_monte_carlo"

  # Scenario generator parameters for Monte Carlo planning.
  # Gaussian AR(1) feature-space perturbation as in
  # Section~\ref{sec:staffing-generator-gaussian}.
  scenario_generator:
    type: "gaussian_ar1"
    # AR(1) coefficient rho in [-1,1).
    rho: 0.9
    # Initial residual covariance Σ_0 ≈ (sigma0^2 I).
    sigma0: 0.1
    # Innovation covariance Q ≈ (q_scale^2 I).
    q_scale: 0.1
