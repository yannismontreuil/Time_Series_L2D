environment:
  # "Stale Prior" theoretical trap experiment for the paper.
  data_source: synthetic
  num_experts: 3
  num_regimes: 2
  # Special setting implemented in synthetic_env.py that constructs:
  #   - Regime 0 (good times): experts 0 and 1 ~ 0, expert 2 ~ 1
  #   - Regime 1 (bad times):  experts 0 and 1 ~ 10, expert 2 ~ 1
  #   - Expert 1 goes offline during the start of the bad regime,
  #     creating a "stale prior" on its historical performance.
  setting: theoretical_trap
  T: 3000
  seed: 42

routers:
  # Risk sensitivity for SLDS-IMM routers.
  lambda_risk: -2.0

  # Consultation costs for routers (β_j).
  beta: 0.0

  # Independent-expert SLDS-IMM router (baseline).
  slds_imm:
    Q_scales: [0.005, 0.05]
    R_scalar: 1.0
    Pi:
      - [0.5, 0.5]
      - [0.2, 0.8]
    pop_mean: [0.0, 0.0]
    pop_cov:
      - [1.0, 0.0]
      - [0.0, 1.0]
    eps: 1.0e-8

  # Correlated-expert SLDS-IMM router: this is the model expected to
  # exploit the shared-factor correlation between "Barometer" and
  # "Sidekick" and infer the Sidekick's reliability when it is offline.
  slds_imm_corr:
    exploration_mode: "ids"
    feature_mode: "learnable"
    feature_learning_rate: 0.001
    feature_arch: "linear"
    feature_hidden_dim: 4
    feature_activation: "tanh"

    # Shared factor / idiosyncratic dimensions (base values; see
    # partial_overrides/full_overrides below for mode-specific tuning).
    shared_dim: 1
    idiosyncratic_dim: 1

    # Base process noise scales; mode-specific overrides below set the
    # actual values used by partial/full correlated routers.
    Q_g_scales: 0.001
    Q_u_scales: 0.001

    B_intercept_load: 1.0

    g_mean0: [0.0]
    g_cov0:
      - [1.0]
    u_mean0: [0.0]
    u_cov0:
      - [1.0]
    eps: 1.0e-8

    # Mode-specific overrides to match best_corr_hyperparams.json.
    # partial: num_regimes=2, lambda_risk=-2.0, q_g=0.001, q_u=0.001,
    #          r=0.001, feature_arch="linear", shared_dim=1, idio_dim=1
    # full:    num_regimes=2, lambda_risk=-2.0, q_g=0.001, q_u=0.001,
    #          r=1.0,   feature_arch="linear", shared_dim=1, idio_dim=1
    partial_overrides:
      shared_dim: 1
      idiosyncratic_dim: 1
      Q_g_scales: 0.001
      Q_u_scales: 0.001
      feature_arch: "linear"
      R_scalar: 0.001
    full_overrides:
      shared_dim: 1
      idiosyncratic_dim: 1
      Q_g_scales: 0.001
      Q_u_scales: 0.001
      feature_arch: "linear"
      R_scalar: 1.0

  # EM-capable correlated SLDS-IMM router: same base configuration as
  # slds_imm_corr, but with an EM-style learning phase over the initial
  # window t = 1,...,t_k. For the theoretical-trap experiment, Phase 1
  # occupies t ≈ 1..999 in regime 0, so we choose t_k = 999.
  slds_imm_corr_em:
    exploration_mode: "ids"
    feature_mode: "learnable"
    # Base learning rate used in Phase 0; Phase 1 uses a 10x smaller
    # rate by default unless overridden below, and Phase 2 defaults to
    # freezing features (feature_lr_phase2 = 0.0).
    feature_learning_rate: 0.001
    feature_arch: "linear"
    feature_hidden_dim: 4
    feature_activation: "tanh"

    shared_dim: 1
    idiosyncratic_dim: 1

    Q_g_scales: 0.001
    Q_u_scales: 0.001

    B_intercept_load: 1.0

    g_mean0: [0.0]
    g_cov0:
      - [1.0]
    u_mean0: [0.0]
    u_cov0:
      - [1.0]
    eps: 1.0e-8

    # EM-specific configuration and feature-learning schedule:
    #   - Phase 0: t in [1, phase0_t_end] with lr ≈ feature_lr_phase0
    #   - Phase 1: t in (phase0_t_end, em_tk] with lr ≈ feature_lr_phase1
    #   - Phase 2: t > em_tk or training_mode=False with lr ≈ feature_lr_phase2
    em_tk: 999
    em_min_weight: 1.0e-6
    em_verbose: true
    # Use the first half of the EM window for Phase 0 pretraining.
    phase0_t_end: 500
    # Explicit Phase 0/1/2 learning rates (optional; if omitted, defaults
    # inside SLDSIMMRouter_Corr_EM are used).
    feature_lr_phase0: 0.001   # pretraining
    feature_lr_phase1: 0.0001  # EM / SLDS training
    feature_lr_phase2: 0.00001     # deployment (features frozen)

    partial_overrides:
      shared_dim: 1
      idiosyncratic_dim: 1
      Q_g_scales: 0.001
      Q_u_scales: 0.001
      feature_arch: "linear"
      R_scalar: 0.001
    full_overrides:
      shared_dim: 1
      idiosyncratic_dim: 1
      Q_g_scales: 0.001
      Q_u_scales: 0.001
      feature_arch: "linear"
      R_scalar: 1.0

  staleness_threshold: null

baselines:
  l2d:
    arch: "mlp"
    # Very small learning rate so that the stale prior on Expert 1 from
    # Phase 1 is not quickly overwritten by a few bad pulls in Phase 3.
    learning_rate: 0.0005
    hidden_dim: 8
    window_size: 1
    alpha: 1.0
    beta: null

  l2d_sw:
    arch: "rnn"
    # Similarly slow adaptation for the sliding-window baseline.
    learning_rate: 0.0005
    hidden_dim: 8
    window_size: 200
    alpha: 1.0
    beta: null

  linucb:
    # Strong optimism so that the (stale) historical mean for Expert 1
    # dominates when it returns, triggering the trap.
    alpha_ucb: 100.0
    # Strong ridge prior so that a few bad pulls in Phase 3 do not
    # immediately erase the excellent Phase-1 history for Expert 1.
    lambda_reg: 100.0

  neural_ucb:
    # NeuralUCB baseline: we mirror the stale-prior regime by using
    # strong optimism and a slow-moving embedding.
    alpha_ucb: 10.0
    lambda_reg: 100.0
    hidden_dim: 16
    nn_learning_rate: 0.0001

horizon_planning:
  # Starting time index for horizon planning in slds_imm_router.py
  t0: 800

  # Planning horizon length
  H: 20
  # Planning method for horizon scheduling:
  #   - "regressive": original router-influenced context planning
  #   - "monte_carlo": scenario-based staffing planning with N=1
  #   - "{N}_monte_carlo": N-scenario Monte Carlo staffing planning
  method: "20_monte_carlo"

  # Scenario generator parameters for Monte Carlo planning.
  # Gaussian AR(1) feature-space perturbation as in
  # Section~\ref{sec:staffing-generator-gaussian}.
  scenario_generator:
    type: "gaussian_ar1"
    # AR(1) coefficient rho in [-1,1).
    rho: 0.9
    # Initial residual covariance Σ_0 ≈ (sigma0^2 I).
    sigma0: 0.1
    # Innovation covariance Q ≈ (q_scale^2 I).
    q_scale: 0.1
