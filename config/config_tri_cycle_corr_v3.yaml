# Tri-cycle correlation experiment (synthetic) - v3 (shared-factor favored).

plot_shift: 1
plot_target: "y"

transition_log:
  enabled: true
  print: false
  precision: 4
  stride: 1
  print_wlin_blin: true
  plot: true
  plot_show: true
  plot_save: false
  plot_na: true
  online_only: true
  plot_only_ours: true
  plot_entropy: true

environment:
  data_source: synthetic
  setting: tri_cycle_corr
  T: 3000
  seed: 7

  # Dimensions
  num_experts: 5
  num_regimes: 3
  state_dim: 1

  # Lower target noise; residuals dominated by shared expert noise.
  noise_scale: 0.12

  # Regime cycle and expert behavior configuration.
  tri_cycle:
    regime_pattern: [0, 1, 2, 1, 0, 2]
    # Shorter blocks increase non-stationarity and favor IMM structure.
    regime_block_len: 200
    drift_levels: [0.0, 1.0, 2.0]
    # Strong shared correlation, moderate idiosyncratic noise.
    shared_noise_scale: 0.8
    indiv_noise_scale: 0.08
    base_slope: 0.8
    # Make the correlated pair best but not dominant (noisy routing).
    biases_by_regime:
      - [0.3, 1.2, 1.1, 1.3, 1.6]
      - [1.0, 1.2, 0.4, 0.6, 1.6]
      - [0.4, 1.3, 1.0, 1.2, 0.6]

  #SHould we also update a newly entered experts? it seems intuitive to do so no?

routers:
  beta: 0.0
  staleness_threshold: null


  factorized_slds:
    enabled: true
    num_regimes: 3
    shared_dim: 1
    idiosyncratic_dim: 1
    delta_max: 10000 # keep 1000 if no pruning
    R_scalar: 0.5
    Q_g_scales: [0.05, 0.05, 0.05]
    Q_u_scales: [0.02, 0.02, 0.02]
    # Exploration mode for partial feedback:
    #   - "g": IDS using only shared-factor information (current default)
    #   - "g_z": IDS using joint (z, g) information
    #   - "ucb": Bayes-UCB (LCB) on predictive costs
    #   - "sampling": posterior sampling on predictive costs
    exploration: ["g_z"]
    # Diagnostic: log I(z; e) estimates to verify g vs g_z differences.
    exploration_diag_enabled: true
    exploration_diag_stride: 200
    exploration_diag_samples: 50
    exploration_diag_print: true

    em_enabled: true
    # Offline EM uses full feedback even for partial routers.
    em_offline_full_feedback: true
    em_tk: 1500
    em_n: 2
    em_samples: 10
    em_burn_in: 20
    # Offline EM validation.
    em_use_validation: true
    # Validation strategy: "tail" or "rolling".
    em_val_strategy: "rolling"
    # Size of the validation tail (fraction of em_tk).
    em_val_fraction: 0.4
    # Rolling-origin window length (steps). Falls back to em_val_len if null.
    em_val_roll_len: 200
    # Number of rolling validation windows in the tail.
    em_val_roll_splits: 3
    # Optional stride between rolling windows; null -> evenly spaced.
    em_val_roll_stride: null
    em_val_len: null
    em_theta_lr: 0.01
    em_theta_steps: 10
    em_print_val_loss: true
    em_eps_n: 1.0e-3

    # Online (sliding-window) EM adaptation.
    em_online_enabled: false
    em_online_window: 400
    em_online_period: 300
    em_online_n: 1
    em_online_samples: 20
    em_online_burn_in: 3
    em_online_theta_lr: 0.001
    em_online_theta_steps: 1
    em_online_eps_n: 1.0e-3
    em_online_print_val_loss: true
    # If null, defaults to em_tk + 1.
    em_online_start_t: null

    transition_mode: "linear"
    # Parallelize full pipelines per exploration mode.
    parallel_pipeline: true
    parallel_pipeline_workers: 4
    parallel_pipeline_disable_plot_show: true

baselines:
  l2d:
    arch: "mlp"
    learning_rate: 0.001
    hidden_dim: 16
    window_size: 1
    alpha: 1.0
    beta: null

  l2d_sw:
    arch: "rnn"
    learning_rate: 0.001
    hidden_dim: 16
    window_size: 200
    alpha: 1.0
    beta: null

  linucb:
    alpha: 1.0
    regularization: 1.0

  neuralucb:
    hidden_dim: 32
    lr: 0.001
    lambda_reg: 1.0

horizon_planning:
  # Start horizon planning after EM warm-start.
  t0: 2200

  H: 10
  method: "20_monte_carlo"

  # Coverage tolerance for active-set scheduling.
  delta: 0.1

  scenario_generator:
    type: "gaussian_ar1"
    rho: 0.9
    sigma0: 0.1
    q_scale: 0.1
    noise_scale: 1.0
