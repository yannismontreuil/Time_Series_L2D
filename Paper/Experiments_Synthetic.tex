\section{Synthetic Data Generation Process}
\label{sec:synthetic_generation}

We construct a two-regime synthetic environment designed to match the factorized
SLDS observation model used in our router. The environment is parameterized by
the configuration in \texttt{config/exp\_synthetic\_1.yaml} and yields a fully
specified probabilistic data-generating process.

\paragraph{Regime process.}
Let $z_t \in \{0,1\}$ denote the regime at time $t$. We use a deterministic
periodic schedule with block length $L$:
\begin{equation}
z_t = \left\lfloor \frac{t}{L} \right\rfloor \bmod 2,
\qquad t=0,1,\dots,T-1.
\label{eq:synthetic_regime}
\end{equation}
This produces a repeating $0 \to 1 \to 0 \to 1$ pattern with fixed dwell times.

\paragraph{Context and target process.}
The latent target follows a regime-dependent AR(1):
\begin{equation}
y_t = 0.8\, y_{t-1} + d_{z_t} + \eta_t,
\qquad \eta_t \sim \mathcal{N}(0,\sigma_y^2),
\label{eq:synthetic_y}
\end{equation}
with $y_0 = 0$ and drift levels $d_0,d_1$ specified by \texttt{drift\_levels}.
The context is the one-step lag,
\begin{equation}
x_t = y_{t-1}.
\label{eq:synthetic_x}
\end{equation}
We use $\phi(x_t)=x_t$ (scalar features) throughout.

\paragraph{Shared factor dynamics.}
We introduce a shared latent factor $g_t \in \mathbb{R}^{d_g}$ with
regime-conditioned linear dynamics:
\begin{equation}
g_t = A_g^{(z_t)} g_{t-1} + \xi_t,
\qquad \xi_t \sim \mathcal{N}(0, Q_g^{(z_t)}).
\label{eq:synthetic_g}
\end{equation}
In the experiment we set $d_g=2$ and choose $Q_g^{(0)}$ and $Q_g^{(1)}$
so that the first component dominates in regime~0 and the second dominates
in regime~1. This makes the shared factor regime-informative.

\paragraph{Idiosyncratic dynamics.}
Each expert has a regime-dependent latent idiosyncratic state $u_{t,k}$ that
evolves around a regime-specific mean $b_{z_t,k}$:
\begin{equation}
u_{t,k} = b_{z_t,k} + A_u^{(z_t)}\!\left(u_{t-1,k} - b_{z_{t-1},k}\right) + \nu_{t,k},
\qquad \nu_{t,k} \sim \mathcal{N}(0, Q_u^{(z_t)}).
\label{eq:synthetic_u}
\end{equation}
This yields stable, slowly varying expert-specific biases whose means shift
with the regime.

\paragraph{Observation model and expert predictions.}
We generate a signed residual for each expert $k$,
\begin{equation}
e_{t,k} = \phi(x_t)^\top \!\left(B_k g_t + u_{t,k}\right) + \varepsilon_{t,k},
\qquad \varepsilon_{t,k} \sim \mathcal{N}(0, R),
\label{eq:synthetic_residual}
\end{equation}
and define the expert prediction by
\begin{equation}
\hat{y}_{t,k} = y_t - e_{t,k}.
\label{eq:synthetic_prediction}
\end{equation}
The expert performance is therefore controlled by the regime-specific means
$b_{z_t,k}$ and the shared correlation structure $B_k g_t$.

\paragraph{Parameter mapping.}
The configuration parameters map directly to the model:
\begin{align}
&L = \texttt{tri\_cycle.regime\_block\_len}, \qquad
d_m = \texttt{tri\_cycle.drift\_levels}[m], \\
&A_g^{(m)} = \texttt{routers.factorized\_slds.A\_g}[m], \qquad
Q_g^{(m)} = \texttt{tri\_cycle.g\_covs}[m], \\
&A_u^{(m)} = \texttt{routers.factorized\_slds.A\_u}[m], \qquad
Q_u^{(m)} = \texttt{tri\_cycle.u\_covs}[m], \\
&B_k = \texttt{tri\_cycle.shared\_loadings}[k]^\top, \qquad
b_{m,k} = \texttt{tri\_cycle.biases\_by\_regime}[m][k], \\
&R = \texttt{tri\_cycle.eps\_noise\_scale}^2, \qquad
\sigma_y^2 = \texttt{environment.noise\_scale}^2.
\end{align}
Regime~0 is constructed so that experts $\{0,1\}$ have small $|b_{0,k}|$ and
are best; regime~1 is constructed so that experts $\{2,3\}$ have small $|b_{1,k}|$
and are best.

\section{Model--Environment Alignment}
\label{sec:synthetic_alignment}

The factorized SLDS model assumes residuals of the form
\begin{equation}
e_{t,k} = \phi(x_t)^\top(B_k g_t + u_{t,k}) + \varepsilon_{t,k},
\label{eq:model_residual}
\end{equation}
with regime-conditioned linear dynamics for $g_t$ and $u_{t,k}$. The synthetic
environment implements exactly \eqref{eq:synthetic_g}--\eqref{eq:synthetic_residual}
with $\phi(x_t)=x_t$, $d_g=2$, and the same $(A_g^{(m)}, Q_g^{(m)})$ and
$(A_u^{(m)}, Q_u^{(m)})$ used by the model. Thus the model family contains the
true data-generating process.

\paragraph{Regime-dependent correlations.}
Let $B_k \in \mathbb{R}^{1 \times 2}$ be the shared loading for expert $k$.
In regime~0 we set $Q_g^{(0)} = \mathrm{diag}(\sigma_{g1}^2, \sigma_{g2}^2)$
with $\sigma_{g1}^2 \gg \sigma_{g2}^2$ and $B_0=B_1=[1,0]$, $B_2=B_3=[0,1]$.
Then the conditional covariance of residuals is
\begin{equation}
\mathrm{Cov}(e_{t,0}, e_{t,1} \mid z_t=0, x_t)
  = x_t^2\, B_0 Q_g^{(0)} B_1^\top
  = x_t^2\, \sigma_{g1}^2 > 0,
\label{eq:cov_reg0}
\end{equation}
while $\mathrm{Cov}(e_{t,2}, e_{t,3}\mid z_t=0,x_t) \approx 0$.
In regime~1 we swap the active component via $Q_g^{(1)}$, yielding
correlation in experts $\{2,3\}$ and near-zero correlation in $\{0,1\}$.
This exactly matches the SLDS structure with regime-specific $Q_g^{(m)}$.

\paragraph{Mapping to model terms.}
\begin{align}
\text{Environment shared term} &= x_t\, B_k g_t \quad \longleftrightarrow \quad
\text{Model } B_k g_t, \\
\text{Environment idiosyncratic term} &= x_t\, u_{t,k} \quad \longleftrightarrow \quad
\text{Model } u_{t,k}, \\
\text{Environment regime blocks} &= z_t \text{ in \eqref{eq:synthetic_regime}}
  \quad \longleftrightarrow \quad \text{Model regime process}.
\end{align}
Because the environment is generated by the same linear-Gaussian structure,
the SLDS with $d_g=2$ can represent it exactly.

\paragraph{Why the no-$g_t$ baseline fails.}
Setting $g_t \equiv 0$ forces residuals to be conditionally independent across
experts:
\begin{equation}
\mathrm{Cov}(e_{t,i}, e_{t,j} \mid z_t, x_t) = 0, \quad i \neq j.
\label{eq:nog_cov}
\end{equation}
The environment violates \eqref{eq:nog_cov} by construction
\eqref{eq:cov_reg0}, so the no-$g_t$ model is misspecified.

\section{Why This Favors SLDS Over Baselines}
\label{sec:synthetic_baseline}

\paragraph{UCB-Linear.}
LinUCB treats each expert independently and does not model shared latent factors
or regime dynamics. When regimes switch, it must relearn which expert is best
purely from observed losses, and it cannot exploit the cross-expert covariance
induced by $g_t$.

\paragraph{UCB-Neural.}
NeuralUCB can represent nonlinear mappings of $x_t$ to losses but still lacks
an explicit latent regime state and shared-factor coupling across experts.
It reacts to switches only after sufficient loss observations and therefore
adapts more slowly than an IMM/Kalman filter.

\paragraph{L2D / L2D-SW.}
Learning-to-defer methods optimize a deferral policy from loss history but do
not infer a latent regime or a shared factor. The sliding-window variant still
lags regime changes and cannot exploit the correlated residual structure.

\paragraph{SLDS without $g_t$.}
The no-$g_t$ model cannot represent the regime-dependent cross-expert
correlations and thus underestimates uncertainty in the bad experts after
switches. This leads to slower reallocation of probability mass to the correct
expert pair.

\section{Expected Performance Characteristics}
\label{sec:synthetic_expected}

Let $c_{t,k} = \ell_{t,k} + \beta_k$ denote the cost of querying expert $k$.
Using the residual model \eqref{eq:synthetic_residual}, the expected squared
loss in regime $m$ is
\begin{equation}
\mathbb{E}[\ell_{t,k} \mid z_t=m, x_t]
  = x_t^2\left(
      b_{m,k}^2
      + B_k Q_g^{(m)} B_k^\top
      + Q_u^{(m)}
    \right)
  + R,
\label{eq:expected_cost}
\end{equation}
so the best expert in regime $m$ is the one with the smallest $|b_{m,k}|$ and
loading variance. By construction, experts $\{0,1\}$ minimize
\eqref{eq:expected_cost} in regime~0, and experts $\{2,3\}$ do so in regime~1.

\paragraph{Oracle.}
The oracle chooses $k^*(m) = \arg\min_k \mathbb{E}[\ell_{t,k} \mid z_t=m]$ and
achieves the minimum expected cost per regime.

\paragraph{Random.}
A uniform policy selects each expert with probability $1/4$, yielding
\begin{equation}
\mathbb{E}[c_t^{\text{rand}} \mid z_t=m] =
\frac{1}{4}\sum_{k=0}^3 \mathbb{E}[\ell_{t,k} \mid z_t=m] + \bar{\beta}.
\end{equation}

\paragraph{SLDS.}
With sufficient warmup, the SLDS posterior concentrates on the correct regime
and estimates $g_t$ and $u_{t,k}$ by Kalman filtering, driving the selected
expert toward the oracle within each block. Its regret is therefore dominated
by switch transitions rather than steady-state error.

\paragraph{Baselines.}
Bandit baselines adapt only after observing losses and cannot exploit the
shared-factor covariance. This yields higher transient regret after each
switch and a higher steady-state loss than the SLDS model.

\paragraph{Context-scenario generator for scheduling.}
When evaluating the predictive scheduling layer on synthetic data, we use a
strong scenario generator that is intentionally close to the ground-truth
context dynamics. Because the context is $x_t=y_{t-1}$ and $y_t$ follows an
AR(1) with regime-dependent drift, we generate future contexts by simulating
the same AR(1) form with coefficient fixed to the true value ($0.8$) and
drift levels estimated from the observed prefix. In the tri-cycle setting the
regime schedule is deterministic, so we reuse that schedule for forecasting.
The process-noise scale is matched to the empirical residual variance (with
the scaling specified in the config), yielding scenarios that are close to
the ground-truth distribution while respecting the no-feedback constraint
during planning.

\paragraph{Planning diagnostics and how to read them.}
We store all planning figures in \texttt{out/tri\_cycle\_corr/planning/} and
use separate plots to isolate interpretability and performance:
\begin{itemize}
    \item \textbf{Context scenario fan chart.} The plot overlays the true
    horizon context $x_{t+1:t+H}$ with multiple Monte Carlo scenario paths,
    plus the median and 10--90\% band. This makes the sampling uncertainty
    explicit and shows whether the generator is calibrated to the true
    context dynamics.
    \item \textbf{Demand heatmap with relative-threshold overlay.} For each horizon
    step $h$, we visualize the estimated demand $\hat{\rho}_{t,h}(k)$ over
    experts (heatmap) and overlay the active-set members as outlined squares.
    This directly shows how the planner translates stochastic demand into a
    relative-thresholded on-call set (relative to the max probability at each step) and how that differs across models (e.g., L2D SLDS vs
    the no-$g_t$ ablation).
    \item \textbf{Relative-thresholded mass curve.} The curve plots
    $\sum_{k\in\widehat{\mathcal{S}}_{t,h}(\delta)} \hat{\rho}_{t,h}(k)$ for
    each $h$, i.e., how much probability mass is captured by the relative-thresholded
    set. Lower values indicate a sharp winner, while higher values indicate ties or
    flatter demand that keeps multiple experts on-call.
    \item \textbf{Active-set size over horizon.} This shows the capacity cost
    of the schedule by reporting $|\widehat{\mathcal{S}}_{t,h}(\delta)|$ for
    each $h$. Larger sets indicate greater uncertainty in the planner's
    predicted choices.
    \item \textbf{Scenario-wise planned expert raster.} Each row is a Monte
    Carlo scenario and each column a horizon step; colors indicate the expert
    selected by the planning rule. This makes discrete variability explicit
    and shows when multiple experts are plausible under the forecast.
    \item \textbf{Planning cost bar chart.} We compare mean true horizon cost
    for the MC planner (L2D SLDS), its no-$g_t$ ablation, a strong
    deterministic point-forecast planner (same model, $N{=}1$ scenario), and
    a static top-$K$ on-call baseline (best fixed set with $K$ matched to the
    MC average set size). This isolates the benefit of modeling demand
    uncertainty rather than committing to a single forecast or a fixed pool.
\end{itemize}

\section{Diagnostics for the Tri-Cycle Correlation Experiment}
\label{sec:synthetic_diagnostics}

We report a suite of diagnostic plots (saved under
\texttt{out/tri\_cycle\_corr/}) that verify whether each model recovers the
intended regime structure, cross-expert correlations, and selection behavior.
Each plot is defined from explicit statistics of the synthetic process and the
model posterior. Below we state the quantity visualized and its interpretation.

\paragraph{Residual correlation heatmaps (\texttt{corr\_heatmaps.pdf}).}
For each regime $m$, we compute the empirical residuals
$e_{t,k} = \hat{y}_{t,k} - y_t$ and the regime-restricted correlation matrix
\begin{equation}
R^{(m)}_{ij}
= \frac{\mathrm{Cov}(e_{t,i}, e_{t,j} \mid z_t = m)}
{\sqrt{\mathrm{Var}(e_{t,i} \mid z_t=m)\,\mathrm{Var}(e_{t,j} \mid z_t=m)}},
\label{eq:diag_corr_true}
\end{equation}
estimated by sample covariance over all $t$ with $z_t=m$. The ``estimated''
heatmap uses the model's posterior predictive correlations implied by the
factorized SLDS, averaged over the same regime indices. When available, the
``no-$g_t$'' panel shows the correlations predicted by the ablated model,
which should be near zero off-diagonal due to \eqref{eq:nog_cov}. Agreement
between the true and estimated heatmaps validates that the model captures the
regime-dependent correlation structure induced by $g_t$.

\paragraph{Correlation tracking for expert pairs (\texttt{corr\_pairs.pdf}).}
For selected pairs $(i,j)$ we plot the posterior predictive correlation
\begin{equation}
\widehat{\rho}_{t,ij}
= \mathrm{Corr}(e_{t,i}, e_{t,j} \mid \mathcal{F}_{t-1}),
\end{equation}
where $\mathcal{F}_{t-1}$ is the information available prior to observing time
$t$. The dashed curve shows the true regime-specific correlation
$\rho_{z_t,ij}$; vertical lines mark regime switches. This plot confirms both
the correct regime dependence and the adaptation speed after switches.

\paragraph{Regime posterior (\texttt{regime\_posterior.pdf}).}
The upper panel visualizes the filtering posterior
$w_{t,m} = p(z_t=m \mid \mathcal{F}_{t-1})$, while the lower panel plots the
true regime $z_t$. High posterior mass on the true regime indicates that the
IMM filter correctly infers the latent switching process.

\paragraph{Shared-factor recovery (\texttt{g\_recovery.pdf}).}
We compare the inferred $g_t$ (aligned by permutation/sign to the true
trajectory) against the ground-truth shared factor from
\eqref{eq:synthetic_g}. This directly tests whether the model recovers the
shared latent dynamics that create cross-expert correlations. Alignment is
necessary because linear-Gaussian latent factors are identifiable only up to
permutation and sign.

\paragraph{Selection frequency by regime (\texttt{expert\_structure.pdf}).}
Let $\pi_{m,k}$ denote the fraction of times expert $k$ is selected when
$z_t=m$. The heatmap reports
\begin{equation}
\pi_{m,k}
= \frac{1}{|\{t : z_t=m\}|}\sum_{t: z_t=m} \mathbb{I}[r_t=k],
\end{equation}
where $r_t$ is the chosen expert. This plot checks whether the router learns
the correct regime-specific expert ordering implied by
\eqref{eq:expected_cost}. In the constructed environment, experts
$\{0,1\}$ should dominate in regime~0 and $\{2,3\}$ in regime~1.

\paragraph{Selection frequency by regime for all baselines
(\texttt{expert\_structure\_all.pdf}).}
The same statistic $\pi_{m,k}$ is shown for every baseline and router used in
\texttt{config/exp\_synthetic\_1.yaml}, grouped by feedback type. This figure
isolates whether each method discovers the correct regime-expert mapping, and
highlights the failure modes of baselines that do not model shared factors or
regimes. Only methods actually instantiated by the config are plotted.

\paragraph{Transfer probe under expert unavailability
(\texttt{transfer\_probe.pdf}).}
We temporarily remove expert $1$ and track its predicted loss while updating
on other experts. The main curve reports the posterior mean predicted loss for
expert $1$,
\begin{equation}
\widehat{\ell}_{t,1}^{\text{post}}
= \mathbb{E}\!\left[\ell_{t,1} \mid \mathcal{F}_t\right],
\end{equation}
computed after the belief update at time $t$, alongside the true loss
$\ell_{t,1} = (\hat{y}_{t,1}-y_t)^2$. The shaded interval indicates when expert
$1$ is unavailable. Vertical markers indicate selections of the correlated
expert, testing whether those observations propagate through $g_t$ to update
the belief about expert $1$. The lower panel shows the update magnitude
$|\widehat{\ell}_{t,1}^{\text{post}}-\widehat{\ell}_{t,1}^{\text{pre}}|$,
demonstrating how much the posterior belief changes after each observation.
