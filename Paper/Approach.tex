\section{Context-Aware Routing in Non-Stationary Environments}

\subsection{Problem Formulation}
\label{sec:preliminaries}

We study sequential expert routing over a finite horizon \(t\in [T]\coloneqq\{1,\dots,T\}\) with
\emph{bandit feedback on expert predictions (equivalently, per-expert costs)}: at each round, the
router observes the target \(y_t\) but only the queried expert's prediction; since the fee schedule is
known, this is equivalent to observing only the queried expert's cost. Let
\((\Omega,\mathcal{F},\mathbb{P})\) be a probability space supporting all random variables. Let
\(\mathcal{Y}\subseteq\mathbb{R}\) denote the target space.

\paragraph{Interaction protocol.}
The process unfolds in discrete rounds. At each time \(t\):
\begin{enumerate}
    \item \textbf{Context and availability:} the environment reveals a context vector
    \(\mathbf{x}_t \in \mathcal{X} \subseteq \mathbb{R}^d\) and a non-empty finite set of available
    experts \(\mathcal{E}_t \subseteq \mathcal{K}\), where \(\mathcal{K}\) is a universe of expert
    identities.
    \item \textbf{Action:} the router selects an expert index \(I_t \in \mathcal{E}_t\) using only
    decision-time information.
    \item \textbf{Feedback:} querying expert \(I_t\) returns a prediction \(\widehat{y}_{t,I_t}\);
    the environment then reveals the target \(y_t \in \mathcal{Y}\).
    The router incurs a cost associated with selecting expert \(I_t\).
\end{enumerate}

\paragraph{Expert registry and potential outcomes.}
To accommodate time-varying availability, we maintain a cumulative \emph{expert registry}
\begin{equation}
    \mathcal{K}_t \coloneqq \mathcal{K}_{t-1} \cup \mathcal{E}_t, \qquad \mathcal{K}_0=\varnothing .
\end{equation}
Distinct indices correspond to distinct persistent experts.
Each expert \(k\in\mathcal{K}\) implements an (unknown) prediction rule
\(f_k:\mathcal{X}\to\mathcal{Y}\), accessible only through queries.
We define the \emph{potential prediction} of expert \(k\) at time \(t\) as
\(\widehat{y}_{t,k}\coloneqq f_k(\mathbf{x}_t)\).
Due to bandit feedback, \(\widehat{y}_{t,k}\) is \emph{censored}: the router observes
\(\widehat{y}_{t,k}\) if and only if \(I_t=k\). For \(k\neq I_t\), \(\widehat{y}_{t,k}\) remains
counterfactual.

\paragraph{Residuals, loss, and (potential) cost.}
Define the (potential) realized residual of expert \(k\) at time \(t\) as
\begin{equation}
\label{eq:realized_residual}
    e_{t,k} \coloneqq \widehat{y}_{t,k}-y_t ,
\end{equation}
which is observed only for the queried expert \(k=I_t\).
The corresponding (potential) realized cost is
\begin{equation}
\label{eq:realized_cost}
    C_{t,k} \coloneqq \psi(e_{t,k}) + \beta_k ,
\end{equation}
where \(\psi:\mathbb{R}\to\mathbb{R}_{\ge 0}\) is a known convex loss (e.g., squared error) and
\(\beta_k\ge 0\) is a known, expert-specific query fee. The post-action feedback at round \(t\) is
$O_t \coloneqq (I_t,\widehat{y}_{t,I_t},y_t)$, which determines the observed cost \(C_{t,I_t}\).

\paragraph{Filtrations and policies.}
Let \(\mathcal{H}_{t}\coloneqq \big((\mathbf{x}_\tau,\mathcal{E}_\tau,O_\tau)\big)_{\tau=1}^{t}\)
be the interaction history up to the end of round \(t\).
Decisions are non-anticipative, i.e., made before observing \(O_t\).
We define the \emph{decision-time} sigma-algebra (information available before choosing \(I_t\)) as
\begin{equation}
    \mathcal{F}_t \coloneqq \sigma\!\left(\mathcal{H}_{t-1},\mathbf{x}_t,\mathcal{E}_t\right).
\end{equation}
A policy \(\pi=(\pi_t)_{t=1}^T\) is a sequence of decision rules where
\(\pi_t(\cdot \mid \mathcal{F}_t)\) is an \(\mathcal{F}_t\)-measurable distribution over
\(\mathcal{E}_t\).
The action is sampled as \(I_t \sim \pi_t(\cdot \mid \mathcal{F}_t)\), so that
\(I_t\in\mathcal{E}_t\) almost surely.

\paragraph{Non-stationarity and exogeneity.}
We do not assume i.i.d.\ data. Instead, the environment follows a sequence of time-varying conditional
laws \(\{\mathcal{D}_t\}_{t\ge 1}\) such that
\begin{equation}
    (\mathbf{x}_t,\mathcal{E}_t,y_t)\mid \sigma\big((\mathbf{x}_\tau,\mathcal{E}_\tau,y_\tau)_{\tau<t}\big)
    \sim \mathcal{D}_t\!\left(\cdot \,\middle|\, (\mathbf{x}_\tau,\mathcal{E}_\tau,y_\tau)_{\tau<t}\right).
\end{equation}
This captures \emph{non-stationarity}: \(\mathcal{D}_t\) may vary with \(t\).
We additionally assume \emph{exogeneity} (non-reactivity to routing): past routing actions affect which
expert predictions are observed, but do not influence the data-generating process. Formally, for all
\(t\) and measurable sets \(A\),
\begin{equation}
\begin{aligned}
\mathbb{P}\!\left((\mathbf{x}_t,\mathcal{E}_t,y_t)\in A \,\middle|\,
\sigma\big((\mathbf{x}_\tau,\mathcal{E}_\tau,y_\tau)_{\tau<t}\big)\vee\sigma(I_{1:t-1})\right)
& =\\
\mathbb{P}\!\left((\mathbf{x}_t,\mathcal{E}_t,y_t)\in A \,\middle|\,
\sigma\big((\mathbf{x}_\tau,\mathcal{E}_\tau,y_\tau)_{\tau<t}\big)\right).
\end{aligned}
\label{eq:exogeneity}
\end{equation}

\paragraph{Objective and myopic Bayes selector.}
Our goal is to sequentially select an available expert \(k \in \mathcal{E}_t\) that balances accuracy
(low \(\psi(e_{t,k})\)) and cost (low \(\beta_k\)). Over the horizon \(t\in[T]\), we seek a policy
\(\pi\) minimizing the expected cumulative cost
\begin{equation}
    J(\pi) \coloneqq \mathbb{E}\!\left[\sum_{t=1}^T C_{t,I_t}\right]
    = \mathbb{E}\!\left[\sum_{t=1}^T \left(\psi(e_{t,I_t})+\beta_{I_t}\right)\right].
\end{equation}
Conditioned on the decision-time information \(\mathcal{F}_t\), the \emph{myopic Bayes selector} chooses an
index minimizing the posterior expected instantaneous cost:
\begin{equation}
\label{eq:bayes_selection}
    k_t^{\star} \in \arg\min_{k\in\mathcal{E}_t} \mathbb{E}\!\left[C_{t,k} \mid \mathcal{F}_t\right].
\end{equation}
Computing or approximating \eqref{eq:bayes_selection} requires a predictive model for the residuals
\((e_{t,k})_{k\in\mathcal{E}_t}\) under bandit censoring. In subsequent sections, we introduce a
latent-state model that yields a tractable one-step-ahead predictive belief.

\subsection{Generative Model: Factorized Switching LDS}
\label{sec:generative_model}

To obtain a tractable predictive model under non-stationarity and bandit feedback, we model the
\emph{potential residuals} \(e_{t,k}=\widehat{y}_{t,k}-y_t\) from Section~\ref{sec:preliminaries}
with a \textbf{factorized switching linear dynamical system (SLDS)}. The central bottleneck is
censoring: at round \(t\) we observe only the queried residual
\(
e_t \coloneqq e_{t,I_t},
\)
while \((e_{t,k})_{k\neq I_t}\) remain counterfactual. If each expert were endowed with fully
independent dynamics, then unqueried experts would be propagated essentially ``open-loop'' and their
posterior uncertainty would not contract (and can increase under unstable dynamics). We address this
by decomposing expert reliability into (i) a \emph{shared} global factor \(\mathbf{g}_t\) that couples
experts and enables information transfer across them, and (ii) \emph{idiosyncratic} expert-specific
dynamics \(\mathbf{u}_{t,k}\).

At decision time (before querying any expert at round \(t\)), the model induces a one-step-ahead
predictive distribution \(p(e_{t,k}\mid \mathcal{F}_t)\) for each \(k\in\mathcal{E}_t\). For clarity,
we write \(e_{t,k}^{\mathrm{pred}}\) for the \emph{hypothetical} residual that would be observed if
expert \(k\) were queried at time \(t\); thus
\(e_{t,k}^{\mathrm{pred}}\sim p(e_{t,k}\mid \mathcal{F}_t)\) (and, when needed, conditional on \(z_t\)).
This predictive law marginalizes over latent-state uncertainty and is distinct from the conditional
emission in \eqref{eq:residual_emission}.

\subsubsection{Latent state hierarchy}
We represent non-stationarity via a two-level hierarchy separating systemic shifts from
expert-specific drifts.

\paragraph{Context-dependent regime switching.}
A discrete regime \(z_t\in\{1,\dots,M\}\) selects the active dynamical law (e.g., ``bull'' vs.\ ``crisis'').
While classical SLDSs often use a time-homogeneous transition matrix, we allow transitions to depend
on the observed context \(\mathbf{x}_t\), enabling the model to update its regime belief using
exogenous signals before observing the queried residual \(e_t\). We model
\[
\mathbb{P}(z_t=m \mid z_{t-1}=\ell,\mathbf{x}_t)=\Pi_\theta(\mathbf{x}_t)_{\ell m}.
\]
We parameterize the \emph{logits} of \(\Pi_\theta(\mathbf{x}_t)\) via a low-rank attention map.
Specifically, we compute \(Q_\theta(\mathbf{x}_t),K_\theta(\mathbf{x}_t)\in\mathbb{R}^{M\times d_{\mathrm{attn}}}\) and set
\[
S(\mathbf{x}_t)\coloneqq \frac{1}{\sqrt{d_{\mathrm{attn}}}}\,Q_\theta(\mathbf{x}_t)K_\theta(\mathbf{x}_t)^\top,
\]
so that \(\mathrm{rank}(S(\mathbf{x}_t))\le d_{\mathrm{attn}}\). Row \(\ell\) corresponds to the
previous regime and column \(m\) to the next regime, and the row-wise softmax yields a valid
transition matrix:
\begin{equation}
\label{eq:context_transitions}
\mathbb{P}(z_t=m \mid z_{t-1}=\ell,\mathbf{x}_t)
=
\frac{\exp(S_{\ell m}(\mathbf{x}_t))}{\sum_{j=1}^M \exp(S_{\ell j}(\mathbf{x}_t))}.
\end{equation}

\paragraph{Extension: Flexible regime switching (optional).}
The context-dependent transition \eqref{eq:context_transitions} does not condition on the model's
continuous belief state (e.g., the current posterior over \(\mathbf{g}_{t-1}\)). One may instead
consider a recurrent variant in which the transition logits depend on \(\mathbf{g}_{t-1}\), e.g.,
\[
\mathbb{P}(z_t=m \mid z_{t-1}=\ell,\mathbf{x}_t,\mathbf{g}_{t-1})
\propto
\exp\!\big([f_\theta(\mathbf{x}_t)]_{\ell m} + \mathbf{w}_{\ell m}^\top \mathbf{g}_{t-1}\big),
\]
with \(\mathbf{w}_{\ell m}\in\mathbb{R}^{d_g}\).
However, this breaks conjugacy because the filtering belief for \(\mathbf{g}_{t-1}\) is Gaussian
while the transition involves a softmax, requiring additional approximations. In this work we focus
on \eqref{eq:context_transitions} for tractable inference.

\paragraph{Global factor dynamics.}
We introduce a continuous latent state \(\mathbf{g}_t\in\mathbb{R}^{d_g}\) to capture shared,
time-varying structure affecting all experts. Under bandit feedback, only one expert is queried per
round, so learning fully independent expert models is statistically inefficient and cannot exploit
cross-expert correlations. The shared factor \(\mathbf{g}_t\) enables \emph{information transfer}:
observing \(e_t=e_{t,I_t}\) updates the posterior over \(\mathbf{g}_t\), which in turn refines the
predictive belief over \(e_{t,k}^{\mathrm{pred}}\) for unqueried experts \(k\neq I_t\).
Conditioned on \(z_t=m\), the global factor follows linear-Gaussian dynamics:
\begin{equation}
\label{eq:global_dynamics}
\mathbf{g}_t
=
\mathbf{A}^{(g)}_{m}\mathbf{g}_{t-1}+\mathbf{w}^{(g)}_{t},
\qquad
\mathbf{w}^{(g)}_{t}\sim\mathcal{N}(\mathbf{0},\mathbf{Q}^{(g)}_{m}),
\end{equation}
where \(\mathbf{A}^{(g)}_{m}\in\mathbb{R}^{d_g\times d_g}\) and
\(\mathbf{Q}^{(g)}_{m}\in\mathbb{S}^{d_g}_{++}\).
We assume \((\mathbf{w}^{(g)}_{t})_{t}\) are independent across time and independent of all other
process and emission noise terms.

\paragraph{Expert-specific dynamics.}
Each expert \(k\) also has an idiosyncratic latent state
\(\mathbf{u}_{t,k}\in\mathbb{R}^{d_\alpha}\) capturing expert-specific drift not explained by the
shared factor. Conditioned on \(z_t=m\),
\begin{equation}
\label{eq:idiosyncratic_dynamics}
\mathbf{u}_{t,k}
=
\mathbf{A}^{(u)}_{m}\mathbf{u}_{t-1,k}+\mathbf{w}^{(u)}_{t,k},
\quad
\mathbf{w}^{(u)}_{t,k}\sim\mathcal{N}(\mathbf{0},\mathbf{Q}^{(u)}_{m}),
\end{equation}
where conditional on \((z_t)\), the noise terms are independent across experts and time with
\(\mathbf{w}^{(u)}_{t,k}\mid z_t=m\sim\mathcal{N}(\mathbf{0},\mathbf{Q}^{(u)}_{m})\). To maintain
statistical strength in the sparse data regime, we share
\((\mathbf{A}^{(u)}_{m},\mathbf{Q}^{(u)}_{m})\) across experts, while preserving expert-specific
behavior through expert-specific loadings \(\mathbf{B}_k\) in \eqref{eq:alpha_def}.

\paragraph{Reliability composition and residual emission.}
We capture time- and context-dependent expert performance through a latent \emph{reliability} vector
\(\boldsymbol\alpha_{t,k}\in\mathbb{R}^{d_\alpha}\), decomposed into a shared component driven by the
global factor and an expert-specific drift:
\begin{equation}
\label{eq:alpha_def}
\boldsymbol\alpha_{t,k}\coloneqq \mathbf{B}_k\mathbf{g}_t+\mathbf{u}_{t,k},
\qquad
\mathbf{B}_k\in\mathbb{R}^{d_\alpha\times d_g}.
\end{equation}
Here, \(\mathbf{B}_k\mathbf{g}_t\) captures how expert \(k\) responds to system-wide conditions, while
\(\mathbf{u}_{t,k}\) accounts for idiosyncratic effects not explained by \(\mathbf{g}_t\).

Given a regime \(z_t=m\) and latent states \((\mathbf{g}_t,\mathbf{u}_{t,k})\), we posit that the
(potential) residual \(e_{t,k}\) is linear in context features \(\phi(\mathbf{x}_t)\) with Gaussian
noise, where \(\phi:\mathcal{X}\to\mathbb{R}^{d_\alpha}\) is a fixed feature map (optionally including
a constant term):
\begin{equation}
\label{eq:residual_emission}
e_{t,k}\mid (z_t=m,\mathbf{g}_t,\mathbf{u}_{t,k},\mathbf{x}_t)
\sim
\mathcal{N}\!\big(\phi(\mathbf{x}_t)^\top\boldsymbol\alpha_{t,k},\,R_{m,k}\big).
\end{equation}
The mean \(\phi(\mathbf{x}_t)^\top\boldsymbol\alpha_{t,k}\) represents a systematic, context-dependent
bias term, while \(R_{m,k}>0\) captures irreducible noise. We assume emission noise is conditionally
independent across experts and time given \((z_t,\mathbf{g}_t,(\mathbf{u}_{t,k})_k)\). Under bandit
feedback, the router observes
only the realized draw from \eqref{eq:residual_emission} corresponding to the queried expert,
\(e_t\coloneqq e_{t,I_t}\), at round \(t\); all other \(e_{t,k}\) remain counterfactual.


\paragraph{Selective information transfer via factorization.}
In the exact Bayesian filter, incorporating the observation \(e_t\) induces dependence between the
shared factor \(\mathbf{g}_t\) and the idiosyncratic states \((\mathbf{u}_{t,k})_k\), and hence across
experts. For scalability, our inference maintains a \emph{factorized} approximation: after each
update, we project the filtering belief back to a family in which (conditional on \(z_t\)) the
idiosyncratic states are independent across experts and independent of \(\mathbf{g}_t\); see
Appendix~\ref{app:cross_covariance} for the full (non-factorized) update and why we avoid it.

\begin{restatable}[Information transfer under a shared factor]{proposition}{propinfo}
\label{prop:cross_update}
Fix \(t\) and \(z_t=m\). Condition on decision-time information \(\mathcal{F}_t\) and on the realized
action \(I_t\). Let \(j\neq I_t\) and let \((e_{t,j}^{\mathrm{pred}},e_{t,I_t}^{\mathrm{pred}})\) denote the one-step-ahead predictive
residuals under \(p(e_{t,\cdot}\mid \mathcal{F}_t,z_t=m)\). Assume this predictive distribution is
jointly Gaussian. Then
\[
\mathbb{E}\!\left[e_{t,j}^{\mathrm{pred}}\mid I_t,e_t,\mathcal{F}_t,z_t=m\right]
=
\mathbb{E}\!\left[e_{t,j}^{\mathrm{pred}}\mid I_t,\mathcal{F}_t,z_t=m\right]
\quad\Longleftrightarrow\quad
\mathrm{Cov}\!\left(e_{t,j}^{\mathrm{pred}},e_{t,I_t}^{\mathrm{pred}}\mid I_t,\mathcal{F}_t,z_t=m\right)=0.
\]
In particular, if the covariance is non-zero, then observing \(e_t=e_{t,I_t}\) updates the posterior
predictive mean of \(e_{t,j}^{\mathrm{pred}}\).
\end{restatable}

We prove Proposition~\ref{prop:cross_update} in Appendix~\ref{app:proof_cross_update}. The transfer is
\emph{selective}: observing the queried residual affects unqueried experts exactly when their predictive
residuals are correlated. In our factorized SLDS, this correlation is induced by the shared factor
\(\mathbf{g}_t\); for example, conditional on \((\mathcal{F}_t,z_t=m)\),
\(\mathrm{Cov}(e_{t,j}^{\mathrm{pred}},e_{t,i}^{\mathrm{pred}})\) contains the shared-factor term
\[
\phi(\mathbf{x}_t)^\top \mathbf{B}_j \Sigma^{(m)}_{g,t\mid t-1}\mathbf{B}_{i}^\top \phi(\mathbf{x}_t),
\]
where \(\Sigma^{(m)}_{g,t\mid t-1}\) is the one-step predictive covariance of \(\mathbf{g}_t\)
under regime \(m\). Thus, querying \(i=I_t\) tightens the predictive distribution of expert \(j\)
whenever the coupling through \(\mathbf{g}_t\) is non-negligible in the directions probed by
\(\phi(\mathbf{x}_t)\). Conversely, if this term is zero (and the idiosyncratic channel is independent),
then under the factorized predictive belief there is no information transfer from \(I_t\) to \(j\) at
time \(t\).

\subsubsection{Exploration via Information-Directed Sampling}
\label{sec:exploration}

Bandit feedback reveals only the residual of the queried expert, so the router must balance immediate
cost minimization with learning about the shared dynamics. We use
\textbf{Information-Directed Sampling (IDS)} to formalize this trade-off.

\paragraph{Predictive cost gap.}
At decision time, the model induces a one-step-ahead predictive distribution
\(p(e_{t,k}\mid \mathcal{F}_t)\) for each \(k\in\mathcal{E}_t\). We write
\[
e_{t,k}^{\mathrm{pred}} \sim p(e_{t,k}\mid \mathcal{F}_t)
\]
for a pre-query residual random variable distributed according to this predictive belief. If \(I_t=k\), the
realized observation \(e_t=e_{t,k}\) is a draw from this same law.

Define the corresponding predictive (virtual) cost random variable
\begin{equation}
\label{eq:virtual_cost_def}
C_{t,k}^{\mathrm{pred}}
\coloneqq
\psi(e_{t,k}^{\mathrm{pred}})+\beta_k,
\qquad k\in\mathcal{E}_t.
\end{equation}
Let the myopic baseline be
\begin{equation}
\label{eq:myopic_baseline_ids}
k_t^{\star} \in \arg\min_{k\in\mathcal{E}_t} \mathbb{E}\!\left[C_{t,k}^{\mathrm{pred}} \mid \mathcal{F}_t\right].
\end{equation}
The (predictive) value gap of choosing \(k\) is
\begin{equation}
\label{eq:model_gap}
\Delta_t(k)
\coloneqq
\mathbb{E}\!\left[ C_{t,k}^{\mathrm{pred}} - C_{t, k_t^\star}^{\mathrm{pred}} \,\middle|\, \mathcal{F}_t \right]
\ge 0,
\end{equation}
where nonnegativity follows from the definition of \(k_t^\star\).

\paragraph{Epistemic value: information gain about the joint latent state \((z_t,\mathbf{g}_t)\).}
In the factorized SLDS, non-stationarity is governed by the regime \(z_t\) and cross-expert
information transfer is mediated by the shared factor \(\mathbf{g}_t\)
(Proposition~\ref{prop:cross_update}). To accelerate adaptation under regime uncertainty, we score
expert \(k\) by the mutual information between the \emph{joint} latent state \((z_t,\mathbf{g}_t)\)
and the hypothetical residual:
\begin{equation}
\label{eq:ig_operational}
\mathrm{IG}_t(k)
\coloneqq
\mathcal{I}\!\left((z_t,\mathbf{g}_t);\, e_{t,k}^{\mathrm{pred}} \,\middle|\, \mathcal{F}_t\right).
\end{equation}
By the chain rule for mutual information, this decomposes as
\begin{equation}
\label{eq:ig_chain_rule}
\mathrm{IG}_t(k)
=
\underbrace{\mathcal{I}\!\left(z_t;\, e_{t,k}^{\mathrm{pred}} \,\middle|\, \mathcal{F}_t\right)}_{\text{mode-identification}}
+
\underbrace{\mathcal{I}\!\left(\mathbf{g}_t;\, e_{t,k}^{\mathrm{pred}} \,\middle|\, z_t, \mathcal{F}_t\right)}_{\text{shared-factor refinement}}.
\end{equation}
The first term measures how much observing the residual helps identify the current regime---crucial
for adapting to regime changes. The second term measures the reduction in uncertainty about the
shared factor, enabling information transfer across experts.

\paragraph{Closed-form shared-factor refinement.}
Fix a regime \(z_t=m\). Under the factorized one-step-ahead predictive belief, the conditional
predictive law is Gaussian:
\begin{equation}
\label{eq:channel_form}
e_{t,k}^{\mathrm{pred}} \mid (\mathbf{g}_t, z_t=m)
\sim
\mathcal{N}\!\big(h_{t,k}^\top \mathbf{g}_t + b^{(m)}_{t,k},\, s^{(m)}_{t,k}\big),
\end{equation}
where \(h_{t,k}\coloneqq \mathbf{B}_k^\top \phi(\mathbf{x}_t)\in\mathbb{R}^{d_g}\),
\(b^{(m)}_{t,k}\coloneqq \phi(\mathbf{x}_t)^\top \mu^{(m)}_{u,k,t\mid t-1}\), and
\begin{equation}
\label{eq:noise_var}
s^{(m)}_{t,k}
\coloneqq
\phi(\mathbf{x}_t)^\top \Sigma^{(m)}_{u,k,t\mid t-1}\phi(\mathbf{x}_t) + R_{m,k}.
\end{equation}
Let \(\Sigma^{(m)}_{g,t\mid t-1}\) denote the one-step predictive covariance of \(\mathbf{g}_t\) under
regime \(m\). The mode-conditioned shared-factor information gain admits the closed form
\begin{equation}
\label{eq:ig_mode}
\mathcal{I}\!\left(\mathbf{g}_t;e_{t,k}^{\mathrm{pred}}\mid \mathcal{F}_t,z_t=m\right)
=
\frac12\log\!\left(1+\frac{ h_{t,k}^\top \Sigma^{(m)}_{g,t\mid t-1} h_{t,k} }{s^{(m)}_{t,k}}\right).
\end{equation}
Averaging over the predictive regime weights \(\bar{w}_t^{(m)} \coloneqq \mathbb{P}(z_t=m\mid\mathcal{F}_t)\):
\begin{equation}
\label{eq:ig_g_mix}
\mathcal{I}\!\left(\mathbf{g}_t;e_{t,k}^{\mathrm{pred}}\mid z_t, \mathcal{F}_t\right)
=
\sum_{m=1}^M \bar{w}_t^{(m)}\,
\mathcal{I}\!\left(\mathbf{g}_t;e_{t,k}^{\mathrm{pred}}\mid \mathcal{F}_t,z_t=m\right).
\end{equation}

\paragraph{Mode-identification via Monte Carlo.}
The mode-identification term \(\mathcal{I}(z_t; e_{t,k}^{\mathrm{pred}} \mid \mathcal{F}_t)\) measures
how distinguishable the per-mode predictive distributions are. Since
\(p(e_{t,k}\mid\mathcal{F}_t)=\sum_m \bar{w}_t^{(m)}\mathcal{N}(\mu^{(m)}_{e,t,k}, v^{(m)}_{e,t,k})\)
is a Gaussian mixture (where \(\mu^{(m)}_{e,t,k}, v^{(m)}_{e,t,k}\) are the per-mode predictive mean
and variance), this term does not admit a closed form but can be estimated efficiently via Monte
Carlo; see Appendix for algorithmic details.

\paragraph{Minimizing the information ratio.}
IDS selects the routing action by minimizing the squared information ratio
\begin{equation}
\label{eq:ids_deterministic}
I_t \in \arg\min_{k\in\mathcal{E}_t}\ \frac{\Delta_t(k)^2}{\mathrm{IG}_t(k)}.
\end{equation}
If \(\mathrm{IG}_t(k)=0\), we interpret the ratio as \(+\infty\) unless \(\Delta_t(k)=0\); if all
\(\mathrm{IG}_t(k)=0\), IDS reduces to the myopic choice in \eqref{eq:myopic_baseline_ids}.
This criterion prioritizes experts that are either near-optimal (small \(\Delta_t(k)\)) or highly
informative about the latent state \((z_t,\mathbf{g}_t)\) (large \(\mathrm{IG}_t(k)\)), accelerating
adaptation to both regime changes and shared-factor drift without auxiliary queries.

\subsubsection{The IMM filtering recursion}
The IMM maintains $M$ mode-conditioned Gaussian filters over the shared factor \(\mathbf{g}_t\) and
the idiosyncratic states \((\mathbf{u}_{t,k})_{k\in\mathcal{K}^{\text{work}}_t}\), along with regime
weights. At each round $t$, given context $\mathbf{x}_t$, it performs:

\begin{enumerate}
\item \textbf{Interaction (mixing).}
Let $w_{t-1}^i=\mathbb{P}(z_{t-1}=i\mid\mathcal{H}_{t-1})$ be the posterior regime probability from the previous step, and let
$\Pi_{i\to j}(\mathbf{x}_t)=\mathbb{P}(z_t=j\mid z_{t-1}=i,\mathbf{x}_t)$ be the transition matrix (Eq~\ref{eq:context_transitions}).
We compute the \emph{predicted regime probability} \(\bar{w}_t^j=\mathbb{P}(z_t=j\mid\mathcal{F}_t)\):
\[
\bar{w}_t^j = \sum_{i=1}^M \Pi_{i\to j}(\mathbf{x}_t)w_{t-1}^i.
\]
The \emph{mixing weights} (probability that the previous mode was $i$, given the current mode is $j$) are:
\[
\mu_{t-1}^{i|j}=\frac{\Pi_{i\to j}(\mathbf{x}_t)w_{t-1}^i}{\bar{w}_t^j}.
\]
For each target mode $j$, we compute the mixed continuous-state prior for \(\mathbf{g}_{t-1}\) and all
\((\mathbf{u}_{t-1,k})_{k\in\mathcal{K}^{\text{work}}_{t-1}}\) by moment-matching the mixture defined
by weights $\{\mu_{t-1}^{i|j}\}_{i=1}^M$.

\item \textbf{Time update.}
Each mode $j$ propagates its mixed Gaussian through the regime-specific dynamics
(Eq~\ref{eq:global_dynamics}, \ref{eq:idiosyncratic_dynamics}) to form the one-step predictive belief.

\item \textbf{Measurement update.}
After selecting $I_t$, we observe the residual $e_t=e_{t,I_t}$ and perform a Kalman correction in each
mode using the observation model for expert \(I_t\), updating \(\mathbf{g}_t\) and \(\mathbf{u}_{t,I_t}\)
while leaving \((\mathbf{u}_{t,k})_{k\neq I_t}\) at their predictive marginals. This yields likelihoods
\(\mathcal{L}_t^{(m)}=p(e_t\mid \mathcal{F}_t,I_t,z_t=m)\) and updated posterior regime weights:
\[
w_t^m=\frac{\mathcal{L}_t^{(m)}\bar{w}_t^m}{\sum_{\ell=1}^M \mathcal{L}_t^{(\ell)}\bar{w}_t^\ell}.
\]

\item \textbf{Output.}
The resulting belief is the weighted mixture of the $M$ mode-conditioned Gaussians; predictive costs
and information scores are computed by mixture-averaging using the regime weights.
\end{enumerate}

\subsubsection{Dynamic Registry Management}
\label{sec:registry}

In production, experts can appear and disappear abruptly (e.g., models are deployed/deprecated or
external providers churn). Routers with a fixed output head typically require either
(i) over-provisioning a superset of experts with masking, or (ii) architectural changes followed by
retraining, both of which hinder real-time adaptability.

Our Factorized SLDS instead maintains a dynamically sized \emph{working registry} of experts for which
we explicitly store idiosyncratic filtering marginals. We distinguish the cumulative registry
\(\mathcal{K}_t=\mathcal{K}_{t-1}\cup \mathcal{E}_t\) (Section~\ref{sec:preliminaries}) from a prunable
working set \(\mathcal{K}^{\text{work}}_t\subseteq \mathcal{K}_t\), initialized as
\(\mathcal{K}^{\text{work}}_0=\varnothing\) and updated online.

\paragraph{Pruning (dropping stored idiosyncratic marginals).}
Let \(\tau_{\mathrm{last}}(k)\in\{0,1,\dots,t-1\}\) be the last round at which expert \(k\) was queried
(with the convention \(\tau_{\mathrm{last}}(k)=0\) if \(k\) has never been queried).
We call an expert \emph{stale} if it is currently unavailable and has not been queried for more than
\(\Delta_{\max}\) steps, where \(\Delta_{\max}\ge 1\) is a user-chosen staleness horizon:
\begin{equation}
\label{eq:stale_set}
\mathcal{K}^{\mathrm{stale}}_t
\coloneqq
\left\{k\in \mathcal{K}^{\text{work}}_{t-1}\setminus \mathcal{E}_t:\ t-\tau_{\mathrm{last}}(k)>\Delta_{\max}\right\}.
\end{equation}
We prune by updating
\(\mathcal{K}^{\text{work}}_t \leftarrow \mathcal{K}^{\text{work}}_{t-1}\setminus \mathcal{K}^{\mathrm{stale}}_t\).
Operationally, pruning means we stop storing the idiosyncratic filtering marginal(s) associated with
\(\mathbf{u}_{t-1,k}\) (and hence do not propagate it forward) for \(k\in\mathcal{K}^{\mathrm{stale}}_t\).

Crucially, pruning does \emph{not} alter the maintained belief over the retained variables: it is
exact marginalization of dropped coordinates in the filtering distribution.

\begin{restatable}[Pruning does not affect retained experts]{proposition}{invariance}
\label{prop:invariance}
Fix time \(t\) and let \(P_t \subseteq \mathcal{K}^{\text{work}}_{t-1}\) be any set of experts to be pruned.
Let
\(
q_{t-1\mid t-1}\big(\mathbf{g}_{t-1},(\mathbf{u}_{t-1,\ell})_{\ell\in\mathcal{K}^{\text{work}}_{t-1}}\big)
\)
denote the (exact or approximate) filtering belief at the end of round \(t-1\) conditioned on the realized history.
Define the pruned belief by marginalization:
\begin{equation*}
    \begin{aligned}
        q^{\mathrm{pr}(P_t)}_{t-1\mid t-1}\big(\mathbf{g}_{t-1},(\mathbf{u}_{t-1,\ell})_{\ell\in\mathcal{K}^{\text{work}}_{t-1}\setminus P_t}\big)
& \coloneqq \\
\int q_{t-1\mid t-1}\big(\mathbf{g}_{t-1},(\mathbf{u}_{t-1,\ell})_{\ell\in\mathcal{K}^{\text{work}}_{t-1}}\big)\,
\prod_{k\in P_t} d\mathbf{u}_{t-1,k}.
    \end{aligned}
\end{equation*}
Then \(q^{\mathrm{pr}(P_t)}_{t-1\mid t-1}\) equals the marginal of \(q_{t-1\mid t-1}\) on the retained variables.
Consequently, after applying the standard SLDS time update to obtain the predictive belief at round \(t\),
the predictive distribution of \(\boldsymbol\alpha_{t,\ell}\) and the one-step predictive law of
\(e_{t,\ell}^{\mathrm{pred}}\) are identical before and after pruning, for every retained \(\ell\notin P_t\).
\end{restatable}

We defer the proof to Appendix~\ref{app:invariance}. If a pruned expert later reappears, we treat it
as a re-entry and reinitialize its idiosyncratic state; \(\Delta_{\max}\) controls the resulting
memory--accuracy trade-off.

\paragraph{Birth and re-entry (informed initialization).}
Let
\(\mathcal{E}^{\mathrm{init}}_t\coloneqq \mathcal{E}_t\setminus \mathcal{K}^{\text{work}}_{t-1}\)
denote experts that \emph{enter} the working registry at time \(t\) (either newly observed or
re-entering after pruning). For each \(j\in\mathcal{E}^{\mathrm{init}}_t\), we instantiate an
idiosyncratic state at the predictive time. When no side information is available, we use the
population prior:
\begin{equation}
\label{eq:birth_prior}
\mathbf{u}_{t,j}\mid(z_t=m)\sim \mathcal{N}\!\big(\mu^{(m)}_{\mathrm{pop}},\Sigma^{(m)}_{\mathrm{pop}}\big),
\qquad m\in[M].
\end{equation}
A convenient choice consistent with an ``initialize-then-time-update'' view is
\(
\mu^{(m)}_{\mathrm{pop}}=\mathbf{A}^{(u)}_m\mu_{\mathrm{pop}}
\)
and
\(
\Sigma^{(m)}_{\mathrm{pop}}=\mathbf{A}^{(u)}_m\Sigma_{\mathrm{pop}}\mathbf{A}^{(u)\top}_m+\mathbf{Q}^{(u)}_m,
\)
where \((\mu_{\mathrm{pop}},\Sigma_{\mathrm{pop}})\) are global hyperparameters.
This adds experts online without gradient-based retraining.

If side information is available for a re-entering expert \(j\), we can set a more specific Gaussian
prior \((\mu^{(m)}_{\mathrm{init},j},\Sigma^{(m)}_{\mathrm{init},j})\) that reflects that knowledge (e.g.,
historical performance or metadata). When no such information is available, we default to the
population prior \eqref{eq:birth_prior}. To make the expert immediately \emph{context-aware}, we assume its loading matrix
\(\mathbf B_j\) is available at entry (e.g., provided by metadata or an embedding model, or initialized
from a population prior); thus the shared factor \(\mathbf g_t\) inferred from past queried experts
couples into \(\boldsymbol\alpha_{t,j}=\mathbf B_j\mathbf g_t+\mathbf u_{t,j}\) from the first round of
availability.

\begin{restatable}[Coupling at birth through the shared factor]{proposition}{transfer}
\label{prop:transfer}
Fix time \(t\) and condition on \((\mathcal{F}_t,z_t=m)\). Under the Factorized SLDS one-step predictive
belief (i.e., with \(\mathrm{Cov}(\mathbf{g}_t,\mathbf{u}_{t,k}\mid\cdot)=\mathbf{0}\) and
\(\mathrm{Cov}(\mathbf{u}_{t,i},\mathbf{u}_{t,j}\mid\cdot)=\mathbf{0}\) for \(i\neq j\)), for any experts \(j\neq k\),
\[
\mathrm{Cov}\!\left(\boldsymbol\alpha_{t,j},\boldsymbol\alpha_{t,k}\mid \mathcal{F}_t,z_t=m\right)
=
\mathbf{B}_j\,\Sigma^{(m)}_{g,t\mid t-1}\,\mathbf{B}_k^\top,
\]
where \(\Sigma^{(m)}_{g,t\mid t-1}\) is the regime-\(m\) one-step predictive covariance of \(\mathbf{g}_t\).
In particular, if the joint predictive law is Gaussian and
\(\mathbf{B}_j\,\Sigma^{(m)}_{g,t\mid t-1}\,\mathbf{B}_k^\top\neq \mathbf{0}\),
then \(\boldsymbol\alpha_{t,j}\) and \(\boldsymbol\alpha_{t,k}\) are not independent and hence
\(\mathcal{I}(\boldsymbol\alpha_{t,j};\boldsymbol\alpha_{t,k}\mid \mathcal{F}_t,z_t=m)>0\).
\end{restatable}

We give the proof in Appendix~\ref{app:transfer}.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[
    node distance=1.5cm and 3cm,
    >=latex,
    thick,
    latent_node/.style={latent, minimum size=1cm},
    obs_node/.style={obs, minimum size=1cm},
    rcurinfo_node/.style={latent, minimum size=1cm},
    action_node/.style={draw, rectangle, minimum size=0.9cm},
    param_edge/.style={->, dashed, color=gray!80}
]

% --- NODES ---
\node[latent_node] (zt_prev) {$z_{t-1}$};
\node[latent_node, right=of zt_prev] (zt) {$z_t$};
\node[latent_node, right=of zt] (zt_next) {$z_{t+1}$};

\node[obs_node, above=1.2cm of zt] (xt) {$\mathbf{x}_t$};

\node[latent_node, below=1.8cm of zt_prev] (gt_prev) {$\mathbf{g}_{t-1}$};
\node[latent_node, right=of gt_prev] (gt) {$\mathbf{g}_t$};

\node[latent_node, below=1.8cm of gt_prev] (ut_prev) {$\mathbf{u}_{t-1,j}$};
\node[latent_node, right=of ut_prev] (ut) {$\mathbf{u}_{t,j}$};

\node[obs_node, below=1.5cm of ut] (lt) {$e_{t,j}$};

% Plate over maintained registry (working set)
\plate {experts} {(ut_prev)(ut)(lt)} {$j \in \mathcal{K}^{\text{work}}_t$};

% Availability and action
\node[obs_node, right=4cm of ut] (Kt) {$\mc{E}_t$};
\node[action_node, below=1.5cm of Kt] (rt) {$I_t$};

% --- EDGES ---
\edge {zt_prev} {zt};
\edge {zt} {zt_next};
\edge {gt_prev} {gt};
\edge {ut_prev} {ut};

\draw[param_edge] (zt) to [bend right=20] node[pos=0.3, midway, right, font=\tiny] {$A^{(g)}_{z_t},Q^{(g)}_{z_t}$} (gt);
\draw[param_edge] (zt.south east) to [bend left=45] node[pos=0.7, right, font=\tiny, xshift=2pt] {$A^{(u)}_{z_t},Q^{(u)}_{z_t}$} (ut.north east);

\draw[->] (gt) to [bend left=45] (lt);
\edge {ut} {lt};

\draw[param_edge]
  (xt)
  to
  node[pos=0.5, right, font=\tiny, xshift=2pt]
  {$\Pi_\theta(\cdot,\cdot\mid \mathbf{x}_t)$}
  (zt);

\draw[->] (xt.east) to [out=0, in=0, looseness=1] node[midway, right, font=\small] {$\phi(\mathbf{x}_t)$} (lt.east);

\edge {Kt} {rt};

% Selection affects what is observed (not what is generated)
\draw[->, dotted] (rt) -- (lt) node[midway, above, font=\tiny] {reveals};

\end{tikzpicture}
    \caption{Factorized SLDS with bandit feedback and \emph{context-dependent} regime switching:
\(p(z_t\mid z_{t-1},\mathbf{x}_t)\). The plate \(j\in\mathcal{K}^{\text{work}}_t\) indexes experts whose
idiosyncratic states are stored. Each \(e_{t,j}\) is a \emph{potential} residual, but only \(e_{t,I_t}\)
is revealed at round \(t\).}
    \label{fig:slds_pgm_final}
\end{figure}



\section{Predictive Resource Allocation}
\label{sec:scheduling}

In many operational settings, experts are scarce resources (e.g., human analysts, licensed APIs, or specialized compute nodes). Keeping an expert \emph{on-call} incurs an opportunity cost, as that resource could otherwise be deployed to background tasks. Releasing experts that are unlikely to be queried frees capacity but risks missing a good prediction. We therefore treat future demand probabilistically, rather than trying to identify a single ``exact'' future expert.

We formalize a \textbf{predictive scheduling} layer that operates above the instance-level router. At decision
time $t$, the scheduler selects an \emph{active-set trajectory}
\(
\mathcal{S}_{t:t+H} \coloneqq (\mathcal{S}_{t,h})_{h=1}^H
\)
with $\mathcal{S}_{t,h}\subseteq \mathcal{K}^{\text{work}}_t$, where $\mathcal{S}_{t,h}$ is the set of experts
kept on-call for time $t+h$. Our goal is to choose $\mathcal{S}_{t:t+H}$ so that, for each $h$, the
planner's predicted choice at time $t+h$ (defined below) falls in $\mathcal{S}_{t,h}$ with probability at
least $1-\delta$. We achieve this by estimating the planner's time-marginal demand and taking a
quantile-based coverage set at each lookahead. We distinguish the ideal schedule
$\mathcal{S}_{t:t+H}(\delta)$ (built from the true marginals) from its Monte Carlo estimate
$\widehat{\mathcal{S}}_{t:t+H}(\delta)$.

\subsection{Stochastic Demand Formulation}
\label{sec:demand}

Scheduling requires forecasting which experts will be needed over the next $H$ rounds under
uncertainty in future contexts and latent dynamics. We model future contexts by a (possibly
degenerate) forecast distribution $p(\mathbf{X}_{t+1:t+H}\mid \mathcal{F}_t)$, where
$\mathcal{F}_t=\sigma(\mathcal{H}_{t-1},\mathbf{x}_t,\mathcal{E}_t)$ is the decision-time information.

A key constraint is that \emph{planning itself must not trigger extra expert queries}. Querying experts
purely to evaluate their future performance would (i) immediately incur the query fees $\beta_k$ and
(ii) keep the experts busy, contradicting the purpose of scheduling (freeing scarce resources).



\paragraph{Known availability constraints.}
At time $t$, some future feasibility information may be known (e.g., calendars, rate limits, or
maintenance windows). We encode it by an $\mathcal{F}_t$-measurable family
$(\mathcal{A}_{t+h})_{h=1}^{H}$ with $\mathcal{A}_{t+h}\subseteq \mathcal{K}$, where
$\mathcal{A}_{t+h}$ is the set of experts known at time $t$ to be feasible at future time $t+h$.
Since the planner requires a predictive belief for each candidate expert, we restrict to those with
maintained idiosyncratic marginals at time $t$, i.e. the working registry $\mathcal{K}^{\text{work}}_t$:
\begin{equation}
\label{eq:feas_set}
\mathcal{E}^{\mathrm{feas}}_{t+h \mid t}
\coloneqq
\mathcal{K}^{\text{work}}_t \cap \mathcal{A}_{t+h},
\qquad h=1,\dots,H.
\end{equation}
If expert $j$ is known at time $t$ to be unavailable on Mondays, then for any
$h\in\{1,\dots,H\}$ such that $t+h$ falls on a Monday we set $j\notin\mathcal{A}_{t+h}$, hence
$j\notin\mathcal{E}^{\mathrm{feas}}_{t+h \mid t}$, and $j$ cannot be selected by planning at that time.

\paragraph{Model-predictive optimal path (planning at time $t$).}
Fix a candidate future context trajectory $\mathbf{x}_{t+1:t+H}$.
For each lookahead step $h\in\{1,\dots,H\}$ and $k\in\mathcal{E}^{\mathrm{feas}}_{t+h \mid t}$, the SLDS
induces an \emph{open-loop} predictive law for the potential residual, obtained by propagating the
time-$t$ belief forward without measurement updates and conditioning on the context prefix
$\mathbf{X}_{t+1:t+h}=\mathbf{x}_{t+1:t+h}$. Let $e_{t+h,k}^{\mathrm{pred}}$ denote the resulting
pre-query residual random variable. We score expert $k$ by its \emph{planning-time predicted cost}
\begin{align}
\label{eq:pred_cost}
C_{t+h,k}^{\dagger}(\mathbf{x}_{t+1:t+h})
&\coloneqq
\mathbb{E}\!\left[\psi\!\left(e_{t+h,k}^{\mathrm{pred}}\right)+\beta_k\ \middle|\ \mathcal{F}_t,\ \mathbf{X}_{t+1:t+h}=\mathbf{x}_{t+1:t+h}\right].
\end{align}
Equivalently, \(C_{t+h,k}^{\dagger}\) is the conditional expectation of
\(C_{t+h,k}^{\mathrm{pred}}\) from \eqref{eq:virtual_cost_def}.
Crucially, \eqref{eq:pred_cost} is computed \emph{without} querying experts during planning (no
$\widehat{y}_{t+h,k}$ is revealed) and \emph{without} observing the future target $y_{t+h}$; it depends
only on the time-$t$ SLDS belief and the hypothesized context prefix.

The \emph{predictive} choice at time $t+h$ is
\begin{equation}
\label{eq:pred_choice}
\widehat{I}_{t+h}(\mathbf{x}_{t+1:t+h})
\in
\arg\min_{k\in\mathcal{E}^{\mathrm{feas}}_{t+h \mid t}} C_{t+h,k}^{\dagger}(\mathbf{x}_{t+1:t+h}),
\end{equation}
with a fixed deterministic tie-break, and the corresponding planned path is
\begin{equation}
\label{eq:pred_path}
\widehat{I}_{t+1:t+H}(\mathbf{x}_{t+1:t+H})
\coloneqq
\bigl(\widehat{I}_{t+h}(\mathbf{x}_{t+1:t+h})\bigr)_{h=1}^{H}.
\end{equation}

\paragraph{Demand induced by context uncertainty.}
For a \emph{fixed} candidate future context trajectory $\mathbf{x}_{t+1:t+H}$, the planning rule in
\eqref{eq:pred_choice}--\eqref{eq:pred_path} is deterministic: it maps the context path to a single
planned expert sequence $\widehat{I}_{t+1:t+H}(\mathbf{x}_{t+1:t+H})$. Consequently, the only source
of randomness in the planned sequence is uncertainty about future contexts. Under the forecast model,
$\mathbf{X}_{t+1:t+H}\sim p(\cdot\mid \mathcal{F}_t)$, this induces a distribution over planned expert
paths. Formally, for any feasible index sequence
$\mathbf{i}\in\prod_{h=1}^{H}\mathcal{E}^{\mathrm{feas}}_{t+h \mid t}$,
\begin{equation}
\label{eq:path_dist}
\mathbb{P}\!\left(\widehat{I}_{t+1:t+H}(\mathbf{X}_{t+1:t+H})=\mathbf{i} \ \middle|\ \mathcal{F}_t\right).
\end{equation}
In practice, scheduling rarely needs the full path distribution. A convenient summary is the set of
time-marginal selection probabilities: for each lookahead step $h\in\{1,\dots,H\}$ and expert
$k\in\mathcal{K}^{\text{work}}_t$,
\begin{equation}
\label{eq:time_marginals}
\rho_{t,h}(k)
\coloneqq
\mathbb{P}\!\left(\widehat{I}_{t+h}=k \ \middle|\ \mathcal{F}_t\right).
\end{equation}
The collection $\bigl(\rho_{t,h}(k)\bigr)_{h,k}$ forms an
$H\times|\mathcal{K}^{\text{work}}_t|$ demand matrix; each row is supported on the feasible set
$\mathcal{E}^{\mathrm{feas}}_{t+h\mid t}$. When the forecast is sharp, a row concentrates on one
expert; when futures are ambiguous, mass spreads across multiple experts, motivating on-call sets that
retain several candidates at the same lookahead.

\subsection{Monte Carlo Planning for Predictive Scheduling}
\label{sec:mc_planning}

The demand objects in Section~\ref{sec:demand} are rarely available in closed form. Even for a fixed
lookahead step $h$, the planner's choice rule $\widehat{I}_{t+h}(\cdot)$ is defined by an
$\arg\min$ over predictive conditional expectations \eqref{eq:pred_cost}, and the resulting path
distribution \eqref{eq:path_dist} and time-marginals \eqref{eq:time_marginals} inherit this
nonlinearity through the discrete minimization. We therefore approximate these quantities by forward
Monte Carlo.

\paragraph{Sampling future context scenarios.}
We assume access to a conditional forecast law $p(\mathbf{X}_{t+1:t+H}\mid \mathcal{F}_t)$ and require
i.i.d.\ samples from it. Concretely, let $s_t$ denote a realized $\mathcal{F}_t$-measurable
information state (e.g., $s_t=(\mathcal{H}_{t-1},\mathbf{x}_t,\mathcal{E}_t)$). We assume a (possibly
learned) scenario generator $\mathcal{G}$ and i.i.d.\ noise $\epsilon^{(n)}\sim p_\epsilon$ with $n\leq N$ such that
$\mathcal{G}(s_t,\epsilon)\sim p(\cdot\mid \mathcal{F}_t)$. We then generate
\begin{equation}
\label{eq:mc_contexts}
\mathbf{X}^{(n)}_{t+1:t+H}
=
\mathcal{G}(s_t,\epsilon^{(n)}),
\qquad
\epsilon^{(n)}\stackrel{\text{i.i.d.}}{\sim}p_\epsilon
\end{equation}
In practice, $\mathcal{G}$ can be instantiated as a lightweight autoregressive model or a non-parametric sampler from historical trajectories, ensuring that the computational cost of planning does not exceed the cost of the experts we aim to prune.

\paragraph{Scenario-wise planned paths.}
Fix a sampled scenario $n$ with context trajectory $\mathbf{X}^{(n)}_{t+1:t+H}$. For each lookahead
step $h\in\{1,\dots,H\}$, we restrict attention to experts that are (i) in the working registry at
time $t$ and (ii) known feasible at time $t+h$, i.e.,
$k\in\mathcal{E}^{\mathrm{feas}}_{t+h \mid t}$ from \eqref{eq:feas_set}. Using the scenario prefix
$\mathbf{X}^{(n)}_{t+1:t+h}$, we evaluate the planning-time predicted costs
$C_{t+h,k}^{\dagger}(\mathbf{X}^{(n)}_{t+1:t+h})$ via \eqref{eq:pred_cost} for all
$k\in\mathcal{E}^{\mathrm{feas}}_{t+h \mid t}$, and then apply the deterministic decision rule for
$h \in \{1,\dots,H\}$,
\begin{equation}
\label{eq:mc_choice}
\widehat{I}^{(n)}_{t+h}
\in
\arg\min_{k\in\mathcal{E}^{\mathrm{feas}}_{t+h \mid t}}
C_{t+h,k}^{\dagger}\!\left(\mathbf{X}^{(n)}_{t+1:t+h}\right)
\end{equation}
using the same fixed tie-breaking as in \eqref{eq:pred_choice}. Importantly, this is \emph{pure
planning}: it does not query any expert during the lookahead (so no $\widehat{y}_{t+h,k}$ is
revealed), does not observe the future target $y_{t+h}$, and performs no measurement updates inside
the horizon. Collecting these per-time choices yields the scenario-wise planned expert path
\begin{equation}
\label{eq:mc_path}
\widehat{I}^{(n)}_{t+1:t+H}
\coloneqq
\bigl(\widehat{I}^{(n)}_{t+h}\bigr)_{h=1}^{H}
\in
\prod_{h=1}^{H}\mathcal{E}^{\mathrm{feas}}_{t+h \mid t}.
\end{equation}

\paragraph{Estimating time-marginal demand.}
We estimate the time-marginal demand profile by empirical frequencies. For each $h\in\{1,\dots,H\}$
and $k\in\mathcal{K}^{\text{work}}_t$,
\begin{equation}
\label{eq:mc_time_marginals}
\hat{\rho}_{t,h}(k)
\coloneqq
\frac{1}{N}\sum_{n=1}^N \mathbf{1}\!\left\{\widehat{I}^{(n)}_{t+h}=k\right\}.
\end{equation}
For fixed $h$, the vector $\bigl(\hat{\rho}_{t,h}(k)\bigr)_{k\in\mathcal{K}^{\text{work}}_t}$ is
a pmf supported on $\mathcal{E}^{\mathrm{feas}}_{t+h\mid t}$: it assigns non-zero mass to multiple
experts exactly when different scenarios yield different planned winners at time $t+h$. Conditioned
on $\mathcal{F}_t$, the conditional law of large numbers gives
$\hat{\rho}_{t,h}(k)\to \rho_{t,h}(k)$ almost surely as $N\to\infty$
for each $(h,k)$, under the context-sampling distribution actually used.

\paragraph{From time-marginals to an active-set trajectory.}
The time-marginals $\rho_{t,h}(\cdot)$ describe which expert the planner would select at time $t+h$ under
the forecast. To translate these marginals into an on-call decision, we construct for each $h$ a
(possibly multi-expert) \emph{coverage set} $\mathcal{S}_{t,h}(\delta)$ that contains the
highest-probability experts summing to at least $1-\delta$.

Fix a tolerance $\delta\in[0,1)$. For each $h\in\{1,\dots,H\}$, let $k_{(1)}, k_{(2)}, \dots$ denote experts
sorted so that $\rho_{t,h}(k_{(1)})\ge \rho_{t,h}(k_{(2)})\ge \cdots$. Define the minimum set size
\begin{equation}
K_h \coloneqq \min \left\{ K : \sum_{i=1}^K \rho_{t,h}(k_{(i)}) \ge 1-\delta \right\},
\end{equation}
and set $\mathcal{S}_{t,h}(\delta) \coloneqq \{ k_{(1)}, \dots, k_{(K_h)} \}$. The resulting
\emph{active-set trajectory} is
\begin{equation}
\label{eq:active_schedule}
\mathcal{S}_{t:t+H}(\delta)
\coloneqq
\bigl(\mathcal{S}_{t,h}(\delta)\bigr)_{h=1}^H,
\end{equation}
which prescribes the on-call set for each future time $t+h$.

By construction, $\sum_{k\in\mathcal{S}_{t,h}(\delta)} \rho_{t,h}(k)\ge 1-\delta$ for each $h$, so
\[
\mathbb{P}\!\left(\widehat{I}_{t+h}\in \mathcal{S}_{t,h}(\delta)\ \middle|\ \mathcal{F}_t\right)\ge 1-\delta,
\qquad h=1,\dots,H.
\]
Moreover, $\mathcal{S}_{t,h}(\delta)\subseteq \mathcal{E}^{\mathrm{feas}}_{t+h\mid t}$ for each $h$,
so each scheduled set is feasible and contained in $\mathcal{K}^{\text{work}}_t$.

\paragraph{Monte Carlo implementation.}
In practice we replace $\rho_{t,h}$ by its Monte Carlo estimate $\hat{\rho}_{t,h}$ in the same construction.
The resulting plug-in sets are denoted $\widehat{\mathcal{S}}_{t,h}(\delta)$, and the output schedule is
\(
\widehat{\mathcal{S}}_{t:t+H}(\delta)\coloneqq (\widehat{\mathcal{S}}_{t,h}(\delta))_{h=1}^H
\).
By the conditional law of large numbers, $\hat{\rho}_{t,h}(k)\to\rho_{t,h}(k)$ almost surely for each $(h,k)$,
so $\widehat{\mathcal{S}}_{t,h}(\delta)$ converges to $\mathcal{S}_{t,h}(\delta)$ except at ties.
For example, with $H=3$ and experts \(\{1,2,3\}\), a realized schedule might be
\(\widehat{\mathcal{S}}_{t:t+3}(\delta)=(\{2\},\{2,3\},\{1\})\), meaning that time $t+1$ calls for expert
2, time $t+2$ keeps \(\{2,3\}\) on-call, and time $t+3$ calls for expert 1. If we also include the
current round, we can prepend the singleton \(\{I_t\}\) at $h=0$. If a single static pool is required,
one can form $\widehat{\mathcal{S}}^{\cup}_t(\delta)\coloneqq\bigcup_{h=1}^H \widehat{\mathcal{S}}_{t,h}(\delta)$.
When uncertainty is high, the marginal mass can be spread across several experts and
$\widehat{\mathcal{S}}_{t,h}(\delta)$ may include multiple candidates; when demand concentrates, it typically
reduces to a single dominant expert.

\begin{algorithm}[H]
\caption{Predictive Scheduling via Monte Carlo}
\label{alg:predictive_schedule}
\begin{algorithmic}[1]
\STATE {\bfseries Input:} horizon $H$; scenarios $N$; tolerance $\delta$; working registry $\mathcal{K}^{\text{work}}_t$; feasibility sets $(\mathcal{A}_{t+h})_{h=1}^{H}$; forecast generator $\mathcal{G}$; time-$t$ SLDS belief.
\STATE Sample scenarios $\mathbf{X}^{(n)}_{t+1:t+H}$ via \eqref{eq:mc_contexts} for $n=1,\dots,N$.
\FOR{$n=1$ to $N$}
    \FOR{$h=1$ to $H$}
        \STATE $\mathcal{E}^{\mathrm{feas}}_{t+h\mid t} \leftarrow \mathcal{K}^{\text{work}}_t \cap \mathcal{A}_{t+h}$
        \STATE Compute $C_{t+h,k}^{\dagger}(\mathbf{X}^{(n)}_{t+1:t+h})$ for all $k\in\mathcal{E}^{\mathrm{feas}}_{t+h\mid t}$ via \eqref{eq:pred_cost}
        \STATE $\widehat{I}^{(n)}_{t+h} \in \arg\min_{k\in\mathcal{E}^{\mathrm{feas}}_{t+h\mid t}} C_{t+h,k}^{\dagger}(\mathbf{X}^{(n)}_{t+1:t+h})$
    \ENDFOR
\ENDFOR
\STATE Compute $\hat{\rho}_{t,h}(k)$ via \eqref{eq:mc_time_marginals} for all $h,k$.
\FOR{$h=1$ to $H$}
    \STATE Sort experts by $\hat{\rho}_{t,h}(k)$ and choose the smallest $K_h$ with cumulative mass $\ge 1-\delta$
    \STATE $\widehat{\mathcal{S}}_{t,h}(\delta) \leftarrow \{k_{(1)},\dots,k_{(K_h)}\}$
\ENDFOR
\STATE {\bfseries Return:} $\widehat{\mathcal{S}}_{t:t+H}(\delta)=(\widehat{\mathcal{S}}_{t,h}(\delta))_{h=1}^H$.
\end{algorithmic}
\end{algorithm}
