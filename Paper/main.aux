\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{madras2018predict,mozannar2021consistent,Narasimhan,mao2023twostage,montreuil2025ask}
\citation{hamilton2020time,sezer2020financial}
\citation{ghahramani2000variational,linderman2016recurrent,hu2024modeling}
\citation{russo2014learning}
\citation{Chow_1970,Bartlett_Wegkamp_2008,cortes,Geifman_El-Yaniv_2017,cao2022generalizing,cortes2024cardinalityaware}
\citation{madras2018predict,mozannar2021consistent,Verma2022LearningTD}
\citation{mozannar2021consistent,Verma2022LearningTD,Cao_Mozannar_Feng_Wei_An_2023,Mozannar2023WhoSP,mao2024realizablehconsistentbayesconsistentloss,mao2025realizablehconsistentbayesconsistentloss,charusaie2022sample,mao2024principledapproacheslearningdefer,wei2024exploiting}
\citation{mao2024regressionmultiexpertdeferral,strong2024towards,palomba2025a,montreuil2024twostagelearningtodefermultitasklearning,montreuil2025optimalqueryallocationextractive}
\citation{nguyen2025probabilistic}
\citation{joshi2021learning}
\citation{madras2018predict,mozannar2021consistent,Narasimhan,mao2024regressionmultiexpertdeferral}
\citation{mozannar2021consistent}
\citation{mozannar2021consistent,Verma2022LearningTD,mao2024regressionmultiexpertdeferral}
\citation{hamilton2020time}
\citation{rabiner2003introduction,shumway2006time}
\citation{kalman1960new,welch1995introduction}
\citation{bengio1994input,ghahramani2000variational,fox2008nonparametric,hu2024modeling,geadah2024parsing}
\citation{neu2010online,dani2008stochastic}
\newlabel{sec:background}{{3}{2}{}{section.3}{}}
\newlabel{sec:background@cref}{{[section][3][]3}{[1][2][]2}}
\newlabel{sec:background_l2d}{{3.1}{2}{}{subsection.3.1}{}}
\newlabel{sec:background_l2d@cref}{{[subsection][1][3]3.1}{[1][2][]2}}
\newlabel{eq:l2d_cost}{{1}{2}{}{equation.3.1}{}}
\newlabel{eq:l2d_cost@cref}{{[equation][1][]1}{[1][2][]2}}
\newlabel{eq:l2d_objective}{{2}{2}{}{equation.3.2}{}}
\newlabel{eq:l2d_objective@cref}{{[equation][2][]2}{[1][2][]2}}
\newlabel{eq:l2d_bayes_rule}{{3}{2}{}{equation.3.3}{}}
\newlabel{eq:l2d_bayes_rule@cref}{{[equation][3][]3}{[1][2][]2}}
\newlabel{sec:background_slds}{{3.2}{2}{}{subsection.3.2}{}}
\newlabel{sec:background_slds@cref}{{[subsection][2][3]3.2}{[1][2][]2}}
\newlabel{sec:problem_formulation}{{4.1}{2}{}{subsection.4.1}{}}
\newlabel{sec:problem_formulation@cref}{{[subsection][1][4]4.1}{[1][2][]2}}
\citation{neu2010online}
\citation{bengio1994input,linderman2016recurrent,hu2024modeling}
\citation{bengio1994input}
\newlabel{eq:residual}{{6}{3}{}{equation.4.6}{}}
\newlabel{eq:residual@cref}{{[equation][6][]6}{[1][3][]3}}
\newlabel{eq:routing_cost}{{7}{3}{}{equation.4.7}{}}
\newlabel{eq:routing_cost@cref}{{[equation][7][]7}{[1][3][]3}}
\newlabel{eq:routing_objective}{{8}{3}{}{equation.4.8}{}}
\newlabel{eq:routing_objective@cref}{{[equation][8][]8}{[1][3][]3}}
\newlabel{eq:bayes_selection}{{9}{3}{}{equation.4.9}{}}
\newlabel{eq:bayes_selection@cref}{{[equation][9][]9}{[1][3][]3}}
\newlabel{sec:generative_model}{{4.2}{3}{}{subsection.4.2}{}}
\newlabel{sec:generative_model@cref}{{[subsection][2][4]4.2}{[1][3][]3}}
\citation{allyouneed,kossen2021self,mehta2022neural}
\newlabel{eq:context_transitions}{{10}{4}{}{equation.4.10}{}}
\newlabel{eq:context_transitions@cref}{{[equation][10][]10}{[1][4][]4}}
\newlabel{eq:global_dynamics}{{11}{4}{}{equation.4.11}{}}
\newlabel{eq:global_dynamics@cref}{{[equation][11][]11}{[1][4][]4}}
\newlabel{eq:idiosyncratic_dynamics}{{12}{4}{}{equation.4.12}{}}
\newlabel{eq:idiosyncratic_dynamics@cref}{{[equation][12][]12}{[1][4][]4}}
\newlabel{def:l2d_slds_emission}{{1}{4}{L2D-SLDS reliability and residual emission}{definition.1}{}}
\newlabel{def:l2d_slds_emission@cref}{{[definition][1][]1}{[1][4][]4}}
\newlabel{eq:alpha_def}{{13}{4}{L2D-SLDS reliability and residual emission}{equation.4.13}{}}
\newlabel{eq:alpha_def@cref}{{[equation][13][]13}{[1][4][]4}}
\newlabel{eq:residual_emission}{{14}{4}{L2D-SLDS reliability and residual emission}{equation.4.14}{}}
\newlabel{eq:residual_emission@cref}{{[equation][14][]14}{[1][4][]4}}
\citation{russo2014learning}
\citation{madras2018predict,mozannar2021consistent}
\newlabel{thmt@@propinfo@data}{{\def \theequation {\@arabic {\c@equation }}\def \theHequation {(restate \theHthmt@dummyctr )4.\@arabic {\c@equation }}\setcounter {equation}{14}}{5}{}{subsubsection.4.2.2}{}}
\newlabel{thmt@@propinfo@data@cref}{{[subsubsection][2][4,2]4.2.2}{[1][5][]5}}
\newlabel{thmt@@propinfo}{{2}{5}{Information transfer under a shared factor}{proposition.2}{}}
\newlabel{thmt@@propinfo@cref}{{[proposition][2][]2}{[1][5][]5}}
\newlabel{prop:cross_update}{{2}{5}{Information transfer under a shared factor}{proposition.2}{}}
\newlabel{prop:cross_update@cref}{{[proposition][2][]2}{[1][5][]5}}
\newlabel{sec:exploration}{{4.2.3}{5}{}{subsubsection.4.2.3}{}}
\newlabel{sec:exploration@cref}{{[subsubsection][3][4,2]4.2.3}{[1][5][]5}}
\newlabel{eq:model_gap}{{15}{5}{}{equation.4.15}{}}
\newlabel{eq:model_gap@cref}{{[equation][15][]15}{[1][5][]5}}
\newlabel{eq:ig_operational}{{16}{5}{}{equation.4.16}{}}
\newlabel{eq:ig_operational@cref}{{[equation][16][]16}{[1][5][]5}}
\newlabel{eq:ids_deterministic}{{17}{5}{}{equation.4.17}{}}
\newlabel{eq:ids_deterministic@cref}{{[equation][17][]17}{[1][5][]5}}
\newlabel{sec:registry}{{4.2.4}{5}{}{subsubsection.4.2.4}{}}
\newlabel{sec:registry@cref}{{[subsubsection][4][4,2]4.2.4}{[1][5][]5}}
\newlabel{eq:stale_set}{{18}{5}{}{equation.4.18}{}}
\newlabel{eq:stale_set@cref}{{[equation][18][]18}{[1][5][]5}}
\citation{mozannar2021consistent,Narasimhan,mao2024regressionmultiexpertdeferral}
\newlabel{eq:registry_update}{{19}{6}{}{equation.4.19}{}}
\newlabel{eq:registry_update@cref}{{[equation][19][]19}{[1][5][]6}}
\newlabel{thmt@@invariance@data}{{\def \theequation {\@arabic {\c@equation }}\def \theHequation {(restate \theHthmt@dummyctr )4.\@arabic {\c@equation }}\setcounter {equation}{19}}{6}{}{equation.4.19}{}}
\newlabel{thmt@@invariance@data@cref}{{[subsubsection][4][4,2]4.2.4}{[1][6][]6}}
\newlabel{thmt@@invariance}{{3}{6}{Pruning does not affect retained experts}{proposition.3}{}}
\newlabel{thmt@@invariance@cref}{{[proposition][3][]3}{[1][6][]6}}
\newlabel{prop:invariance}{{3}{6}{Pruning does not affect retained experts}{proposition.3}{}}
\newlabel{prop:invariance@cref}{{[proposition][3][]3}{[1][6][]6}}
\newlabel{eq:birth_prior}{{20}{6}{}{equation.4.20}{}}
\newlabel{eq:birth_prior@cref}{{[equation][20][]20}{[1][6][]6}}
\newlabel{thmt@@transfer@data}{{\def \theequation {\@arabic {\c@equation }}\def \theHequation {(restate \theHthmt@dummyctr )4.\@arabic {\c@equation }}\setcounter {equation}{20}}{6}{}{equation.4.20}{}}
\newlabel{thmt@@transfer@data@cref}{{[subsubsection][4][4,2]4.2.4}{[1][6][]6}}
\newlabel{thmt@@transfer}{{4}{6}{Coupling at birth through the shared factor}{proposition.4}{}}
\newlabel{thmt@@transfer@cref}{{[proposition][4][]4}{[1][6][]6}}
\newlabel{prop:transfer}{{4}{6}{Coupling at birth through the shared factor}{proposition.4}{}}
\newlabel{prop:transfer@cref}{{[proposition][4][]4}{[1][6][]6}}
\newlabel{section:experiments}{{5}{6}{}{section.5}{}}
\newlabel{section:experiments@cref}{{[section][5][]5}{[1][6][]6}}
\newlabel{sec:exp_synthetic_transfer}{{5.1}{6}{}{subsection.5.1}{}}
\newlabel{sec:exp_synthetic_transfer@cref}{{[subsection][1][5]5.1}{[1][6][]6}}
\newlabel{eq:exp_tri_cycle_ts}{{21}{6}{}{equation.5.21}{}}
\newlabel{eq:exp_tri_cycle_ts@cref}{{[equation][21][]21}{[1][6][]6}}
\citation{li2010contextual}
\citation{zhou2020neuralcontextualbanditsucbbased}
\citation{mao2024regressionmultiexpertdeferral,Narasimhan}
\citation{Narasimhan,mao2024regressionmultiexpertdeferral}
\citation{rumelhart1985learning}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:exp_synthetic_transfer_regime_estimation}{{1}{7}{Regime-0 expert dependence in the synthetic transfer experiment. Each heatmap shows the pairwise Pearson correlation (color: \([-1,1]\)) between experts' per-round losses (experts indexed \(0\)--\(3\)). Top row: partial feedback (only queried losses observed); bottom row: full feedback. Columns (left-to-right) show the ground-truth correlation implied by the simulator and the correlations estimated by each method. L2D-SLDS best recovers the block-structured correlations (experts \(\{0,1\}\) vs.\ \(\{2,3\}\)), highlighting the benefit of modeling shared latent factors for cross-expert information transfer under censoring.\relax }{figure.caption.1}{}}
\newlabel{fig:exp_synthetic_transfer_regime_estimation@cref}{{[figure][1][]1}{[1][6][]7}}
\newlabel{eq:exp_synth_expert_rule}{{22}{7}{}{equation.5.22}{}}
\newlabel{eq:exp_synth_expert_rule@cref}{{[equation][22][]22}{[1][7][]7}}
\newlabel{tab:exp_avg_costs}{{1}{7}{Averaged cumulative cost \eqref {eq:routing_objective} on experiment (Section~\ref {sec:exp_synthetic_transfer}). We report mean \(\pm \) standard error across five runs. Lower is better.\relax }{table.caption.2}{}}
\newlabel{tab:exp_avg_costs@cref}{{[table][1][]1}{[1][7][]7}}
\bibdata{biblio}
\bibcite{Bartlett_Wegkamp_2008}{{1}{2008}{{Bartlett \& Wegkamp}}{{Bartlett and Wegkamp}}}
\bibcite{bengio1994input}{{2}{1994}{{Bengio \& Frasconi}}{{Bengio and Frasconi}}}
\bibcite{cao2022generalizing}{{3}{2022}{{Cao et~al.}}{{Cao, Cai, Feng, Gu, Gu, An, Niu, and Sugiyama}}}
\bibcite{Cao_Mozannar_Feng_Wei_An_2023}{{4}{2024}{{Cao et~al.}}{{Cao, Mozannar, Feng, Wei, and An}}}
\bibcite{charusaie2022sample}{{5}{2022}{{Charusaie et~al.}}{{Charusaie, Mozannar, Sontag, and Samadi}}}
\bibcite{Chow_1970}{{6}{1970}{{Chow}}{{}}}
\bibcite{cortes}{{7}{2016}{{Cortes et~al.}}{{Cortes, DeSalvo, and Mohri}}}
\bibcite{cortes2024cardinalityaware}{{8}{2024}{{Cortes et~al.}}{{Cortes, Mao, Mohri, Mohri, and Zhong}}}
\bibcite{dani2008stochastic}{{9}{2008}{{Dani et~al.}}{{Dani, Hayes, and Kakade}}}
\bibcite{fox2008nonparametric}{{10}{2008}{{Fox et~al.}}{{Fox, Sudderth, Jordan, and Willsky}}}
\bibcite{geadah2024parsing}{{11}{2024}{{Geadah et~al.}}{{Geadah, Pillow, et~al.}}}
\bibcite{Geifman_El-Yaniv_2017}{{12}{2017}{{Geifman \& El-Yaniv}}{{Geifman and El-Yaniv}}}
\bibcite{ghahramani2000variational}{{13}{2000}{{Ghahramani \& Hinton}}{{Ghahramani and Hinton}}}
\bibcite{hamilton2020time}{{14}{2020}{{Hamilton}}{{}}}
\bibcite{hu2024modeling}{{15}{2024}{{Hu et~al.}}{{Hu, Zoltowski, Nair, Anderson, Duncker, and Linderman}}}
\bibcite{joshi2021learning}{{16}{2021}{{Joshi et~al.}}{{Joshi, Parbhoo, and Doshi-Velez}}}
\bibcite{kalman1960new}{{17}{1960}{{Kalman}}{{}}}
\bibcite{kossen2021self}{{18}{2021}{{Kossen et~al.}}{{Kossen, Band, Lyle, Gomez, Rainforth, and Gal}}}
\bibcite{li2010contextual}{{19}{2010}{{Li et~al.}}{{Li, Chu, Langford, and Schapire}}}
\bibcite{linderman2016recurrent}{{20}{2016}{{Linderman et~al.}}{{Linderman, Miller, Adams, Blei, Paninski, and Johnson}}}
\bibcite{madras2018predict}{{21}{2018}{{Madras et~al.}}{{Madras, Pitassi, and Zemel}}}
\bibcite{mao2023twostage}{{22}{2023}{{Mao et~al.}}{{Mao, Mohri, Mohri, and Zhong}}}
\bibcite{mao2024principledapproacheslearningdefer}{{23}{2024{a}}{{Mao et~al.}}{{Mao, Mohri, and Zhong}}}
\bibcite{mao2024realizablehconsistentbayesconsistentloss}{{24}{2024{b}}{{Mao et~al.}}{{Mao, Mohri, and Zhong}}}
\bibcite{mao2024regressionmultiexpertdeferral}{{25}{2024{c}}{{Mao et~al.}}{{Mao, Mohri, and Zhong}}}
\bibcite{mao2025realizablehconsistentbayesconsistentloss}{{26}{2025}{{Mao et~al.}}{{Mao, Mohri, and Zhong}}}
\bibcite{mehta2022neural}{{27}{2022}{{Mehta et~al.}}{{Mehta, Sz{\'e}kely, Beskow, and Henter}}}
\bibcite{montreuil2025ask}{{28}{2025{a}}{{Montreuil et~al.}}{{Montreuil, Carlier, Ng, and Ooi}}}
\bibcite{montreuil2024twostagelearningtodefermultitasklearning}{{29}{2025{b}}{{Montreuil et~al.}}{{Montreuil, Heng, Carlier, Ng, and Ooi}}}
\bibcite{montreuil2025optimalqueryallocationextractive}{{30}{2025{c}}{{Montreuil et~al.}}{{Montreuil, Yeo, Carlier, Ng, and Ooi}}}
\bibcite{mozannar2021consistent}{{31}{2020}{{Mozannar \& Sontag}}{{Mozannar and Sontag}}}
\bibcite{Mozannar2023WhoSP}{{32}{2023}{{Mozannar et~al.}}{{Mozannar, Lang, Wei, Sattigeri, Das, and Sontag}}}
\bibcite{Narasimhan}{{33}{2022}{{Narasimhan et~al.}}{{Narasimhan, Jitkrittum, Menon, Rawat, and Kumar}}}
\bibcite{neu2010online}{{34}{2010}{{Neu et~al.}}{{Neu, Antos, Gy{\"o}rgy, and Szepesv{\'a}ri}}}
\bibcite{nguyen2025probabilistic}{{35}{2025}{{Nguyen et~al.}}{{Nguyen, Do, and Carneiro}}}
\bibcite{palomba2025a}{{36}{2025}{{Palomba et~al.}}{{Palomba, Pugnana, Alvarez, and Ruggieri}}}
\bibcite{rabiner2003introduction}{{37}{2003}{{Rabiner \& Juang}}{{Rabiner and Juang}}}
\bibcite{rumelhart1985learning}{{38}{1985}{{Rumelhart et~al.}}{{Rumelhart, Hinton, and Williams}}}
\bibcite{russo2014learning}{{39}{2014}{{Russo \& Van~Roy}}{{Russo and Van~Roy}}}
\bibcite{sezer2020financial}{{40}{2020}{{Sezer et~al.}}{{Sezer, Gudelek, and Ozbayoglu}}}
\bibcite{shumway2006time}{{41}{2006}{{Shumway}}{{}}}
\bibcite{strong2024towards}{{42}{2024}{{Strong et~al.}}{{Strong, Men, and Noble}}}
\bibcite{allyouneed}{{43}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{Verma2022LearningTD}{{44}{2022}{{Verma et~al.}}{{Verma, Barrejon, and Nalisnick}}}
\bibcite{wei2024exploiting}{{45}{2024}{{Wei et~al.}}{{Wei, Cao, and Feng}}}
\bibcite{welch1995introduction}{{46}{1995}{{Welch et~al.}}{{Welch, Bishop, et~al.}}}
\bibcite{zhou2020neuralcontextualbanditsucbbased}{{47}{2020}{{Zhou et~al.}}{{Zhou, Li, and Gu}}}
\bibstyle{icml2026}
\gdef \LT@i {\LT@entry 
    {1}{113.32153pt}\LT@entry 
    {1}{366.99132pt}}
\newlabel{app:roadmap}{{A}{11}{}{appendix.A}{}}
\newlabel{app:roadmap@cref}{{[appendix][1][2147483647]A}{[1][11][]11}}
\newlabel{app:notation}{{B}{11}{}{appendix.B}{}}
\newlabel{app:notation@cref}{{[appendix][2][2147483647]B}{[1][11][]11}}
\newlabel{fig:slds_pgm_final}{{2}{13}{L2D-SLDS with bandit feedback and \emph {context-dependent} regime switching: \(p(z_t\mid z_{t-1},\mathbf {x}_t)\). The plate \(j\in \mathcal {K}_t\) indexes experts whose idiosyncratic states are stored. Each \(e_{t,j}\) is a \emph {potential} residual, but only \(e_{t,I_t}\) is revealed at round \(t\).\relax }{figure.caption.4}{}}
\newlabel{fig:slds_pgm_final@cref}{{[figure][2][2147483647]2}{[1][12][]13}}
\newlabel{algo}{{D}{13}{}{appendix.D}{}}
\newlabel{algo@cref}{{[appendix][4][2147483647]D}{[1][13][]13}}
\newlabel{app:alg_router}{{D.1}{13}{}{subsection.D.1}{}}
\newlabel{app:alg_router@cref}{{[subappendix][1][2147483647,4]D.1}{[1][13][]13}}
\newlabel{alg:router_main}{{1}{14}{Context-Aware Router (Factorized SLDS + IMM + IDS)\relax }{algorithm.1}{}}
\newlabel{alg:router_main@cref}{{[algorithm][1][2147483647]1}{[1][13][]14}}
\newlabel{alg:correct_reweight}{{2}{14}{\textsc {Correct}: Queried Kalman Update and Mode Posterior\relax }{algorithm.2}{}}
\newlabel{alg:correct_reweight@cref}{{[algorithm][2][2147483647]2}{[1][14][]14}}
\newlabel{app:alg_learning}{{D.2}{15}{}{subsection.D.2}{}}
\newlabel{app:alg_learning@cref}{{[subappendix][2][2147483647,4]D.2}{[1][14][]15}}
\newlabel{alg:trainmodel_em}{{3}{15}{\textsc {LearnParameters\_MCEM}: Monte Carlo EM for the Factorized SLDS (windowed batch)\relax }{algorithm.3}{}}
\newlabel{alg:trainmodel_em@cref}{{[algorithm][3][2147483647]3}{[1][14][]15}}
\newlabel{alg:online_em}{{4}{16}{\textsc {OnlineUpdate}: Sliding-Window Monte Carlo EM (non-stationary adaptation)\relax }{algorithm.4}{}}
\newlabel{alg:online_em@cref}{{[algorithm][4][2147483647]4}{[1][16][]16}}
\newlabel{app:cross_covariance}{{D.3}{16}{}{subsection.D.3}{}}
\newlabel{app:cross_covariance@cref}{{[subappendix][3][2147483647,4]D.3}{[1][16][]16}}
\newlabel{app:info_gain}{{E}{17}{}{appendix.E}{}}
\newlabel{app:info_gain@cref}{{[appendix][5][2147483647]E}{[1][17][]17}}
\newlabel{rmk:zg_information}{{5}{17}{$(z_t,\mathbf {g}_t)$-Information Gain for Non-Stationary Routing}{remark.5}{}}
\newlabel{rmk:zg_information@cref}{{[remark][5][2147483647]5}{[1][17][]17}}
\newlabel{eq:ig_chain_rule_rmk}{{23}{17}{$(z_t,\mathbf {g}_t)$-Information Gain for Non-Stationary Routing}{equation.E.23}{}}
\newlabel{eq:ig_chain_rule_rmk@cref}{{[equation][23][2147483647]23}{[1][17][]17}}
\newlabel{sec:exploration_zg}{{E.1}{17}{}{subsection.E.1}{}}
\newlabel{sec:exploration_zg@cref}{{[subappendix][1][2147483647,5]E.1}{[1][17][]17}}
\newlabel{eq:exp_pred_rv}{{24}{17}{}{equation.E.24}{}}
\newlabel{eq:exp_pred_rv@cref}{{[equation][24][2147483647]24}{[1][17][]17}}
\newlabel{eq:exp_g_prior}{{25}{18}{}{equation.E.25}{}}
\newlabel{eq:exp_g_prior@cref}{{[equation][25][2147483647]25}{[1][17][]18}}
\newlabel{eq:exp_channel}{{26}{18}{}{equation.E.26}{}}
\newlabel{eq:exp_channel@cref}{{[equation][26][2147483647]26}{[1][18][]18}}
\newlabel{eq:exp_channel_params}{{27}{18}{}{equation.E.27}{}}
\newlabel{eq:exp_channel_params@cref}{{[equation][27][2147483647]27}{[1][18][]18}}
\newlabel{eq:exp_squared_loss}{{28}{18}{}{equation.E.28}{}}
\newlabel{eq:exp_squared_loss@cref}{{[equation][28][2147483647]28}{[1][18][]18}}
\newlabel{eq:exp_virtual_cost}{{29}{18}{}{equation.E.29}{}}
\newlabel{eq:exp_virtual_cost@cref}{{[equation][29][2147483647]29}{[1][18][]18}}
\newlabel{eq:exp_mean_cost}{{30}{18}{}{equation.E.30}{}}
\newlabel{eq:exp_mean_cost@cref}{{[equation][30][2147483647]30}{[1][18][]18}}
\newlabel{eq:exp_gap}{{31}{18}{}{equation.E.31}{}}
\newlabel{eq:exp_gap@cref}{{[equation][31][2147483647]31}{[1][18][]18}}
\newlabel{eq:exp_residual_mean}{{32}{18}{}{equation.E.32}{}}
\newlabel{eq:exp_residual_mean@cref}{{[equation][32][2147483647]32}{[1][18][]18}}
\newlabel{eq:exp_residual_var}{{33}{18}{}{equation.E.33}{}}
\newlabel{eq:exp_residual_var@cref}{{[equation][33][2147483647]33}{[1][18][]18}}
\newlabel{eq:exp_cost_mix}{{34}{18}{}{equation.E.34}{}}
\newlabel{eq:exp_cost_mix@cref}{{[equation][34][2147483647]34}{[1][18][]18}}
\newlabel{eq:exp_square_loss_mix}{{35}{18}{}{equation.E.35}{}}
\newlabel{eq:exp_square_loss_mix@cref}{{[equation][35][2147483647]35}{[1][18][]18}}
\newlabel{eq:exp_ig_zg_def}{{36}{18}{}{equation.E.36}{}}
\newlabel{eq:exp_ig_zg_def@cref}{{[equation][36][2147483647]36}{[1][18][]18}}
\newlabel{eq:exp_ig_zg_chain}{{37}{19}{}{equation.E.37}{}}
\newlabel{eq:exp_ig_zg_chain@cref}{{[equation][37][2147483647]37}{[1][18][]19}}
\newlabel{eq:exp_ig_g_mode}{{39}{19}{}{equation.E.39}{}}
\newlabel{eq:exp_ig_g_mode@cref}{{[equation][39][2147483647]39}{[1][19][]19}}
\newlabel{eq:exp_iz_mc}{{40}{19}{}{equation.E.40}{}}
\newlabel{eq:exp_iz_mc@cref}{{[equation][40][2147483647]40}{[1][19][]19}}
\newlabel{eq:exp_iz_estimator}{{42}{19}{}{equation.E.42}{}}
\newlabel{eq:exp_iz_estimator@cref}{{[equation][42][2147483647]42}{[1][19][]19}}
\newlabel{eq:exp_logpdf_gauss}{{43}{19}{}{equation.E.43}{}}
\newlabel{eq:exp_logpdf_gauss@cref}{{[equation][43][2147483647]43}{[1][19][]19}}
\newlabel{eq:exp_logmix_lse}{{44}{19}{}{equation.E.44}{}}
\newlabel{eq:exp_logmix_lse@cref}{{[equation][44][2147483647]44}{[1][19][]19}}
\newlabel{eq:exp_ig_zg_final}{{45}{19}{}{equation.E.45}{}}
\newlabel{eq:exp_ig_zg_final@cref}{{[equation][45][2147483647]45}{[1][19][]19}}
\newlabel{app:proof_cross_update}{{F.1}{19}{}{subsection.F.1}{}}
\newlabel{app:proof_cross_update@cref}{{[subappendix][1][2147483647,6]F.1}{[1][19][]19}}
\newlabel{app:invariance}{{F.2}{20}{}{subsection.F.2}{}}
\newlabel{app:invariance@cref}{{[subappendix][2][2147483647,6]F.2}{[1][20][]20}}
\newlabel{eq:marg_def}{{46}{20}{}{equation.F.46}{}}
\newlabel{eq:marg_def@cref}{{[equation][46][2147483647]46}{[1][20][]20}}
\citation{li2010contextual}
\citation{zhou2020neuralcontextualbanditsucbbased}
\newlabel{app:transfer}{{F.3}{21}{}{subsection.F.3}{}}
\newlabel{app:transfer@cref}{{[subappendix][3][2147483647,6]F.3}{[1][21][]21}}
\newlabel{sec:experiments-details}{{G}{21}{}{appendix.G}{}}
\newlabel{sec:experiments-details@cref}{{[appendix][7][2147483647]G}{[1][21][]21}}
\newlabel{subsection:baselines}{{G.1}{22}{}{subsection.G.1}{}}
\newlabel{subsection:baselines@cref}{{[subappendix][1][2147483647,7]G.1}{[1][22][]22}}
\newlabel{sec:exp_synthetic_transfer_appendix}{{G.2}{23}{}{subsection.G.2}{}}
\newlabel{sec:exp_synthetic_transfer_appendix@cref}{{[subappendix][2][2147483647,7]G.2}{[1][22][]23}}
\newlabel{eq:exp_tri_cycle_ts_appendix}{{47}{23}{}{equation.G.47}{}}
\newlabel{eq:exp_tri_cycle_ts_appendix@cref}{{[equation][47][2147483647]47}{[1][23][]23}}
\newlabel{eq:exp_synth_expert_rule_appendix}{{48}{23}{}{equation.G.48}{}}
\newlabel{eq:exp_synth_expert_rule_appendix@cref}{{[equation][48][2147483647]48}{[1][23][]23}}
\newlabel{tab:exp_avg_costs_appendix}{{3}{23}{Averaged cumulative cost \eqref {eq:routing_objective} on experiment (Section~\ref {sec:exp_synthetic_transfer}). We report mean \(\pm \) standard error across five runs. Lower is better.\relax }{table.caption.5}{}}
\newlabel{tab:exp_avg_costs_appendix@cref}{{[table][3][2147483647]3}{[1][23][]23}}
\citation{haoyietal-informer-2021}
\citation{FRED_DGS10}
\newlabel{fig:expert_structure_all}{{3}{24}{We report the selection frequency of each expert over time as a function of the underlying regime. The top figure corresponds to the oracle, while the bottom figure shows our approach evaluated against the baselines. By construction, experts~0 and~1 perform better in regime~1, whereas experts~2 and~3 perform better in regime~2. Accordingly, a well-adapted router should select experts~0 and~1 more frequently in regime~1 and experts~2 and~3 more frequently in regime~2. L2D-SLDS (with and without $g_t$) is the only method that captures this structure, closely matching the oracleâ€™s selection behavior. In contrast, LinUCB and NeuralUCB fail to adapt their selection frequencies to the regimes.\relax }{figure.caption.6}{}}
\newlabel{fig:expert_structure_all@cref}{{[figure][3][2147483647]3}{[1][24][]24}}
\newlabel{sec:exp_etth1_appendix}{{G.3}{24}{}{subsection.G.3}{}}
\newlabel{sec:exp_etth1_appendix@cref}{{[subappendix][3][2147483647,7]G.3}{[1][24][]24}}
\newlabel{tab:experts_etth_appendix}{{4}{24}{Configuration of experts for ETTh1.\relax }{table.caption.7}{}}
\newlabel{tab:experts_etth_appendix@cref}{{[table][4][2147483647]4}{[1][24][]24}}
\newlabel{tab:exp_avg_costs_etth_appendix}{{5}{25}{Averaged cumulative cost \eqref {eq:routing_objective} on ETTh1 (Section~\ref {sec:exp_etth1}). We report the mean $\pm $ standard error over five runs; lower is better. The averaged cumulative cost is computed both over the full time horizon and, for each expert, only over the periods during which that expert is available. This explains why Expert~4 attains a low cost despite being available for only a short duration. Consequently, it is expected that baseline methods exhibit higher averaged costs than Expert~4.\relax }{table.caption.8}{}}
\newlabel{tab:exp_avg_costs_etth_appendix@cref}{{[table][5][2147483647]5}{[1][24][]25}}
\newlabel{subsection_appendix_fred}{{G.4}{25}{}{subsection.G.4}{}}
\newlabel{subsection_appendix_fred@cref}{{[subappendix][4][2147483647,7]G.4}{[1][24][]25}}
\newlabel{tab:experts_fred_appendix}{{6}{25}{Configuration of experts for the FRED DGS10 experiment.\relax }{table.caption.9}{}}
\newlabel{tab:experts_fred_appendix@cref}{{[table][6][2147483647]6}{[1][25][]25}}
\ttl@finishall
\newlabel{tab:exp_avg_var_l2d_fred}{{7}{26}{Averaged cumulative cost \eqref {eq:routing_objective} on the FRED (DGS10) experiment (Appendix~\ref {subsection_appendix_fred}). We report mean \(\pm \) standard error across five runs; lower is better.\relax }{table.caption.10}{}}
\newlabel{tab:exp_avg_var_l2d_fred@cref}{{[table][7][2147483647]7}{[1][25][]26}}
\gdef \@abspage@last{26}
