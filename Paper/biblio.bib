@article{rajpurkar2016squad,
  title={SQuAD: 100,000+ Questions for Machine Comprehension of Text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}

@inproceedings{wei2024exploiting,
  title={Exploiting human-ai dependence for learning to defer},
  author={Wei, Zixi and Cao, Yuzhou and Feng, Lei},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}
@article{cao2022generalizing,
  title={Generalizing consistent multi-class classification with rejection to be compatible with arbitrary losses},
  author={Cao, Yuzhou and Cai, Tianchi and Feng, Lei and Gu, Lihong and Gu, Jinjie and An, Bo and Niu, Gang and Sugiyama, Masashi},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={521--534},
  year={2022}
}
@article{cortes2024theory,
  title={Theory and algorithms for learning with rejection in binary classification},
  author={Cortes, Corinna and DeSalvo, Giulia and Mohri, Mehryar},
  journal={Annals of Mathematics and Artificial Intelligence},
  volume={92},
  number={2},
  pages={277--315},
  year={2024},
  publisher={Springer}
}


@misc{vats2024survey,
      title={A Survey on Human-AI Teaming with Large Pre-Trained Models},
      author={Vanshika Vats and Marzia Binta Nizam and Minghao Liu and Ziyuan Wang and Richard Ho and Mohnish Sai Prasad and Vincent Titterton and Sai Venkat Malreddy and Riya Aggarwal and Yanwen Xu and Lei Ding and Jay Mehta and Nathan Grinnell and Li Liu and Sijia Zhong and Devanathan Nallur Gandamani and Xinyi Tang and Rohan Ghosalkar and Celeste Shen and Rachel Shen and Nafisa Hussain and Kesav Ravichandran and James Davis},
      year={2024},
      eprint={2403.04931},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}


@book{Foundations,
title = "Foundations of machine learning",
author = "Mehryar Mohri and Afshin Rostamizadeh and Ameet Talwalkar",
year = "2012",
language = "English (US)",
publisher = "MIT Press",

}
@article{mao2024h,
  title={$ H $-Consistency Bounds: Characterization and Extensions},
  author={Mao, Anqi and Mohri, Mehryar and Zhong, Yutao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{Keswani,
author = {Keswani, Vijay and Lease, Matthew and Kenthapadi, Krishnaram},
title = {Towards Unbiased and Accurate Deferral to Multiple Experts},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462516},
doi = {10.1145/3461702.3462516},
abstract = {Machine learning models are often implemented in cohort with humans in the pipeline, with the model having an option to defer to a domain expert in cases where it has low confidence in its inference. Our goal is to design mechanisms for ensuring accuracy and fairness in such prediction systems that combine machine learning model inferences and domain expert predictions. Prior work on "deferral systems" in classification settings has focused on the setting of a pipeline with a single expert and aimed to accommodate the inaccuracies and biases of this expert to simultaneously learn an inference model and a deferral system. Our work extends this framework to settings where multiple experts are available, with each expert having their own domain of expertise and biases. We propose a framework that simultaneously learns a classifier and a deferral system, with the deferral system choosing to defer to one or more human experts in cases of input where the classifier has low confidence. We test our framework on a synthetic dataset and a content moderation dataset with biased synthetic experts, and show that it significantly improves the accuracy and fairness of the final predictions, compared to the baselines. We also collect crowdsourced labels for the content moderation task to construct a real-world dataset for the evaluation of hybrid machine-human frameworks and show that our proposed framework outperforms baselines on this real-world dataset as well.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {154–165},
numpages = {12},
keywords = {deferral models, fairness, hybrid human-machine frameworks},
location = {Virtual Event, USA},
series = {AIES '21}
}
@inproceedings{Kerrigan,
 author = {Kerrigan, Gavin and Smyth, Padhraic and Steyvers, Mark},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {4421--4434},
 publisher = {Curran Associates, Inc.},
 title = {Combining Human Predictions with Model Probabilities via Confusion Matrices and Calibration},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/234b941e88b755b7a72a1c1dd5022f30-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{liu2024mitigating,
  title={Mitigating Underfitting in Learning to Defer with Consistent Losses},
  author={Liu, Shuqi and Cao, Yuzhou and Zhang, Qiaozhen and Feng, Lei and An, Bo},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4816--4824},
  year={2024},
  organization={PMLR}
}

@InProceedings{Tailor,
  title = 	 {Learning to Defer to a Population: A Meta-Learning Approach},
  author =       {Tailor, Dharmesh and Patra, Aditya and Verma, Rajeev and Manggala, Putra and Nalisnick, Eric},
  booktitle = 	 {Proceedings of The 27th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3475--3483},
  year = 	 {2024},
  editor = 	 {Dasgupta, Sanjoy and Mandt, Stephan and Li, Yingzhen},
  volume = 	 {238},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {02--04 May},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v238/tailor24a/tailor24a.pdf},
  url = 	 {https://proceedings.mlr.press/v238/tailor24a.html},
  abstract = 	 {The learning to defer (L2D) framework allows autonomous systems to be safe and robust by allocating difficult decisions to a human expert. All existing work on L2D assumes that each expert is well-identified, and if any expert were to change, the system should be re-trained. In this work, we alleviate this constraint, formulating an L2D system that can cope with never-before-seen experts at test-time. We accomplish this by using meta-learning, considering both optimization- and model-based variants. Given a small context set to characterize the currently available expert, our framework can quickly adapt its deferral policy. For the model-based approach, we employ an attention mechanism that is able to look for points in the context set that are similar to a given test point, leading to an even more precise assessment of the expert’s abilities. In the experiments, we validate our methods on image recognition, traffic sign detection, and skin lesion diagnosis benchmarks.}
}
@inproceedings{Benz,
  title={Counterfactual inference of second opinions},
  author={Benz, Nina L Corvelo and Rodriguez, Manuel Gomez},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={453--463},
  year={2022},
  organization={PMLR}
}
@inproceedings{Hemmer,
  title     = {Forming Effective Human-{AI} Teams: Building Machine Learning Models that Complement the Capabilities of Multiple Experts},
  author    = {Hemmer, Patrick and Schellhammer, Sebastian and Vössing, Michael and Jakubik, Johannes and Satzger, Gerhard},
  booktitle = {Proceedings of the Thirty-First International Joint Conference on
               Artificial Intelligence, {IJCAI-22}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Lud De Raedt},
  pages     = {2478--2484},
  year      = {2022},
  month     = {7},
  note      = {Main Track},
  doi       = {10.24963/ijcai.2022/344},
  url       = {https://doi.org/10.24963/ijcai.2022/344},
}

@inproceedings{theoretically,
  title = 	 {Theoretically Grounded Loss Functions and Algorithms for Score-Based Multi-Class Abstention},
  author =       {Mao, Anqi and Mohri, Mehryar and Zhong, Yutao},
  booktitle = 	 {Proceedings of The 27th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {4753--4761},
  year = 	 {2024},
  editor = 	 {Dasgupta, Sanjoy and Mandt, Stephan and Li, Yingzhen},
  volume = 	 {238},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {02--04 May},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v238/mao24a/mao24a.pdf},
  url = 	 {https://proceedings.mlr.press/v238/mao24a.html},
  abstract = 	 {Learning with abstention is a key scenario where the learner can abstain from making a prediction at some cost. In this paper, we analyze the score-based formulation of learning with abstention in the multi-class classification setting. We introduce new families of surrogate losses for the abstention loss function, which include the state-of-the-art surrogate losses in the single-stage setting and a novel family of loss functions in the two-stage setting. We prove strong non-asymptotic and hypothesis set-specific consistency guarantees for these surrogate losses, which upper-bound the estimation error of the abstention loss function in terms of the estimation error of the surrogate loss. Our bounds can help compare different score-based surrogates and guide the design of novel abstention algorithms by minimizing the proposed surrogate losses. We experimentally evaluate our new algorithms on CIFAR-10, CIFAR-100, and SVHN datasets and the practical significance of our new surrogate losses and two-stage abstention algorithms. Our results also show that the relative performance of the state-of-the-art score-based surrogate losses can vary across datasets.}
}


@article{pascal,
author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher and Winn, John and Zisserman, Andrew},
year = {2010},
month = {06},
pages = {303-338},
title = {The Pascal Visual Object Classes (VOC) challenge},
volume = {88},
journal = {International Journal of Computer Vision},
doi = {10.1007/s11263-009-0275-4}
}
@misc{raghu2019algorithmicautomationproblemprediction,
      title={The Algorithmic Automation Problem: Prediction, Triage, and Human Effort}, 
      author={Maithra Raghu and Katy Blumer and Greg Corrado and Jon Kleinberg and Ziad Obermeyer and Sendhil Mullainathan},
      year={2019},
      eprint={1903.12220},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1903.12220}, 
}
@article{krizhevsky2009learning,
  title={Learning Multiple Layers of Features from Tiny Images},
  author={Alex Krizhevsky},
  year={2009},
  url={https://api.semanticscholar.org/CorpusID:18268744}
}
@article{KELLEYPACE1997291,
title = {Sparse spatial autoregressions},
journal = {Statistics and Probability Letters},
volume = {33},
number = {3},
pages = {291-297},
year = {1997},
issn = {0167-7152},
doi = {https://doi.org/10.1016/S0167-7152(96)00140-X},
url = {https://www.sciencedirect.com/science/article/pii/S016771529600140X},
author = {R. {Kelley Pace} and Ronald Barry},
keywords = {Spatial autoregression, SAR, Sparse matrices},
abstract = {Given local spatial error dependence, one can construct sparse spatial weight matrices. As an illustration of the power of such sparse structures, we computed a simultaneous autoregression using 20 640 observations in under 19 min despite needing to compute a 20 640 by 20 640 determinant 10 times.}
}



@ARTICLE{Ohn_Aldrich1997-wn,
  title   = "Fisher and the making of maximum likelihood 1912-1922",
  author  = "Ohn Aldrich, R A",
  journal = "Statistical Science",
  volume  =  12,
  number  =  3,
  pages   = "162--179",
  year    =  1997
}





@misc{howard2017mobilenetsefficientconvolutionalneural,
      title={MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications}, 
      author={Andrew G. Howard and Menglong Zhu and Bo Chen and Dmitry Kalenichenko and Weijun Wang and Tobias Weyand and Marco Andreetto and Hartwig Adam},
      year={2017},
      eprint={1704.04861},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1704.04861}, 
}
@misc{ren2016fasterrcnnrealtimeobject,
      title={Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}, 
      author={Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun},
      year={2016},
      eprint={1506.01497},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1506.01497}, 
}


@inproceedings{mao2024principledapproacheslearningdefer,
  title={Principled Approaches for Learning to Defer with Multiple Experts},
  author={Mao, Anqi and Mohri, Mehryar and Zhong, Yutao},
  booktitle={ISAIM},
  year={2024}
}

@misc{awasthi2021calibrationconsistencyadversarialsurrogate,
      title={Calibration and Consistency of Adversarial Surrogate Losses}, 
      author={Pranjal Awasthi and Natalie Frank and Anqi Mao and Mehryar Mohri and Yutao Zhong},
      year={2021},
      eprint={2104.09658},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2104.09658}, 
}

@article{kingma2017adammethodstochasticoptimization,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}


@misc{meunier2022consistencyadversarialclassification,
      title={Towards Consistency in Adversarial Classification}, 
      author={Laurent Meunier and Raphaël Ettedgui and Rafael Pinot and Yann Chevaleyre and Jamal Atif},
      year={2022},
      eprint={2205.10022},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2205.10022}, 
}
@misc{bao2021calibratedsurrogatelossesadversarially,
      title={Calibrated Surrogate Losses for Adversarially Robust Classification}, 
      author={Han Bao and Clayton Scott and Masashi Sugiyama},
      year={2021},
      eprint={2005.13748},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2005.13748}, 
}
@inbook{Biggio_2013,
   title={Evasion Attacks against Machine Learning at Test Time},
   ISBN={9783642387098},
   ISSN={1611-3349},
   url={http://dx.doi.org/10.1007/978-3-642-40994-3_25},
   DOI={10.1007/978-3-642-40994-3_25},
   booktitle={Advanced Information Systems Engineering},
   publisher={Springer Berlin Heidelberg},
   author={Biggio, Battista and Corona, Igino and Maiorca, Davide and Nelson, Blaine and Šrndić, Nedim and Laskov, Pavel and Giacinto, Giorgio and Roli, Fabio},
   year={2013},
   pages={387–402} }


@article{mao2024realizablehconsistentbayesconsistentloss,
  title={Realizable $ H $-Consistent and Bayes-Consistent Loss Functions for Learning to Defer},
  author={Mao, Anqi and Mohri, Mehryar and Zhong, Yutao},
  journal={Advances in neural information processing systems},
  volume={37},
  pages={73638--73671},
  year={2024}
}


@InProceedings{Grounded,
  title = 	 {Theoretically Grounded Loss Functions and Algorithms for Adversarial Robustness},
  author =       {Awasthi, Pranjal and Mao, Anqi and Mohri, Mehryar and Zhong, Yutao},
  booktitle = 	 {Proceedings of The 26th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {10077--10094},
  year = 	 {2023},
  editor = 	 {Ruiz, Francisco and Dy, Jennifer and van de Meent, Jan-Willem},
  volume = 	 {206},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {25--27 Apr},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v206/awasthi23c/awasthi23c.pdf},
  url = 	 {https://proceedings.mlr.press/v206/awasthi23c.html},
  abstract = 	 {Adversarial robustness is a critical property of classifiers in applications as they are increasingly deployed in complex real-world systems. Yet, achieving accurate adversarial robustness in machine learning remains a persistent challenge and the choice of the surrogate loss function used for training a key factor. We present a family of new loss functions for adversarial robustness, smooth adversarial losses, which we show can be derived in a general way from broad families of loss functions used in multi-class classification. We prove strong H-consistency theoretical guarantees for these loss functions, including multi-class H-consistency bounds for sum losses in the adversarial setting. We design new regularized algorithms based on the minimization of these principled smooth adversarial losses (PSAL). We further show through a series of extensive experiments with the CIFAR-10, CIFAR-100 and SVHN datasets that our PSAL algorithm consistently outperforms the current state-of-the-art technique, TRADES, for both robust accuracy against l-infinity-norm bounded perturbations and, even more significantly, for clean accuracy. Finally, we prove that, unlike PSAL, the TRADES loss in general does not admit an H-consistency property.}
}







@techreport{weston1998multi,
  title={Multi-class support vector machines},
  author={Weston, Jason and Watkins, Chris},
  year={1998},
  institution={Citeseer}
}
@inproceedings{Ghosh,
author = {Ghosh, Aritra and Kumar, Himanshu and Sastry, P. S.},
title = {Robust loss functions under label noise for deep neural networks},
year = {2017},
publisher = {AAAI Press},
abstract = {In many applications of classifier learning, training data suffers from label noise. Deep networks are learned using huge training data where the problem of noisy labels is particularly relevant. The current techniques proposed for learning deep networks under label noise focus on modifying the network architecture and on algorithms for estimating true labels from noisy labels. An alternate approach would be to look for loss functions that are inherently noise-tolerant. For binary classification there exist theoretical results on loss functions that are robust to label noise. In this paper, we provide some sufficient conditions on a loss function so that risk minimization under that loss function would be inherently tolerant to label noise for multiclass classification problems. These results generalize the existing results on noise-tolerant loss functions for binary classification. We study some of the widely used loss functions in deep networks and show that the loss function based on mean absolute value of error is inherently robust to label noise. Thus standard back propagation is enough to learn the true classifier even under label noise. Through experiments, we illustrate the robustness of risk minimization with such loss functions for learning neural networks.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {1919–1925},
numpages = {7},
location = {San Francisco, California, USA},
series = {AAAI'17}
}
@inproceedings{Cao_Mozannar_Feng_Wei_An_2023,
author = {Cao, Yuzhou and Mozannar, Hussein and Feng, Lei and Wei, Hongxin and An, Bo},
title = {In defense of softmax parametrization for calibrated and consistent learning to defer},
year = {2024},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Enabling machine learning classifiers to defer their decision to a downstream expert when the expert is more accurate will ensure improved safety and performance. This objective can be achieved with the learning-to-defer framework which aims to jointly learn how to classify and how to defer to the expert. In recent studies, it has been theoretically shown that popular estimators for learning to defer parameterized with softmax provide unbounded estimates for the likelihood of deferring which makes them uncalibrated. However, it remains unknown whether this is due to the widely used softmax parameterization and if we can find a softmax-based estimator that is both statistically consistent and possesses a valid probability estimator. In this work, we first show that the cause of the miscalibrated and unbounded estimator in prior literature is due to the symmetric nature of the surrogate losses used and not due to softmax. We then propose a novel statistically consistent asymmetric softmax-based surrogate loss that can produce valid estimates without the issue of unboundedness. We further analyze the non-asymptotic properties of our method and empirically validate its performance and calibration on benchmark datasets.},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {1671},
numpages = {19},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}


@article{ryan2020ai,
  title={In AI we trust: ethics, artificial intelligence, and reliability},
  author={Ryan, Mark},
  journal={Science and Engineering Ethics},
  volume={26},
  number={5},
  pages={2749--2767},
  year={2020},
  publisher={Springer}
}


@article{borsci2022chatbot,
  title={The chatbot usability scale: the design and pilot of a usability scale for interaction with AI-based conversational agents},
  author={Borsci, Simone and Malizia, Alessio and Schmettow, Martin and Van Der Velde, Frank and Tariverdiyeva, Gunay and Balaji, Divyaa and Chamberlain, Alan},
  journal={Personal and ubiquitous computing},
  volume={26},
  pages={95--119},
  year={2022},
  publisher={Springer}
}
@inproceedings{
strong2024towards,
title={Towards Human-{AI} Collaboration in Healthcare: Guided Deferral Systems with Large Language Models},
author={Joshua Strong and Qianhui Men and Alison Noble},
booktitle={ICML 2024 Workshop on LLMs and Cognition},
year={2024},
url={https://openreview.net/forum?id=4c5rg9y4me}
}
@misc{rajpurkar2018knowdontknowunanswerable,
      title={Know What You Don't Know: Unanswerable Questions for SQuAD},
      author={Pranav Rajpurkar and Robin Jia and Percy Liang},
      year={2018},
      eprint={1806.03822},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1806.03822},
}
@inproceedings{
palomba2025a,
title={A Causal Framework for Evaluating Deferring Systems},
author={Filippo Palomba and Andrea Pugnana and Jose Manuel Alvarez and Salvatore Ruggieri},
booktitle={The 28th International Conference on Artificial Intelligence and Statistics},
year={2025},
url={https://openreview.net/forum?id=mkkFubLdNW}
}
@inproceedings{Narasimhan,
 author = {Narasimhan, Harikrishna and Jitkrittum, Wittawat and Menon, Aditya K and Rawat, Ankit and Kumar, Sanjiv},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {29292--29304},
 publisher = {Curran Associates, Inc.},
 title = {Post-hoc estimators for learning to defer to an expert},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/bc8f76d9caadd48f77025b1c889d2e2d-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@inproceedings{Gamma_paper,
  title = 	 {On Classification-Calibration of Gamma-Phi Losses},
  author =       {Wang, Yutong and Scott, Clayton},
  booktitle = 	 {Proceedings of Thirty Sixth Conference on Learning Theory},
  pages = 	 {4929--4951},
  year = 	 {2023},
  editor = 	 {Neu, Gergely and Rosasco, Lorenzo},
  volume = 	 {195},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {12--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v195/wang23c/wang23c.pdf},
  url = 	 {https://proceedings.mlr.press/v195/wang23c.html},
  abstract = 	 {Gamma-Phi losses constitute a family of multiclass classification loss functions that generalize the logistic and other common losses, and have found application in the boosting literature. We establish the first general sufficient condition for the classification-calibration (CC) of such losses. To our knowledge, this sufficient condition gives the first family of nonconvex multiclass surrogate losses for which CC has been fully justified. In addition, we show that a previously proposed sufficient condition is in fact not sufficient. This contribution highlights a technical issue that is important in the study of multiclass CC but has been neglected in priorwork.}
}

@InProceedings{cortes,
author="Cortes, Corinna
and DeSalvo, Giulia
and Mohri, Mehryar",
editor="Ortner, Ronald
and Simon, Hans Ulrich
and Zilles, Sandra",
title="Learning with Rejection",
booktitle="Algorithmic Learning Theory",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="67--82",
abstract="We introduce a novel framework for classification with a rejection option that consists of simultaneously learning two functions: a classifier along with a rejection function. We present a full theoretical analysis of this framework including new data-dependent learning bounds in terms of the Rademacher complexities of the classifier and rejection families as well as consistency and calibration results. These theoretical guarantees guide us in designing new algorithms that can exploit different kernel-based hypothesis sets for the classifier and rejection functions. We compare and contrast our general framework with the special case of confidence-based rejection for which we devise alternative loss functions and algorithms as well. We report the results of several experiments showing that our kernel-based algorithms can yield a notable improvement over the best existing confidence-based rejection algorithm.",
isbn="978-3-319-46379-7"
}

@inproceedings{mao2024regressionmultiexpertdeferral,
author = {Mao, Anqi and Mohri, Mehryar and Zhong, Yutao},
title = {Regression with multi-expert deferral},
year = {2024},
publisher = {JMLR.org},
abstract = {Learning to defer with multiple experts is a framework where the learner can choose to defer the prediction to several experts. While this problem has received significant attention in classification contexts, it presents unique challenges in regression due to the infinite and continuous nature of the label space. In this work, we introduce a novel framework of regression with deferral, which involves deferring the prediction to multiple experts. We present a comprehensive analysis for both the single-stage scenario, where there is simultaneous learning of predictor and deferral functions, and the two-stage scenario, which involves a pre-trained predictor with a learned deferral function. We introduce new surrogate loss functions for both scenarios and prove that they are supported by H-consistency bounds. These bounds provide consistency guarantees that are stronger than Bayes consistency, as they are non-asymptotic and hypothesis set-specific. Our framework is versatile, applying to multiple experts, accommodating any bounded regression losses, addressing both instance-dependent and label-dependent costs, and supporting both single-stage and two-stage methods. Our single-stage formulation subsumes as a special case the recent regression with abstention (Cheng et al., 2023) framework, where only a single expert is considered, specifically for the squared loss and a label-independent cost. Minimizing our proposed loss functions directly leads to novel algorithms for regression with deferral. We report the results of extensive experiments showing the effectiveness of our proposed algorithms.},
booktitle = {Proceedings of the 41st International Conference on Machine Learning},
articleno = {1413},
numpages = {22},
location = {Vienna, Austria},
series = {ICML'24}
}

@inproceedings{montreuil2024twostagelearningtodefermultitasklearning,
  title={A Two-Stage Learning-to-Defer Approach for Multi-Task Learning},
  author={Montreuil, Yannis and Heng, Yeo Shu and Carlier, Axel and Ng, Lai Xing and Ooi, Wei Tsang},
  booktitle={Forty-second International Conference on Machine Learning},
  year={2025}
}


@article{akhtar2018threat,
  title={Threat of adversarial attacks on deep learning in computer vision: A survey},
  author={Akhtar, Naveed and Mian, Ajmal},
  journal={Ieee Access},
  volume={6},
  pages={14410--14430},
  year={2018},
  publisher={IEEE}
}
@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}

@article{chakraborty2021survey,
  title={A survey on adversarial attacks and defences},
  author={Chakraborty, Anirban and Alam, Manaar and Dey, Vishal and Chattopadhyay, Anupam and Mukhopadhyay, Debdeep},
  journal={CAAI Transactions on Intelligence Technology},
  volume={6},
  number={1},
  pages={25--45},
  year={2021},
  publisher={Wiley Online Library}
}



@misc{kalai2024calibrated,
      title={Calibrated Language Models Must Hallucinate},
      author={Adam Tauman Kalai and Santosh S. Vempala},
      year={2024},
      eprint={2311.14648},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{Jiang,
    author = {Jiang, Zhengbao and Araki, Jun and Ding, Haibo and Neubig, Graham},
    title = "{How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {9},
    pages = {962-977},
    year = {2021},
    month = {09},
    abstract = "{Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, “How can we know when language models know, with confidence, the answer to a particular query?” We examine this question from the point of view of calibration, the property of a probabilistic model’s predicted probabilities actually being well correlated with the probabilities of correctness. We examine three strong generative models—T5, BART, and GPT-2—and study whether their probabilities on QA tasks are well calibrated, finding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs. We have released the code at https://github.com/jzbjyb/lm-calibration.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00407},
    url = {https://doi.org/10.1162/tacl\_a\_00407},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00407/1962628/tacl\_a\_00407.pdf},
}

@article{
Jordan,
author = {M. I. Jordan  and T. M. Mitchell },
title = {Machine learning: Trends, perspectives, and prospects},
journal = {Science},
volume = {349},
number = {6245},
pages = {255-260},
year = {2015},
doi = {10.1126/science.aaa8415},
URL = {https://www.science.org/doi/abs/10.1126/science.aaa8415},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aaa8415},
abstract = {Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today’s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.}}


@article{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@inproceedings{Verma2022LearningTD,
  title={Learning to Defer to Multiple Experts: Consistent Surrogate Losses, Confidence Calibration, and Conformal Ensembles},
  author={Rajeev Verma and Daniel Barrejon and Eric Nalisnick},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:253237048}
}

@inproceedings{mozannar2021consistent,
author = {Mozannar, Hussein and Sontag, David},
title = {Consistent estimators for learning to defer to an expert},
year = {2020},
publisher = {JMLR.org},
abstract = {Learning algorithms are often used in conjunction with expert decision makers in practical scenarios, however this fact is largely ignored when designing these algorithms. In this paper we explore how to learn predictors that can either predict or choose to defer the decision to a downstream expert. Given only samples of the expert's decisions, we give a procedure based on learning a classifier and a rejector and analyze it theoretically. Our approach is based on a novel reduction to cost sensitive learning where we give a consistent surrogate loss for cost sensitive learning that generalizes the cross entropy loss. We show the effectiveness of our approach on a variety of experimental tasks.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {656},
numpages = {12},
series = {ICML'20}
}

@inproceedings{cao-etal-2019-controlling,
    title = "Controlling the Specificity of Clarification Question Generation",
    author = "Cao, Yang Trista  and
      Rao, Sudha  and
      Daum{\'e} III, Hal",
    editor = "Axelrod, Amittai  and
      Yang, Diyi  and
      Cunha, Rossana  and
      Shaikh, Samira  and
      Waseem, Zeerak",
    booktitle = "Proceedings of the 2019 Workshop on Widening NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-3619",
    pages = "53--56",
    abstract = "Unlike comprehension-style questions, clarification questions look for some missing information in a given context. However, without guidance, neural models for question generation, similar to dialog generation models, lead to generic and bland questions that cannot elicit useful information. We argue that controlling the level of specificity of the generated questions can have useful applications and propose a neural clarification question generation model for the same. We first train a classifier that annotates a clarification question with its level of specificity (generic or specific) to the given context. Our results on the Amazon questions dataset demonstrate that training a clarification question generation model on specificity annotated data can generate questions with varied levels of specificity to the given context.",
}

@unknown{Rao,
author = {Rao, Sudha and Daumé, III},
year = {2019},
month = {04},
pages = {},
title = {Answer-based Adversarial Training for Generating Clarification Questions}
}
@misc{huang2023survey,
      title={A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions},
      author={Lei Huang and Weijiang Yu and Weitao Ma and Weihong Zhong and Zhangyin Feng and Haotian Wang and Qianglong Chen and Weihua Peng and Xiaocheng Feng and Bing Qin and Ting Liu},
      year={2023},
      eprint={2311.05232},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{liu2019roberta,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{joshi2017triviaqa,
  title={TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1705.03551},
  year={2017}
}


@inproceedings{Mao_Mohri_Zhong_2023,
  title={Predictor-rejector multi-class abstention: Theoretical analysis and algorithms},
  author={Mao, Anqi and Mohri, Mehryar and Zhong, Yutao},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={822--867},
  year={2024},
  organization={PMLR}
}


@inproceedings{Geifman_El-Yaniv_2017,
 author = {Geifman, Yonatan and El-Yaniv, Ran},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Selective Classification for Deep Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/4a8423d5e91fda00bb7e46540e2b0cf1-Paper.pdf},
 volume = {30},
 year = {2017}
}

   


   @article{Bartlett_Wegkamp_2008, title={Classification with a Reject Option using a Hinge Loss}, volume={9}, abstractNote={We consider the problem of binary classification where the classifier can, for a particular cost, choose not to classify an observation. Just as in the conventional classification problem, minimization of the sample average of the cost is a difficult optimization problem. As an alternative, we propose the optimization of a certain convex loss function φ, analogous to the hinge loss used in support vector machines (SVMs). Its convexity ensures that the sample average of this surrogate loss can be efficiently minimized. We study its statistical properties. We show that minimizing the expected surrogate loss—the φ-risk—also minimizes the risk. We also study the rate at which the φ-risk approaches its minimum value. We show that fast rates are possible when the conditional probability P(Y=1|X) is unlikely to be close to certain critical values.}, journal={The Journal of Machine Learning Research}, author={Bartlett, Peter L. and Wegkamp, Marten H.}, year={2008}, month=jun, pages={1823–1840} }



   @article{Chow_1970, title={On optimum recognition error and reject tradeoff}, volume={16}, DOI={10.1109/TIT.1970.1054406}, abstractNote={The performance of a pattern recognition system is characterized by its error and reject tradeoff. This paper describes an optimum rejection rule and presents a general relation between the error and reject probabilities and some simple properties of the tradeoff in the optimum recognition system. The error rate can be directly evaluated from the reject function. Some practical implications of the results are discussed. Examples in normal distributions and uniform distributions are given.}, number={1}, journal={IEEE Transactions on Information Theory}, author={Chow, C.}, year={1970}, month=jan, pages={41–46} }




@inproceedings{yamada-etal-2020-luke,
    title = "{LUKE}: Deep Contextualized Entity Representations with Entity-aware Self-attention",
    author = "Yamada, Ikuya  and
      Asai, Akari  and
      Shindo, Hiroyuki  and
      Takeda, Hideaki  and
      Matsumoto, Yuji",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.523",
    doi = "10.18653/v1/2020.emnlp-main.523",
    pages = "6442--6454",
    abstract = "Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at \url{https://github.com/studio-ousia/luke}.",
}
@inproceedings{hou2015hitsz,
  title={HITSZ-ICRC: Exploiting classification approach for answer selection in community question answering},
  author={Hou, Yongshuai and Tan, Cong and Wang, Xiaolong and Zhang, Yaoyun and Xu, Jun and Chen, Qingcai},
  booktitle={Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015)},
  pages={196--202},
  year={2015}
}

@misc{sun2020mobilebertcompacttaskagnosticbert,
      title={MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices},
      author={Zhiqing Sun and Hongkun Yu and Xiaodan Song and Renjie Liu and Yiming Yang and Denny Zhou},
      year={2020},
      eprint={2004.02984},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2004.02984},
}

@ARTICLE{kmeans,
  author={Lloyd, S.},
  journal={IEEE Transactions on Information Theory},
  title={Least squares quantization in PCM},
  year={1982},
  volume={28},
  number={2},
  pages={129-137},
  keywords={},
  doi={10.1109/TIT.1982.1056489}}

@misc{szegedy2014intriguingpropertiesneuralnetworks,
      title={Intriguing properties of neural networks}, 
      author={Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus},
      year={2014},
      eprint={1312.6199},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1312.6199}, 
}
@article{yuan2020expert,
  title={Expert finding in community question answering: a review},
  author={Yuan, Sha and Zhang, Yu and Tang, Jie and Hall, Wendy and Cabot{\`a}, Juan Bautista},
  journal={Artificial Intelligence Review},
  volume={53},
  pages={843--874},
  year={2020},
  publisher={Springer}
}
@article{Gowal2020UncoveringTL,
  title={Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples},
  author={Sven Gowal and Chongli Qin and Jonathan Uesato and Timothy A. Mann and Pushmeet Kohli},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.03593},
  url={https://api.semanticscholar.org/CorpusID:222208628}
}
@article{Liu,
author = {Liu, Jessie and Gallego, Blanca and Barbieri, Sebastiano},
year = {2022},
month = {02},
pages = {1762},
title = {Incorporating uncertainty in learning to defer algorithms for safe computer-aided diagnosis},
volume = {12},
journal = {Scientific Reports},
doi = {10.1038/s41598-022-05725-7}
}
@misc{raman2021improvinglearningtodeferalgorithmsfinetuning,
      title={Improving Learning-to-Defer Algorithms Through Fine-Tuning}, 
      author={Naveen Raman and Michael Yee},
      year={2021},
      eprint={2112.10768},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2112.10768}, 
}
@article{Madry2017TowardsDL,
  title={Towards Deep Learning Models Resistant to Adversarial Attacks},
  author={Aleksander Madry and Aleksandar Makelov and Ludwig Schmidt and Dimitris Tsipras and Adrian Vladu},
  journal={ArXiv},
  year={2017},
  volume={abs/1706.06083},
  url={https://api.semanticscholar.org/CorpusID:3488815}
}
@inproceedings{mao2023crossentropylossfunctionstheoretical,
  title={Cross-entropy loss functions: Theoretical analysis and applications},
  author={Mao, Anqi and Mohri, Mehryar and Zhong, Yutao},
  booktitle={International conference on Machine learning},
  pages={23803--23828},
  year={2023},
  organization={PMLR}
}

@article{Steinwart2007HowTC,
  title={How to Compare Different Loss Functions and Their Risks},
  author={Ingo Steinwart},
  journal={Constructive Approximation},
  year={2007},
  volume={26},
  pages={225-287},
  url={https://api.semanticscholar.org/CorpusID:16660598}
}

@article{kwiatkowski2019natural,
  title={Natural Questions: a Benchmark for Question Answering Research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019}
}

@inproceedings{he2015deepresiduallearningimage,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@article{zhang2018generalizedcrossentropyloss,
  title={Generalized cross entropy loss for training deep neural networks with noisy labels},
  author={Zhang, Zhilu and Sabuncu, Mert},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}



@article{yang2018hotpotqa,
  title={HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering},
  author={Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and LeCun, Yann and Salakhutdinov, Ruslan and Cohen, William W},
  journal={arXiv preprint arXiv:1809.09600},
  year={2018}
}

@article{miller2016key,
  title={Key-value memory networks for directly reading documents},
  author={Miller, Alexander H and Fisch, Adam and Dodge, Jesse and Karimi, Amir-Hossein and Bordes, Antoine and Weston, Jason},
  journal={arXiv preprint arXiv:1606.03126},
  year={2016}
}

@article{seo2016bidirectional,
  title={Bidirectional attention flow for machine comprehension},
  author={Seo, Minjoon and Kembhavi, Aniruddha and Farhadi, Ali and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:1611.01603},
  year={2016}
}

@article{chen2017reading,
  title={Reading Wikipedia to Answer Open-Domain Questions},
  author={Chen, Danqi and Fisch, Adam and Weston, Jason and Bordes, Antoine},
  journal={arXiv preprint arXiv:1704.00051},
  year={2017}
}

@article{jia2017adversarial,
  title={Adversarial Examples for Evaluating Reading Comprehension Systems},
  author={Jia, Robin and Liang, Percy},
  journal={arXiv preprint arXiv:1707.07328},
  year={2017}
}

@article{wallace2019universal,
  title={Universal Adversarial Triggers for Attacking and Analyzing NLP},
  author={Wallace, Eric and Feng, Shi and Kandpal, Navin and Gardner, Matt and Singh, Sameer and Srikumar, Vivek and Riedel, Sebastian and Mitchell, Tom},
  journal={arXiv preprint arXiv:1908.07125},
  year={2019}
}
@article{bartlett1,
author = {Bartlett, Peter and Jordan, Michael and McAuliffe, Jon},
year = {2006},
month = {02},
pages = {138-156},
title = {Convexity, Classification, and Risk Bounds},
volume = {101},
journal = {Journal of the American Statistical Association},
doi = {10.1198/016214505000000907}
}

@misc{sheng2019woman,
      title={The Woman Worked as a Babysitter: On Biases in Language Generation},
      author={Emily Sheng and Kai-Wei Chang and Premkumar Natarajan and Nanyun Peng},
      year={2019},
      eprint={1909.01326},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{madras2018predict,
  title={Predict responsibly: improving fairness and accuracy by learning to defer},
  author={Madras, David and Pitassi, Toni and Zemel, Richard},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{lapin2016lossfunctionstopkerror,
  title={Loss functions for top-k error: Analysis and insights},
  author={Lapin, Maksim and Hein, Matthias and Schiele, Bernt},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1468--1477},
  year={2016}
}

@article{lapin2015topkmulticlasssvm,
  title={Top-k multiclass SVM},
  author={Lapin, Maksim and Hein, Matthias and Schiele, Bernt},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@misc{touvron2023llamaopenefficientfoundation,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13971}, 
}
@book{james1998majority,
  title={Majority vote classifiers: theory and applications},
  author={James, Gareth Michael},
  year={1998},
  publisher={Stanford University}
}

@article{west2016intelligent,
  title={Intelligent financial fraud detection: a comprehensive review},
  author={West, Jarrod and Bhattacharya, Maumita},
  journal={Computers \& security},
  volume={57},
  pages={47--66},
  year={2016},
  publisher={Elsevier}
}

@article{collins2007lung,
  title={Lung cancer: diagnosis and management},
  author={Collins, Lauren G and Haines, Christopher and Perkel, Robert and Enck, Robert E},
  journal={American family physician},
  volume={75},
  number={1},
  pages={56--63},
  year={2007}
}
@article{jiang1999improving,
  title={Improving breast cancer diagnosis with computer-aided diagnosis},
  author={Jiang, Yulei and Nishikawa, Robert M and Schmidt, Robert A and Metz, Charles E and Giger, Maryellen L and Doi, Kunio},
  journal={Academic radiology},
  volume={6},
  number={1},
  pages={22--33},
  year={1999},
  publisher={Elsevier}
}

@misc{bommasani2022opportunitiesrisksfoundationmodels,
      title={On the Opportunities and Risks of Foundation Models}, 
      author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren Gillespie and Karan Goel and Noah Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and Omar Khattab and Pang Wei Koh and Mark Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Ben Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and Julian Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Rob Reich and Hongyu Ren and Frieda Rong and Yusuf Roohani and Camilo Ruiz and Jack Ryan and Christopher Ré and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishnan Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tramèr and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
      year={2022},
      eprint={2108.07258},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2108.07258}, 
}

@misc{jiang2023mistral7b,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825}, 
}
@misc{kirillov2023segment,
      title={Segment Anything}, 
      author={Alexander Kirillov and Eric Mintun and Nikhila Ravi and Hanzi Mao and Chloe Rolland and Laura Gustafson and Tete Xiao and Spencer Whitehead and Alexander C. Berg and Wan-Yen Lo and Piotr Dollár and Ross Girshick},
      year={2023},
      eprint={2304.02643},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2304.02643}, 
}
@misc{ramesh2021zeroshottexttoimagegeneration,
      title={Zero-Shot Text-to-Image Generation}, 
      author={Aditya Ramesh and Mikhail Pavlov and Gabriel Goh and Scott Gray and Chelsea Voss and Alec Radford and Mark Chen and Ilya Sutskever},
      year={2021},
      eprint={2102.12092},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2102.12092}, 
}

@article{yue2023large,
  title={Large language model cascades with mixture of thoughts representations for cost-efficient reasoning},
  author={Yue, Murong and Zhao, Jie and Zhang, Min and Du, Liang and Yao, Ziyu},
  journal={arXiv preprint arXiv:2310.03094},
  year={2023}
}

@inproceedings{ding2024hybridllmcostefficientqualityaware,
  title={Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing},
  author={Ding, Dujian and Mallick, Ankur and Wang, Chi and Sim, Robert and Mukherjee, Subhabrata and R{\"u}hle, Victor and Lakshmanan, Laks VS and Awadallah, Ahmed Hassan},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@misc{chen2023frugalgptuselargelanguage,
      title={FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance}, 
      author={Lingjiao Chen and Matei Zaharia and James Zou},
      year={2023},
      eprint={2305.05176},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.05176}, 
}

@inproceedings{
ong2025routellm,
title={Route{LLM}: Learning to Route {LLM}s from Preference Data},
author={Isaac Ong and Amjad Almahairi and Vincent Wu and Wei-Lin Chiang and Tianhao Wu and Joseph E. Gonzalez and M Waleed Kadous and Ion Stoica},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=8sSqNntaMr}
}

@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@article{lapin2016analysisoptimizationlossfunctions,
  title={Analysis and optimization of loss functions for multiclass, top-k, and multilabel classification},
  author={Lapin, Maksim and Hein, Matthias and Schiele, Bernt},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={7},
  pages={1533--1554},
  year={2017},
  publisher={IEEE}
}

@article{cascade_3,
  author  = {Mohammad Saberian and Nuno Vasconcelos},
  title   = {Boosting Algorithms for Detector Cascade Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {74},
  pages   = {2569--2605},
  url     = {http://jmlr.org/papers/v15/saberian14a.html}
}


@article{goodfellow2014multidigitnumberrecognitionstreet,
  title={Multi-digit number recognition from street view imagery using deep convolutional neural networks},
  author={Goodfellow, Ian J and Bulatov, Yaroslav and Ibarz, Julian and Arnoud, Sacha and Shet, Vinay},
  journal={arXiv preprint arXiv:1312.6082},
  year={2013}
}

@misc{radford2021learningtransferablevisualmodels,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.00020}, 
}

@article{loshchilov2019decoupledweightdecayregularization,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@inproceedings{early_exit,
author = {Laskaridis, Stefanos and Kouris, Alexandros and Lane, Nicholas D.},
title = {Adaptive Inference through Early-Exit Networks: Design, Challenges and Directions},
year = {2021},
isbn = {9781450385978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469116.3470012},
doi = {10.1145/3469116.3470012},
abstract = {DNNs are becoming less and less over-parametrised due to recent advances in efficient model design, through careful hand-crafted or NAS-based methods. Relying on the fact that not all inputs require the same amount of computation to yield a confident prediction, adaptive inference is gaining attention as a prominent approach for pushing the limits of efficient deployment. Particularly, early-exit networks comprise an emerging direction for tailoring the computation depth of each input sample at runtime, offering complementary performance gains to other efficiency optimisations. In this paper, we decompose the design methodology of early-exit networks to its key components and survey the recent advances in each one of them. We also position early-exiting against other efficient inference solutions and provide our insights on the current challenges and most promising future directions for research in the field.},
booktitle = {Proceedings of the 5th International Workshop on Embedded and Mobile Deep Learning},
pages = {1–6},
numpages = {6},
location = {Virtual, WI, USA},
series = {EMDL'21}
}

@INPROCEEDINGS{cascade_4,
  author={Viola, P. and Jones, M.},
  booktitle={Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001}, 
  title={Rapid object detection using a boosted cascade of simple features}, 
  year={2001},
  volume={1},
  number={},
  pages={I-I},
  keywords={Object detection;Face detection;Pixel;Detectors;Filters;Machine learning;Image representation;Focusing;Skin;Robustness},
  doi={10.1109/CVPR.2001.990517}}

@article{cascade_2,
  title={When does confidence-based cascade deferral suffice?},
  author={Jitkrittum, Wittawat and Gupta, Neha and Menon, Aditya K and Narasimhan, Harikrishna and Rawat, Ankit and Kumar, Sanjiv},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={9891--9906},
  year={2023}
}


@article{cascade_1,
  title={Language model cascades},
  author={Dohan, David and Xu, Winnie and Lewkowycz, Aitor and Austin, Jacob and Bieber, David and Lopes, Raphael Gontijo and Wu, Yuhuai and Michalewski, Henryk and Saurous, Rif A and Sohl-Dickstein, Jascha and others},
  journal={arXiv preprint arXiv:2207.10342},
  year={2022}
}


@article{fatima2017survey,
  title={Survey of machine learning algorithms for disease diagnostic},
  author={Fatima, Meherwar and Pasha, Maruf and others},
  journal={Journal of Intelligent Learning Systems and Applications},
  volume={9},
  number={01},
  pages={1},
  year={2017},
  publisher={Scientific Research Publishing}
}


@InProceedings{ensemble,
author="Dietterich, Thomas G.",
title="Ensemble Methods in Machine Learning",
booktitle="Multiple Classifier Systems",
year="2000",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--15",
abstract="Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classifier. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly.",
isbn="978-3-540-45014-6"
}

@article{montreuil2025optimalqueryallocationextractive,
  title={Optimal Query Allocation in Extractive QA with LLMs: A Learning-to-Defer Framework with Theoretical Guarantees},
  author={Yannis Montreuil and Shu Heng Yeo and Axel Carlier and Lai Xing Ng and Wei Tsang Ooi},
  journal={arXiv preprint arXiv:2410.15761},
  year={2025}}
}

@inproceedings{montreuil2025adversarialrobustnesstwostagelearningtodefer,
  title={Adversarial Robustness in Two-Stage Learning-to-Defer: Algorithms and Guarantees},
  author={Montreuil, Yannis and Carlier, Axel and Ng, Lai Xing and Ooi, Wei Tsang},
  booktitle={Forty-second International Conference on Machine Learning},
  year={2025}
}

@inproceedings{
cortes2024cardinalityaware,
title={Cardinality-Aware Set Prediction and Top-\$k\$ Classification},
author={Corinna Cortes and Anqi Mao and Christopher Mohri and Mehryar Mohri and Yutao Zhong},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=WAT3qu737X}
}
@InProceedings{pmlr-v162-thilagar22a,
  title = 	 {Consistent Polyhedral Surrogates for Top-k Classification and Variants},
  author =       {Thilagar, Anish and Frongillo, Rafael and Finocchiaro, Jessica J and Goodwill, Emma},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {21329--21359},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/thilagar22a/thilagar22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/thilagar22a.html},
  abstract = 	 {Top-$k$ classification is a generalization of multiclass classification used widely in information retrieval, image classification, and other extreme classification settings. Several hinge-like (piecewise-linear) surrogates have been proposed for the problem, yet all are either non-convex or inconsistent. For the proposed hinge-like surrogates that are convex (i.e., polyhedral), we apply the recent embedding framework of Finocchiaro et al. (2019; 2022) to determine the prediction problem for which the surrogate is consistent. These problems can all be interpreted as variants of top-$k$ classification, which may be better aligned with some applications. We leverage this analysis to derive constraints on the conditional label distributions under which these proposed surrogates become consistent for top-$k$. It has been further suggested that every convex hinge-like surrogate must be inconsistent for top-$k$. Yet, we use the same embedding framework to give the first consistent polyhedral surrogate for this problem.}
}


@inproceedings{Awasthi_Mao_Mohri_Zhong_2022_multi,
author = {Awasthi, Pranjal and Mao, Anqi and Mohri, Mehryar and Zhong, Yutao},
title = {Multi-class H-consistency bounds},
year = {2022},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present an extensive study of H-consistency bounds for multi-class classification. These are upper bounds on the target loss estimation error of a predictor in a hypothesis set H, expressed in terms of the surrogate loss estimation error of that predictor. They are stronger and more significant guarantees than Bayes-consistency, H-calibration or H-consistency, and more informative than excess error bounds derived for H being the family of all measurable functions. We give a series of new H-consistency bounds for surrogate multi-class losses, including max losses, sum losses, and constrained losses, both in the non-adversarial and adversarial cases, and for different differentiable or convex auxiliary functions used. We also prove that no non-trivial H-consistency bound can be given in some cases. To our knowledge, these are the first H-consistency bounds proven for the multi-class setting. Our proof techniques are also novel and likely to be useful in the analysis of other such guarantees.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {57},
numpages = {14},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}
@inproceedings{topk_gene,
author = {Cong, Gao and Tan, Kian-Lee and Tung, Anthony K. H. and Xu, Xin},
title = {Mining top-K covering rule groups for gene expression data},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066234},
doi = {10.1145/1066157.1066234},
abstract = {In this paper, we propose a novel algorithm to discover the top-k covering rule groups for each row of gene expression profiles. Several experiments on real bioinformatics datasets show that the new top-k covering rule mining algorithm is orders of magnitude faster than previous association rule mining algorithms.Furthermore, we propose a new classification method RCBT. RCBT classifier is constructed from the top-k covering rule groups. The rule groups generated for building RCBT are bounded in number. This is in contrast to existing rule-based classification methods like CBA [19] which despite generating excessive number of redundant rules, is still unable to cover some training data with the discovered rules. Experiments show that the RCBT classifier can match or outperform other state-of-the-art classifiers on several benchmark gene expression datasets. In addition, the top-k covering rule groups themselves provide insights into the mechanisms responsible for diseases directly.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {670–681},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{yang2020consistencytopksurrogatelosses,
  title={On the consistency of top-k surrogate losses},
  author={Yang, Forest and Koyejo, Sanmi},
  booktitle={International Conference on Machine Learning},
  pages={10727--10735},
  year={2020},
  organization={PMLR}
}

@misc{ruder2017overviewmultitasklearningdeep,
      title={An Overview of Multi-Task Learning in Deep Neural Networks}, 
      author={Sebastian Ruder},
      year={2017},
      eprint={1706.05098},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1706.05098}, 
}
@article{Statistical,
author = {Zhang, Tong},
year = {2002},
month = {12},
pages = {},
title = {Statistical Behavior and Consistency of Classification Methods based on Convex Risk Minimization},
volume = {32},
journal = {Annals of Statistics},
doi = {10.1214/aos/1079120130}
}


@inproceedings{Mozannar2023WhoSP,
  title={Who Should Predict? Exact Algorithms For Learning to Defer to Humans},
  author={Hussein Mozannar and Hunter Lang and Dennis Wei and Prasanna Sattigeri and Subhro Das and David A. Sontag},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:255941521}
}
@inproceedings{mao2023twostage,
title={Two-Stage Learning to Defer with Multiple Experts},
author={Anqi Mao and Christopher Mohri and Mehryar Mohri and Yutao Zhong},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=GIlsH0T4b2}
}
@misc{charusaie2022sample,
      title={Sample Efficient Learning of Predictors that Complement Humans},
      author={Mohammad-Amin Charusaie and Hussein Mozannar and David Sontag and Samira Samadi},
      year={2022},
      eprint={2207.09584},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{lan2020albertlitebertselfsupervised,
      title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations}, 
      author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
      year={2020},
      eprint={1909.11942},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.11942}, 
}
@misc{sanh2020distilbertdistilledversionbert,
      title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}, 
      author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
      year={2020},
      eprint={1910.01108},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1910.01108}, 
}
@inproceedings{allyouneed,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}
@article{tewari07a,
  author  = {Ambuj Tewari and Peter L. Bartlett},
  title   = {On the Consistency of Multiclass Classification Methods},
  journal = {Journal of Machine Learning Research},
  year    = {2007},
  volume  = {8},
  number  = {36},
  pages   = {1007--1025},
  url     = {http://jmlr.org/papers/v8/tewari07a.html}
}
@inproceedings{Zhang,
 author = {Zhang, Mingyuan and Agarwal, Shivani},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {16927--16936},
 publisher = {Curran Associates, Inc.},
 title = {Bayes Consistency vs. H-Consistency: The Interplay between Surrogate Loss Functions and the Scoring Function Class},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/c4c28b367e14df88993ad475dedf6b77-Paper.pdf},
 volume = {33},
 year = {2020}
}


@InProceedings{pmlr-v28-long13,
  title = 	 {Consistency versus Realizable H-Consistency for Multiclass Classification},
  author = 	 {Long, Phil and Servedio, Rocco},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {801--809},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/long13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/long13.html},
  abstract = 	 {A consistent loss function for multiclass classification is one such  that for any source of labeled examples, any tuple  of scoring functions that  minimizes the expected loss will have classification accuracy close to that  of the Bayes optimal classifier. While consistency has been proposed as a  desirable property for multiclass loss functions, we  give experimental and theoretical results exhibiting a  sequence of linearly separable data sources with the following property:  a multiclass classification algorithm which optimizes a loss function  due to Crammer and Singer (which is known not to be consistent) produces  classifiers whose expected error goes to 0, while the expected error  of an algorithm which optimizes a generalization of the loss  function used by LogitBoost (a loss function which is known to be consistent)  is bounded below by a positive constant.    We identify a property of a loss function, realizable  consistency with respect to a restricted class of scoring functions,  that accounts for this difference.   As our main technical results we show  that the Crammer–Singer loss function is  realizable consistent for the class of linear scoring functions, while  the generalization of LogitBoost is not.  Our result for LogitBoost is  a special case of a more general theorem that applies to several other  loss functions that have been proposed for multiclass classification.  }
}


@article{WANG202351,
title = {Pre-Trained Language Models and Their Applications},
journal = {Engineering},
volume = {25},
pages = {51-65},
year = {2023},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2022.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S2095809922006324},
author = {Haifeng Wang and Jiwei Li and Hua Wu and Eduard Hovy and Yu Sun},
keywords = {Pre-trained models, Natural language processing},
abstract = {Pre-trained language models have achieved striking success in natural language processing (NLP), leading to a paradigm shift from supervised learning to pre-training followed by fine-tuning. The NLP community has witnessed a surge of research interest in improving pre-trained models. This article presents a comprehensive review of representative work and recent progress in the NLP field and introduces the taxonomy of pre-trained models. We first give a brief introduction of pre-trained models, followed by characteristic methods and frameworks. We then introduce and analyze the impact and challenges of pre-trained models and their downstream applications. Finally, we briefly conclude and address future research directions in this field.}
}

@INPROCEEDINGS{silouhette,
  author={Shahapure, Ketan Rajshekhar and Nicholas, Charles},
  booktitle={2020 IEEE 7th International Conference on Data Science and Advanced Analytics (DSAA)}, 
  title={Cluster Quality Analysis Using Silhouette Score}, 
  year={2020},
  volume={},
  number={},
  pages={747-748},
  keywords={Clustering algorithms;Iris;Electrical engineering;Computer science;Benchmark testing;Inspection;Writing},
  doi={10.1109/DSAA49011.2020.00096}}


@article{Maaten2008VisualizingDU,
  title={Visualizing Data using t-SNE},
  author={Laurens van der Maaten and Geoffrey E. Hinton},
  journal={Journal of Machine Learning Research},
  year={2008},
  volume={9},
  pages={2579-2605},
  url={https://api.semanticscholar.org/CorpusID:5855042}
}
@INPROCEEDINGS{7253949,
  author={Kadam, Aniket D and Joshi, Shashank D and Shinde, Sachin V and Medhane, Sampat P},
  booktitle={2015 International Conference on Electrical, Electronics, Signals, Communication and Optimization (EESCO)}, 
  title={Notice of Removal: Question Answering Search engine short review and road-map to future QA Search Engine}, 
  year={2015},
  volume={},
  number={},
  pages={1-8},
  keywords={},
  doi={10.1109/EESCO.2015.7253949}}

@INPROCEEDINGS{9071832,
  author={Sreelakshmi, A.S. and Abhinaya, S.B. and Nair, Aishwarya and Jaya Nirmala, S.},
  booktitle={2019 Grace Hopper Celebration India (GHCI)}, 
  title={A Question Answering and Quiz Generation Chatbot for Education}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  keywords={Knowledge discovery;Knowledge based systems;Neural networks;Education;Computer science;Context-aware services;Pattern matching;Question Answering;Question Generation;Artificial Neural Networks (ANN);Text ranking;MS MARCO dataset;Stanford Coref Annotator},
  doi={10.1109/GHCI47972.2019.9071832}}


@article{Cortes1995SupportVector,
  title={Support-Vector Networks},
  author={Corinna Cortes and Vladimir Naumovich Vapnik},
  journal={Machine Learning},
  year={1995},
  volume={20},
  pages={273-297},
  url={https://api.semanticscholar.org/CorpusID:52874011}
}

@inproceedings{Alqifari,
    title = "Question Answering Systems Approaches and Challenges",
    author = "Alqifari, Reem",
    editor = "Kovatchev, Venelin  and
      Temnikova, Irina  and
      {\v{S}}andrih, Branislava  and
      Nikolova, Ivelina",
    booktitle = "Proceedings of the Student Research Workshop Associated with RANLP 2019",
    month = sep,
    year = "2019",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd.",
    url = "https://aclanthology.org/R19-2011",
    doi = "10.26615/issn.2603-2821.2019_011",
    pages = "69--75",
    abstract = "Question answering (QA) systems permit the user to ask a question using natural language, and the system provides a concise and correct answer. QA systems can be implemented for different types of datasets, structured or unstructured. In this paper, some of the recent studies will be reviewed and the limitations will be discussed. Consequently, the current issues are analyzed with the proposed solutions.",
}


@misc{raffel2019exploring,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Mohan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and Jernite, Yen and others},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}


@inproceedings{mao2025realizablehconsistentbayesconsistentloss,
 author = {Anqi Mao and Mehryar Mohri and Yutao Zhong},
 booktitle = {Forty-second International Conference on Machine Learning},
 title = {Mastering Multiple-Expert Routing: Realizable \$H\$-Consistency and Strong Guarantees for Learning to Defer},
 year = {2025}
}

@article{montreuil2025ask,
  title={Why Ask One When You Can Ask $ k $? Two-Stage Learning-to-Defer to the Top-$ k $ Experts},
  author={Montreuil, Yannis and Carlier, Axel and Ng, Lai Xing and Ooi, Wei Tsang},
  journal={arXiv preprint arXiv:2504.12988},
  year={2025}
}


@inproceedings{nguyen2025probabilistic,
  title={Probabilistic learning to defer: Handling missing expert annotations and controlling workload distribution},
  author={Nguyen, Cuong C and Do, Thanh-Toan and Carneiro, Gustavo},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025}
}
@book{hamilton2020time,
  title={Time series analysis},
  author={Hamilton, James D},
  year={2020},
  publisher={Princeton university press}
}
@article{joshi2021learning,
  title={Learning-to-defer for sequential medical decision-making under uncertainty},
  author={Joshi, Shalmali and Parbhoo, Sonali and Doshi-Velez, Finale},
  journal={arXiv preprint arXiv:2109.06312},
  year={2021}
}

@article{sezer2020financial,
  title={Financial time series forecasting with deep learning: A systematic literature review: 2005--2019},
  author={Sezer, Omer Berat and Gudelek, Mehmet Ugur and Ozbayoglu, Ahmet Murat},
  journal={Applied soft computing},
  volume={90},
  pages={106181},
  year={2020},
  publisher={Elsevier}
}
@article{welch1995introduction,
  title={An introduction to the Kalman filter},
  author={Welch, Greg and Bishop, Gary and others},
  year={1995},
  publisher={Chapel Hill, NC, USA}
}
@article{rabiner2003introduction,
  title={An introduction to hidden Markov models},
  author={Rabiner, Lawrence and Juang, Biinghwang},
  journal={ieee assp magazine},
  volume={3},
  number={1},
  pages={4--16},
  year={2003},
  publisher={IEEE}
}

##SLDS

@article{hu2024modeling,
  title={Modeling latent neural dynamics with gaussian process switching linear dynamical systems},
  author={Hu, Amber and Zoltowski, David and Nair, Aditya and Anderson, David and Duncker, Lea and Linderman, Scott},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={33805--33835},
  year={2024}
}

@inproceedings{geadah2024parsing,
  title={Parsing neural dynamics with infinite recurrent switching linear dynamical systems},
  author={Geadah, Victor and Pillow, Jonathan W and others},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{linderman2016recurrent,
  title={Recurrent switching linear dynamical systems},
  author={Linderman, Scott W and Miller, Andrew C and Adams, Ryan P and Blei, David M and Paninski, Liam and Johnson, Matthew J},
  journal={arXiv preprint arXiv:1610.08466},
  year={2016}
}

@article{ghahramani2000variational,
  title={Variational learning for switching state-space models},
  author={Ghahramani, Zoubin and Hinton, Geoffrey E},
  journal={Neural computation},
  volume={12},
  number={4},
  pages={831--864},
  year={2000},
  publisher={MIT Press}
}

@article{fox2008nonparametric,
  title={Nonparametric Bayesian learning of switching linear dynamical systems},
  author={Fox, Emily and Sudderth, Erik and Jordan, Michael and Willsky, Alan},
  journal={Advances in neural information processing systems},
  volume={21},
  year={2008}
}

@article{kossen2021self,
  title={Self-attention between datapoints: Going beyond individual input-output pairs in deep learning},
  author={Kossen, Jannik and Band, Neil and Lyle, Clare and Gomez, Aidan N and Rainforth, Thomas and Gal, Yarin},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={28742--28756},
  year={2021}
}

@inproceedings{mehta2022neural,
  title={Neural HMMs are all you need (for high-quality attention-free TTS)},
  author={Mehta, Shivam and Sz{\'e}kely, {\'E}va and Beskow, Jonas and Henter, Gustav Eje},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7457--7461},
  year={2022},
  organization={IEEE}
}

@article{bengio1994input,
  title={An input output HMM architecture},
  author={Bengio, Yoshua and Frasconi, Paolo},
  journal={Advances in neural information processing systems},
  volume={7},
  year={1994}
}
@article{russo2014learning,
  title={Learning to optimize via information-directed sampling},
  author={Russo, Daniel and Van Roy, Benjamin},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@misc{shumway2006time,
  title={Time Series Analysis and Its Applications: With R Examples},
  author={Shumway, Rober H},
  year={2006},
  publisher={Springer}
}

@article{kalman1960new,
  title={A new approach to linear filtering and prediction problems},
  author={Kalman, Rudolph Emil},
  year={1960}
}

#online
@inproceedings{li2010contextual,
  title={A contextual-bandit approach to personalized news article recommendation},
  author={Li, Lihong and Chu, Wei and Langford, John and Schapire, Robert E},
  booktitle={Proceedings of the 19th international conference on World wide web},
  pages={661--670},
  year={2010}
}
@techreport{rumelhart1985learning,
  title={Learning internal representations by error propagation},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  year={1985}
}

@misc{zhou2020neuralcontextualbanditsucbbased,
      title={Neural Contextual Bandits with UCB-based Exploration},
      author={Dongruo Zhou and Lihong Li and Quanquan Gu},
      year={2020},
      eprint={1911.04462},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1911.04462},
}
@inproceedings{dani2008stochastic,
  title={Stochastic linear optimization under bandit feedback},
  author={Dani, Varsha and Hayes, Thomas P and Kakade, Sham M},
  booktitle={21st Annual Conference on Learning Theory},
  number={101},
  pages={355--366},
  year={2008}
}

@article{neu2010online,
  title={Online Markov decision processes under bandit feedback},
  author={Neu, Gergely and Antos, Andras and Gy{\"o}rgy, Andr{\'a}s and Szepesv{\'a}ri, Csaba},
  journal={Advances in Neural Information Processing Systems},
  volume={23},
  year={2010}
}
