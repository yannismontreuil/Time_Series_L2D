\section{Experiments}
\label{section:experiments}

We evaluate the proposed factorized Switching LDS router (Section~\ref{sec:generative_model}) in the
online learning-to-defer setting of Section~\ref{sec:preliminaries}, focusing on three failure modes
of offline L2D: \emph{censored (bandit) feedback}, \emph{non-stationarity}, and \emph{dynamic expert
availability}. Throughout, at each round \(t\) the router observes \((\mathbf{x}_t,\mathcal{E}_t)\),
selects \(I_t\in\mathcal{E}_t\), and then observes \(\vy_t\) and the queried prediction
\(\widehat{\vy}_{t,I_t}\) (hence the realized residual \(e_t=e_{t,I_t}\)). Unless stated otherwise we
use squared loss and no query fees (\(\psi(e)=\lVert e\rVert_2^2\), \(\beta_k\equiv 0\)). We report
averages over multiple random seeds with standard errors.
Following standard Learning-to-Defer evaluations, we treat each expert as a fixed prediction rule
\(f_k\) and focus on learning the \emph{router} under partial feedback (e.g.,
\citet{mozannar2021consistent,Narasimhan,mao2024regressionmultiexpertdeferral}). In particular, experts
do not adapt online; non-stationarity and performance variation are induced by the environment and
expert error processes, and all methods must respect the time-varying availability set
\(\mathcal{E}_t\).

\subsection{Synthetic: Regime-Dependent Correlation and Information Transfer}
\label{sec:exp_synthetic_transfer}

\paragraph{Design goal.}
We construct a controlled routing instance in which (i) experts are \emph{correlated} in a
regime-dependent way, so that observing one expert should update beliefs about others (information
transfer; Proposition~\ref{prop:cross_update}); and (ii) one expert temporarily disappears and
re-enters, so that the maintained registry \(\mathcal{K}_t\) matters.

\paragraph{Environment (regimes, target, context).}
We use \(M=2\) regimes and deterministic switching in blocks of length \(L=150\) over horizon
\(T=3000\) such as $z_t \coloneqq 1 + \left\lfloor \frac{t-1}{L}\right\rfloor \bmod 2$.
The target follows a regime-dependent AR(1), and the context is the one-step lag:
\begin{equation}
\label{eq:exp_tri_cycle_ts}
y_t = 0.8\,y_{t-1} + d_{z_t} + \eta_t,\qquad \eta_t\sim\mathcal{N}(0,\sigma_y^2).
\end{equation}
We set the router's context to \(x_t\coloneqq y_{t-1}\).
The regime \(z_t\) is latent to the router: the router observes only \(x_t\) (before acting) and the
single queried prediction \(\hat y_{t,I_t}\) (after acting).

\paragraph{Experts.}
We use \(K=4\) experts indexed \(k\in\{0,1,2,3\}\). Expert \(k=1\) is removed from the available set \(\mathcal{E}_t\)
for a contiguous interval \(t\in[2000,2500]\) and then re-enters. Each expert is a one-step forecaster
\(\hat y_{t,k}=f_k(x_t)\) with a shared slope and expert-specific intercept plus noise:
\begin{equation}
\label{eq:exp_synth_expert_rule}
\hat y_{t,k} \coloneqq 0.8\,y_{t-1} + b_k + \varepsilon_{t,k}.
\end{equation}
We set \((b_0,b_1,b_2,b_3)=(d_1,d_1,d_2,d_2)\), so experts \(\{0,1\}\) are well-calibrated in regime
\(z_t=1\) and experts \(\{2,3\}\) are well-calibrated in regime \(z_t=2\).

To induce \emph{regime-dependent correlation} under bandit feedback, we generate the expert noises as
\[
\varepsilon_{t,k} \coloneqq s_{t,g(k)} + \tilde\varepsilon_{t,k},
\qquad
g(k)\coloneqq 1+\mathbf{1}\{k\in\{2,3\}\},
\]
with independent components \(s_{t,1},s_{t,2},(\tilde\varepsilon_{t,k})_{k}\) and regime-dependent
variances $s_{t,1}\sim\mathcal{N}(0,\sigma_{z_t,1}^2), s_{t,2}\sim\mathcal{N}(0,\sigma_{z_t,2}^2),
\tilde\varepsilon_{t,k}\sim\mathcal{N}(0,\sigma_{\mathrm{id}}^2)$, where \((\sigma_{1,1}^2,\sigma_{1,2}^2)=(\sigma_{\mathrm{hi}}^2,\sigma_{\mathrm{lo}}^2)\) and
\((\sigma_{2,1}^2,\sigma_{2,2}^2)=(\sigma_{\mathrm{lo}}^2,\sigma_{\mathrm{hi}}^2)\) with
\(\sigma_{\mathrm{hi}}^2\gg\sigma_{\mathrm{lo}}^2\). This makes experts \(\{0,1\}\) strongly
correlated in regime \(1\) and experts \(\{2,3\}\) strongly correlated in regime \(2\). We report the MSE of each expert in
Table~\ref{tab:exp_avg_costs}.


\paragraph{Compared methods.}
We compare our \textbf{L2D-SLDS} router under bandit feedback to the following baselines.
\emph{(i) Ablation:} L2D-SLDS without the shared global factor (set \(d_g=0\)), which removes
cross-expert information transfer and yields independent per-expert dynamics.
\emph{(ii) Contextual bandits:} LinUCB \citep{li2010contextual} and NeuralUCB \citep{zhou2020neuralcontextualbanditsucbbased},
which select \(I_t\in\mathcal{E}_t\) using only \((\mathbf{x}_t,\) past queried losses\()\) and do not
maintain an explicit latent regime/state model.
\emph{(iii) Full-feedback:} a full-feedback variant of L2D-SLDS and online
Learning-to-Defer baselines \citep{mao2024regressionmultiexpertdeferral,Narasimhan} that assume access
to all experts' predictions each round (hence are not feasible under censoring): standard L2D, and a
sliding-window L2D (L2D\_SW) with \(W=500\) taking the most recent data to handle
non-stationarity. We use an RNN encoder \citep{rumelhart1985learning} as a drop-in context
representation for methods that require learned features.

\begin{table}[ht]
\centering
\small
\setlength{\tabcolsep}{4pt}
\caption{Averaged cumulative cost \eqref{eq:routing_objective} on experiment (Section~\ref{sec:exp_synthetic_transfer}).
We report mean \(\pm\) standard error across five runs. Lower is better.}
\label{tab:exp_avg_costs}
\begin{tabular}{@{}lcc@{}}
\toprule
Method & Partial feedback & Full feedback \\
\midrule
\textbf{L2D-SLDS} & \(\mathbf{13.58 \pm 0.07}\) & \(\mathbf{10.17 \pm 0.01}\) \\
L2D-SLDS w/o \(\mathbf{g}_t\) & \(14.68 \pm 0.01\) & \(10.18 \pm 0.01\) \\
\midrule
L2D & \multicolumn{1}{c}{--} & \(16.69 \pm 0.25\) \\
L2D\_SW (\(W=500\)) & \multicolumn{1}{c}{--} & \(13.26 \pm 0.11\) \\
LinUCB & \(22.94 \pm 0.01\) & \(23.24 \pm 0.01\) \\
NeuralUCB & \(21.92 \pm 0.31\) & \(21.39 \pm 1.89\) \\
Random & \(26.13 \pm 0.25\) & \(26.13 \pm 0.25\)  \\
Always expert 0 & \(23.07\) & \multicolumn{1}{c}{--} \\
Always expert 1 & \(28.66\) & \multicolumn{1}{c}{--} \\
Always expert 2 & \(23.05\) & \multicolumn{1}{c}{--} \\
Always expert 3 & \(29.36\) & \multicolumn{1}{c}{--} \\
Oracle & \(9.04\) & \(9.04\) \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Results.}
Table~\ref{tab:exp_avg_costs} shows that \textbf{L2D-SLDS} achieves the lowest routing cost under
partial feedback (\(13.58\pm 0.07\)), improving over LinUCB/NeuralUCB by a wide margin and also
outperforming the best fixed expert. Crucially, it also beats the ablation that removes the shared
factor \(\mathbf{g}_t\) (\(14.68\pm 0.01\)), a \(\approx 7.5\%\) reduction, which directly supports
our central claim: under censoring, modeling a \emph{global} latent component enables
\emph{cross-expert information transfer} from a single queried residual (see Proposition \ref{prop:transfer}). Intuitively, \(\mathbf{g}_t\)
captures regime-dependent common shocks that couple experts; thus, querying one expert updates beliefs
about unqueried experts in a way that contextual bandits (which treat arms largely independently) and
independent per-expert dynamics cannot replicate. Under full feedback, the gap between L2D-SLDS and
its ablation essentially vanishes (\(\approx 10.17\)), as expected when all experts are observed and
explicit transfer is no longer the bottleneck; in this setting L2D-SLDS is close to the oracle and
substantially improves over full-feedback L2D and L2D\_SW, indicating that explicit latent
regime/state tracking is a more sample-efficient route to handling non-stationarity and expert
re-entry than frequent retraining.
