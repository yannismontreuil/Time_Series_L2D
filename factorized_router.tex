\section{Context-Aware Routing in Non-Stationary Environments}

\subsection{Problem Formulation}
\label{sec:preliminaries}

We study sequential expert routing over a finite horizon \(t\in [T]\coloneqq\{1,\dots,T\}\) with
\emph{bandit feedback on expert predictions (equivalently, per-expert costs)}: at each round, the
router observes the target \(y_t\) but only the queried expert's prediction; since the fee schedule is
known, this is equivalent to observing only the queried expert's cost. Let
\((\Omega,\mathcal{F},\mathbb{P})\) be a probability space supporting all random variables. Let
\(\mathcal{Y}\subseteq\mathbb{R}\) denote the target space.

\paragraph{Interaction protocol.}
The process unfolds in discrete rounds. At each time \(t\):
\begin{enumerate}
    \item \textbf{Context and availability:} the environment reveals a context vector
    \(\mathbf{x}_t \in \mathcal{X} \subseteq \mathbb{R}^d\) and a non-empty finite set of available
    experts \(\mathcal{E}_t \subseteq \mathcal{K}\), where \(\mathcal{K}\) is a universe of expert
    identities.
    \item \textbf{Action:} the router selects an expert index \(I_t \in \mathcal{E}_t\) using only
    decision-time information.
    \item \textbf{Feedback:} querying expert \(I_t\) returns a prediction \(\widehat{y}_{t,I_t}\);
    the environment then reveals the target \(y_t \in \mathcal{Y}\).
    The router incurs a cost associated with selecting expert \(I_t\).
\end{enumerate}

\paragraph{Expert registry and potential outcomes.}
To accommodate time-varying availability, we maintain a cumulative \emph{expert registry}
\begin{equation}
    \mathcal{K}_t \coloneqq \mathcal{K}_{t-1} \cup \mathcal{E}_t, \qquad \mathcal{K}_0=\varnothing .
\end{equation}
Distinct indices correspond to distinct persistent experts.
Each expert \(k\in\mathcal{K}\) implements an (unknown) prediction rule
\(f_k:\mathcal{X}\to\mathcal{Y}\), accessible only through queries.
We define the \emph{potential prediction} of expert \(k\) at time \(t\) as
\(\widehat{y}_{t,k}\coloneqq f_k(\mathbf{x}_t)\).
Due to bandit feedback, \(\widehat{y}_{t,k}\) is \emph{censored}: the router observes
\(\widehat{y}_{t,k}\) if and only if \(I_t=k\). For \(k\neq I_t\), \(\widehat{y}_{t,k}\) remains
counterfactual.

\paragraph{Residuals, loss, and (potential) cost.}
Define the (potential) realized residual of expert \(k\) at time \(t\) as
\begin{equation}
\label{eq:realized_residual}
    e_{t,k} \coloneqq \widehat{y}_{t,k}-y_t ,
\end{equation}
which is observed only for the queried expert \(k=I_t\).
The corresponding (potential) realized cost is
\begin{equation}
\label{eq:realized_cost}
    C_{t,k} \coloneqq \psi(e_{t,k}) + \beta_k ,
\end{equation}
where \(\psi:\mathbb{R}\to\mathbb{R}_{\ge 0}\) is a known convex loss (e.g., squared error) and
\(\beta_k\ge 0\) is a known, expert-specific query fee. The post-action feedback at round \(t\) is
$O_t \coloneqq (I_t,\widehat{y}_{t,I_t},y_t)$, which determines the observed cost \(C_{t,I_t}\).

\paragraph{Filtrations and policies.}
Let \(\mathcal{H}_{t}\coloneqq \big((\mathbf{x}_\tau,\mathcal{E}_\tau,O_\tau)\big)_{\tau=1}^{t}\)
be the interaction history up to the end of round \(t\).
Decisions are non-anticipative, i.e., made before observing \(O_t\).
We define the \emph{decision-time} sigma-algebra (information available before choosing \(I_t\)) as
\begin{equation}
    \mathcal{F}_t \coloneqq \sigma\!\left(\mathcal{H}_{t-1},\mathbf{x}_t,\mathcal{E}_t\right).
\end{equation}
A policy \(\pi=(\pi_t)_{t=1}^T\) is a sequence of decision rules where
\(\pi_t(\cdot \mid \mathcal{F}_t)\) is an \(\mathcal{F}_t\)-measurable distribution over
\(\mathcal{E}_t\).
The action is sampled as \(I_t \sim \pi_t(\cdot \mid \mathcal{F}_t)\), so that
\(I_t\in\mathcal{E}_t\) almost surely.

\paragraph{Non-stationarity and exogeneity.}
We do not assume i.i.d.\ data. Instead, the environment follows a sequence of time-varying conditional
laws \(\{\mathcal{D}_t\}_{t\ge 1}\) such that
\begin{equation}
    (\mathbf{x}_t,\mathcal{E}_t,y_t)\mid \sigma\big((\mathbf{x}_\tau,\mathcal{E}_\tau,y_\tau)_{\tau<t}\big)
    \sim \mathcal{D}_t\!\left(\cdot \,\middle|\, (\mathbf{x}_\tau,\mathcal{E}_\tau,y_\tau)_{\tau<t}\right).
\end{equation}
This captures \emph{non-stationarity}: \(\mathcal{D}_t\) may vary with \(t\).
We additionally assume \emph{exogeneity} (non-reactivity to routing): past routing actions affect which
expert predictions are observed, but do not influence the data-generating process. Formally, for all
\(t\) and measurable sets \(A\),
\begin{equation}
\begin{aligned}
\mathbb{P}\!\left((\mathbf{x}_t,\mathcal{E}_t,y_t)\in A \,\middle|\,
\sigma\big((\mathbf{x}_\tau,\mathcal{E}_\tau,y_\tau)_{\tau<t}\big)\vee\sigma(I_{1:t-1})\right)
& =\\
\mathbb{P}\!\left((\mathbf{x}_t,\mathcal{E}_t,y_t)\in A \,\middle|\,
\sigma\big((\mathbf{x}_\tau,\mathcal{E}_\tau,y_\tau)_{\tau<t}\big)\right).
\end{aligned}
\label{eq:exogeneity}
\end{equation}

\paragraph{Objective and myopic Bayes selector.}
Our goal is to sequentially select an available expert \(k \in \mathcal{E}_t\) that balances accuracy
(low \(\psi(e_{t,k})\)) and cost (low \(\beta_k\)). Over the horizon \(t\in[T]\), we seek a policy
\(\pi\) minimizing the expected cumulative cost
\begin{equation}
    J(\pi) \coloneqq \mathbb{E}\!\left[\sum_{t=1}^T C_{t,I_t}\right]
    = \mathbb{E}\!\left[\sum_{t=1}^T \left(\psi(e_{t,I_t})+\beta_{I_t}\right)\right].
\end{equation}
Conditioned on the decision-time information \(\mathcal{F}_t\), the \emph{myopic Bayes selector} chooses an
index minimizing the posterior expected instantaneous cost:
\begin{equation}
\label{eq:bayes_selection}
    k_t^{\star} \in \arg\min_{k\in\mathcal{E}_t} \mathbb{E}\!\left[C_{t,k} \mid \mathcal{F}_t\right].
\end{equation}
Computing or approximating \eqref{eq:bayes_selection} requires a predictive model for the residuals
\((e_{t,k})_{k\in\mathcal{E}_t}\) under bandit censoring. In subsequent sections, we introduce a
latent-state model that yields a tractable one-step-ahead predictive belief.

\subsection{Generative Model: Factorized Switching LDS}
\label{sec:generative_model}

To obtain a tractable predictive model under non-stationarity and bandit feedback, we model the
\emph{potential residuals} \(e_{t,k}=\widehat{y}_{t,k}-y_t\) from Section~\ref{sec:preliminaries}
with a \textbf{factorized switching linear dynamical system (SLDS)}. The central bottleneck is
censoring: at round \(t\) we observe only the queried residual
\(
e_t \coloneqq e_{t,I_t},
\)
while \((e_{t,k})_{k\neq I_t}\) remain counterfactual. If each expert were endowed with fully
independent dynamics, then unqueried experts would be propagated essentially ``open-loop'' and their
posterior uncertainty would not contract (and can increase under unstable dynamics). We address this
by decomposing expert reliability into (i) a \emph{shared} global factor \(\mathbf{g}_t\) that couples
experts and enables information transfer across them, and (ii) \emph{idiosyncratic} expert-specific
dynamics \(\mathbf{u}_{t,k}\).

At decision time (before querying any expert at round \(t\)), the model induces a one-step-ahead
predictive distribution \(p(e_{t,k}\mid \mathcal{F}_t)\) for each \(k\in\mathcal{E}_t\). For clarity,
we write \(e_{t,k}^{\mathrm{pred}}\) for the \emph{hypothetical} residual that would be observed if
expert \(k\) were queried at time \(t\); thus
\(e_{t,k}^{\mathrm{pred}}\sim p(e_{t,k}\mid \mathcal{F}_t)\) (and, when needed, conditional on \(z_t\)).
This predictive law marginalizes over latent-state uncertainty and is distinct from the conditional
emission in \eqref{eq:residual_emission}.

\subsubsection{Latent state hierarchy}
We represent non-stationarity via a two-level hierarchy separating systemic shifts from
expert-specific drifts.

\paragraph{Context-dependent regime switching.}
A discrete regime \(z_t\in\{1,\dots,M\}\) selects the active dynamical law (e.g., ``bull'' vs.\ ``crisis'').
While classical SLDSs often use a time-homogeneous transition matrix, we allow transitions to depend
on the observed context \(\mathbf{x}_t\), enabling the model to update its regime belief using
exogenous signals before observing the queried residual \(e_t\). We model
\[
\mathbb{P}(z_t=m \mid z_{t-1}=\ell,\mathbf{x}_t)=\Pi_\theta(\mathbf{x}_t)_{\ell m}.
\]
We parameterize the \emph{logits} of \(\Pi_\theta(\mathbf{x}_t)\) via a low-rank attention map.
Specifically, we compute \(Q_\theta(\mathbf{x}_t),K_\theta(\mathbf{x}_t)\in\mathbb{R}^{M\times d_{\mathrm{attn}}}\) and set
\[
S(\mathbf{x}_t)\coloneqq \frac{1}{\sqrt{d_{\mathrm{attn}}}}\,Q_\theta(\mathbf{x}_t)K_\theta(\mathbf{x}_t)^\top,
\]
so that \(\mathrm{rank}(S(\mathbf{x}_t))\le d_{\mathrm{attn}}\). Row \(\ell\) corresponds to the
previous regime and column \(m\) to the next regime, and the row-wise softmax yields a valid
transition matrix:
\begin{equation}
\label{eq:context_transitions}
\mathbb{P}(z_t=m \mid z_{t-1}=\ell,\mathbf{x}_t)
=
\frac{\exp(S_{\ell m}(\mathbf{x}_t))}{\sum_{j=1}^M \exp(S_{\ell j}(\mathbf{x}_t))}.
\end{equation}

\paragraph{Extension: Flexible regime switching (optional).}
The context-dependent transition \eqref{eq:context_transitions} does not condition on the model's
continuous belief state (e.g., the current posterior over \(\mathbf{g}_{t-1}\)). One may instead
consider a recurrent variant in which the transition logits depend on \(\mathbf{g}_{t-1}\), e.g.,
\[
\mathbb{P}(z_t=m \mid z_{t-1}=\ell,\mathbf{x}_t,\mathbf{g}_{t-1})
\propto
\exp\!\big([f_\theta(\mathbf{x}_t)]_{\ell m} + \mathbf{w}_{\ell m}^\top \mathbf{g}_{t-1}\big),
\]
with \(\mathbf{w}_{\ell m}\in\mathbb{R}^{d_g}\).
However, this breaks conjugacy because the filtering belief for \(\mathbf{g}_{t-1}\) is Gaussian
while the transition involves a softmax, requiring additional approximations. In this work we focus
on \eqref{eq:context_transitions} for tractable inference.

\paragraph{Global factor dynamics.}
We introduce a continuous latent state \(\mathbf{g}_t\in\mathbb{R}^{d_g}\) to capture shared,
time-varying structure affecting all experts. Under bandit feedback, only one expert is queried per
round, so learning fully independent expert models is statistically inefficient and cannot exploit
cross-expert correlations. The shared factor \(\mathbf{g}_t\) enables \emph{information transfer}:
observing \(e_t=e_{t,I_t}\) updates the posterior over \(\mathbf{g}_t\), which in turn refines the
predictive belief over \(e_{t,k}^{\mathrm{pred}}\) for unqueried experts \(k\neq I_t\).
Conditioned on \(z_t=m\), the global factor follows linear-Gaussian dynamics:
\begin{equation}
\label{eq:global_dynamics}
\mathbf{g}_t
=
\mathbf{A}^{(g)}_{m}\mathbf{g}_{t-1}+\mathbf{w}^{(g)}_{t},
\qquad
\mathbf{w}^{(g)}_{t}\sim\mathcal{N}(\mathbf{0},\mathbf{Q}^{(g)}_{m}),
\end{equation}
where \(\mathbf{A}^{(g)}_{m}\in\mathbb{R}^{d_g\times d_g}\) and
\(\mathbf{Q}^{(g)}_{m}\in\mathbb{S}^{d_g}_{++}\).
We assume \((\mathbf{w}^{(g)}_{t})_{t}\) are independent across time and independent of all other
process and emission noise terms.

\paragraph{Expert-specific dynamics.}
Each expert \(k\) also has an idiosyncratic latent state
\(\mathbf{u}_{t,k}\in\mathbb{R}^{d_\alpha}\) capturing expert-specific drift not explained by the
shared factor. Conditioned on \(z_t=m\),
\begin{equation}
\label{eq:idiosyncratic_dynamics}
\mathbf{u}_{t,k}
=
\mathbf{A}^{(u)}_{m}\mathbf{u}_{t-1,k}+\mathbf{w}^{(u)}_{t,k},
\quad
\mathbf{w}^{(u)}_{t,k}\sim\mathcal{N}(\mathbf{0},\mathbf{Q}^{(u)}_{m}),
\end{equation}
where conditional on \((z_t)\), the noise terms are independent across experts and time with
\(\mathbf{w}^{(u)}_{t,k}\mid z_t=m\sim\mathcal{N}(\mathbf{0},\mathbf{Q}^{(u)}_{m})\). To maintain
statistical strength in the sparse data regime, we share
\((\mathbf{A}^{(u)}_{m},\mathbf{Q}^{(u)}_{m})\) across experts, while preserving expert-specific
behavior through expert-specific loadings \(\mathbf{B}_k\) in \eqref{eq:alpha_def}.

\paragraph{Reliability composition and residual emission.}
We capture time- and context-dependent expert performance through a latent \emph{reliability} vector
\(\boldsymbol\alpha_{t,k}\in\mathbb{R}^{d_\alpha}\), decomposed into a shared component driven by the
global factor and an expert-specific drift:
\begin{equation}
\label{eq:alpha_def}
\boldsymbol\alpha_{t,k}\coloneqq \mathbf{B}_k\mathbf{g}_t+\mathbf{u}_{t,k},
\qquad
\mathbf{B}_k\in\mathbb{R}^{d_\alpha\times d_g}.
\end{equation}
Here, \(\mathbf{B}_k\mathbf{g}_t\) captures how expert \(k\) responds to system-wide conditions, while
\(\mathbf{u}_{t,k}\) accounts for idiosyncratic effects not explained by \(\mathbf{g}_t\).

Given a regime \(z_t=m\) and latent states \((\mathbf{g}_t,\mathbf{u}_{t,k})\), we posit that the
(potential) residual \(e_{t,k}\) is linear in context features \(\phi(\mathbf{x}_t)\) with Gaussian
noise, where \(\phi:\mathcal{X}\to\mathbb{R}^{d_\alpha}\) is a fixed feature map (optionally including
a constant term):
\begin{equation}
\label{eq:residual_emission}
e_{t,k}\mid (z_t=m,\mathbf{g}_t,\mathbf{u}_{t,k},\mathbf{x}_t)
\sim
\mathcal{N}\!\big(\phi(\mathbf{x}_t)^\top\boldsymbol\alpha_{t,k},\,R_{m,k}\big).
\end{equation}
The mean \(\phi(\mathbf{x}_t)^\top\boldsymbol\alpha_{t,k}\) represents a systematic, context-dependent
bias term, while \(R_{m,k}>0\) captures irreducible noise. We assume emission noise is conditionally
independent across experts and time given \((z_t,\mathbf{g}_t,(\mathbf{u}_{t,k})_k)\). Under bandit
feedback, the router observes
only the realized draw from \eqref{eq:residual_emission} corresponding to the queried expert,
\(e_t\coloneqq e_{t,I_t}\), at round \(t\); all other \(e_{t,k}\) remain counterfactual.


\paragraph{Selective information transfer via factorization.}
In the exact Bayesian filter, incorporating the observation \(e_t\) induces dependence between the
shared factor \(\mathbf{g}_t\) and the idiosyncratic states \((\mathbf{u}_{t,k})_k\), and hence across
experts. For scalability, our inference maintains a \emph{factorized} approximation: after each
update, we project the filtering belief back to a family in which (conditional on \(z_t\)) the
idiosyncratic states are independent across experts and independent of \(\mathbf{g}_t\).

\begin{restatable}[Information transfer under a shared factor]{proposition}{propinfo}
\label{prop:cross_update}
Fix \(t\) and \(z_t=m\). Condition on decision-time information \(\mathcal{F}_t\) and on the realized
action \(I_t\). Let \(j\neq I_t\) and let \((e_{t,j}^{\mathrm{pred}},e_{t,I_t}^{\mathrm{pred}})\) denote the one-step-ahead predictive
residuals under \(p(e_{t,\cdot}\mid \mathcal{F}_t,z_t=m)\). Assume this predictive distribution is
jointly Gaussian. Then
\[
\mathbb{E}\!\left[e_{t,j}^{\mathrm{pred}}\mid I_t,e_t,\mathcal{F}_t,z_t=m\right]
=
\mathbb{E}\!\left[e_{t,j}^{\mathrm{pred}}\mid I_t,\mathcal{F}_t,z_t=m\right]
\quad\Longleftrightarrow\quad
\mathrm{Cov}\!\left(e_{t,j}^{\mathrm{pred}},e_{t,I_t}^{\mathrm{pred}}\mid I_t,\mathcal{F}_t,z_t=m\right)=0.
\]
In particular, if the covariance is non-zero, then observing \(e_t=e_{t,I_t}\) updates the posterior
predictive mean of \(e_{t,j}^{\mathrm{pred}}\).
\end{restatable}

We prove Proposition~\ref{prop:cross_update} in Appendix~\ref{app:proof_cross_update}. The transfer is
\emph{selective}: observing the queried residual affects unqueried experts exactly when their predictive
residuals are correlated. In our factorized SLDS, this correlation is induced by the shared factor
\(\mathbf{g}_t\); for example, conditional on \((\mathcal{F}_t,z_t=m)\),
\(\mathrm{Cov}(e_{t,j}^{\mathrm{pred}},e_{t,i}^{\mathrm{pred}})\) contains the shared-factor term
\[
\phi(\mathbf{x}_t)^\top \mathbf{B}_j \Sigma^{(m)}_{g,t\mid t-1}\mathbf{B}_{i}^\top \phi(\mathbf{x}_t),
\]
where \(\Sigma^{(m)}_{g,t\mid t-1}\) is the one-step predictive covariance of \(\mathbf{g}_t\)
under regime \(m\). Thus, querying \(i=I_t\) tightens the predictive distribution of expert \(j\)
whenever the coupling through \(\mathbf{g}_t\) is non-negligible in the directions probed by
\(\phi(\mathbf{x}_t)\). Conversely, if this term is zero (and the idiosyncratic channel is independent),
then under the factorized predictive belief there is no information transfer from \(I_t\) to \(j\) at
time \(t\).

\subsubsection{Exploration via Information-Directed Sampling}
\label{sec:exploration}

Bandit feedback reveals only the residual of the queried expert, so the router must balance immediate
cost minimization with learning about the shared dynamics. We use
\textbf{Information-Directed Sampling (IDS)} to formalize this trade-off.

\paragraph{Predictive cost gap.}
At decision time, the model induces a one-step-ahead predictive distribution
\(p(e_{t,k}\mid \mathcal{F}_t)\) for each \(k\in\mathcal{E}_t\). We write
\[
e_{t,k}^{\mathrm{pred}} \sim p(e_{t,k}\mid \mathcal{F}_t)
\]
for a pre-query residual random variable distributed according to this predictive belief. If \(I_t=k\), the
realized observation \(e_t=e_{t,k}\) is a draw from this same law.

Define the corresponding predictive (virtual) cost random variable
\begin{equation}
\label{eq:virtual_cost_def}
C_{t,k}^{\mathrm{pred}}
\coloneqq
\psi(e_{t,k}^{\mathrm{pred}})+\beta_k,
\qquad k\in\mathcal{E}_t.
\end{equation}
Let the myopic baseline be
\begin{equation}
\label{eq:myopic_baseline_ids}
k_t^{\star} \in \arg\min_{k\in\mathcal{E}_t} \mathbb{E}\!\left[C_{t,k}^{\mathrm{pred}} \mid \mathcal{F}_t\right].
\end{equation}
The (predictive) value gap of choosing \(k\) is
\begin{equation}
\label{eq:model_gap}
\Delta_t(k)
\coloneqq
\mathbb{E}\!\left[ C_{t,k}^{\mathrm{pred}} - C_{t, k_t^\star}^{\mathrm{pred}} \,\middle|\, \mathcal{F}_t \right]
\ge 0,
\end{equation}
where nonnegativity follows from the definition of \(k_t^\star\).

\paragraph{Epistemic value: information gain about the shared factor.}
In the factorized SLDS, cross-expert information transfer is mediated by the shared factor
\(\mathbf{g}_t\) (Proposition~\ref{prop:cross_update}). We score expert \(k\) by the mutual
information between \(\mathbf{g}_t\) and the \emph{hypothetical} residual that would be observed if
\(k\) were queried:
\begin{equation}
\label{eq:ig_operational}
\mathrm{IG}_t(k)
\coloneqq
\mathcal{I}\!\left(\mathbf{g}_t;\, e_{t,k}^{\mathrm{pred}} \,\middle|\, \mathcal{F}_t\right).
\end{equation}
Equivalently, conditioning on the event \(I_t=k\), the realized observation \(e_t\) is a draw from
the same predictive law as \(e_{t,k}^{\mathrm{pred}}\), so
\(\mathrm{IG}_t(k)=\mathcal{I}(\mathbf{g}_t;\, e_t \mid \mathcal{F}_t, I_t=k)\).

Fix a regime \(z_t=m\). Under the factorized one-step-ahead predictive belief, the conditional
predictive law is Gaussian:
\begin{equation}
\label{eq:channel_form}
e_{t,k}^{\mathrm{pred}} \mid \mathbf{g}_t
\sim
\mathcal{N}\!\big(h_{t,k}^\top \mathbf{g}_t + b^{(m)}_{t,k},\, s^{(m)}_{t,k}\big),
\end{equation}
where \(h_{t,k}\coloneqq \mathbf{B}_k^\top \phi(\mathbf{x}_t)\in\mathbb{R}^{d_g}\),
\(b^{(m)}_{t,k}\coloneqq \phi(\mathbf{x}_t)^\top \mu^{(m)}_{u,k,t\mid t-1}\), and
\begin{equation}
\label{eq:noise_var}
s^{(m)}_{t,k}
\coloneqq
\phi(\mathbf{x}_t)^\top \Sigma^{(m)}_{u,k,t\mid t-1}\phi(\mathbf{x}_t) + R_{m,k}.
\end{equation}
The mutual information depends only on the covariance, so the mean offset \(b^{(m)}_{t,k}\) does not
affect \eqref{eq:ig_mode}.
Let \(\Sigma^{(m)}_{g,t\mid t-1}\) denote the one-step predictive covariance of \(\mathbf{g}_t\) under
regime \(m\). Then the mode-conditioned information gain is
\begin{equation}
\label{eq:ig_mode}
\mathcal{I}\!\left(\mathbf{g}_t;e_{t,k}^{\mathrm{pred}}\mid \mathcal{F}_t,z_t=m\right)
=
\frac12\log\!\left(1+\frac{ h_{t,k}^\top \Sigma^{(m)}_{g,t\mid t-1} h_{t,k} }{s^{(m)}_{t,k}}\right).
\end{equation}

To obtain a tractable score under regime uncertainty, we marginalize using the predictive regime
weights \(\bar{w}_t^{(m)} \coloneqq \mathbb{P}(z_t=m\mid\mathcal{F}_t)\) and define the computable proxy
\begin{equation}
\label{eq:ig_mix}
\mathrm{IG}_t(k)
\coloneqq
\sum_{m=1}^M \bar{w}_t^{(m)}\,
\mathcal{I}\!\left(\mathbf{g}_t;e_{t,k}^{\mathrm{pred}}\mid \mathcal{F}_t,z_t=m\right).
\end{equation}
(We reuse the notation \(\mathrm{IG}_t(k)\) for this mixture proxy to \eqref{eq:ig_operational}; it is
exact when \(z_t\) is known.)

\paragraph{Minimizing the information ratio.}
IDS selects the routing action by minimizing the squared information ratio
\begin{equation}
\label{eq:ids_deterministic}
I_t \in \arg\min_{k\in\mathcal{E}_t}\ \frac{\Delta_t(k)^2}{\mathrm{IG}_t(k)}.
\end{equation}
If \(\mathrm{IG}_t(k)=0\), we interpret the ratio as \(+\infty\) unless \(\Delta_t(k)=0\); if all
\(\mathrm{IG}_t(k)=0\), IDS reduces to the myopic choice in \eqref{eq:myopic_baseline_ids}.
This criterion prioritizes experts that are either near-optimal (small \(\Delta_t(k)\)) or highly
informative about the shared dynamics (large \(\mathrm{IG}_t(k)\)), accelerating adaptation under
censoring without auxiliary queries.

\subsubsection{The IMM filtering recursion}
The IMM maintains $M$ mode-conditioned Gaussian filters over the shared factor \(\mathbf{g}_t\) and
the idiosyncratic states \((\mathbf{u}_{t,k})_{k\in\mathcal{K}^{\text{work}}_t}\), along with regime
weights. At each round $t$, given context $\mathbf{x}_t$, it performs:

\begin{enumerate}
\item \textbf{Interaction (mixing).}
Let $w_{t-1}^i=\mathbb{P}(z_{t-1}=i\mid\mathcal{H}_{t-1})$ be the posterior regime probability from the previous step, and let
$\Pi_{i\to j}(\mathbf{x}_t)=\mathbb{P}(z_t=j\mid z_{t-1}=i,\mathbf{x}_t)$ be the transition matrix (Eq~\ref{eq:context_transitions}).
We compute the \emph{predicted regime probability} \(\bar{w}_t^j=\mathbb{P}(z_t=j\mid\mathcal{F}_t)\):
\[
\bar{w}_t^j = \sum_{i=1}^M \Pi_{i\to j}(\mathbf{x}_t)w_{t-1}^i.
\]
The \emph{mixing weights} (probability that the previous mode was $i$, given the current mode is $j$) are:
\[
\mu_{t-1}^{i|j}=\frac{\Pi_{i\to j}(\mathbf{x}_t)w_{t-1}^i}{\bar{w}_t^j}.
\]
For each target mode $j$, we compute the mixed continuous-state prior for \(\mathbf{g}_{t-1}\) and all
\((\mathbf{u}_{t-1,k})_{k\in\mathcal{K}^{\text{work}}_{t-1}}\) by moment-matching the mixture defined
by weights $\{\mu_{t-1}^{i|j}\}_{i=1}^M$.

\item \textbf{Time update.}
Each mode $j$ propagates its mixed Gaussian through the regime-specific dynamics
(Eq~\ref{eq:global_dynamics}, \ref{eq:idiosyncratic_dynamics}) to form the one-step predictive belief.

\item \textbf{Measurement update.}
After selecting $I_t$, we observe the residual $e_t=e_{t,I_t}$ and perform a Kalman correction in each
mode using the observation model for expert \(I_t\), updating \(\mathbf{g}_t\) and \(\mathbf{u}_{t,I_t}\)
while leaving \((\mathbf{u}_{t,k})_{k\neq I_t}\) at their predictive marginals. This yields likelihoods
\(\mathcal{L}_t^{(m)}=p(e_t\mid \mathcal{F}_t,I_t,z_t=m)\) and updated posterior regime weights:
\[
w_t^m=\frac{\mathcal{L}_t^{(m)}\bar{w}_t^m}{\sum_{\ell=1}^M \mathcal{L}_t^{(\ell)}\bar{w}_t^\ell}.
\]

\item \textbf{Output.}
The resulting belief is the weighted mixture of the $M$ mode-conditioned Gaussians; predictive costs
and information scores are computed by mixture-averaging using the regime weights.
\end{enumerate}




\subsubsection{Dynamic Registry Management}
\label{sec:registry}

In production, experts can appear and disappear abruptly (e.g., models are deployed/deprecated or
external providers churn). Routers with a fixed output head typically require either
(i) over-provisioning a superset of experts with masking, or (ii) architectural changes followed by
retraining, both of which hinder real-time adaptability.

Our Factorized SLDS instead maintains a dynamically sized \emph{working registry} of experts for which
we explicitly store idiosyncratic filtering marginals. We distinguish the cumulative registry
\(\mathcal{K}_t=\mathcal{K}_{t-1}\cup \mathcal{E}_t\) (Section~\ref{sec:preliminaries}) from a prunable
working set \(\mathcal{K}^{\text{work}}_t\subseteq \mathcal{K}_t\), initialized as
\(\mathcal{K}^{\text{work}}_0=\varnothing\) and updated online.

\paragraph{Pruning (dropping stored idiosyncratic marginals).}
Let \(\tau_{\mathrm{last}}(k)\in\{0,1,\dots,t-1\}\) be the last round at which expert \(k\) was queried
(with the convention \(\tau_{\mathrm{last}}(k)=0\) if \(k\) has never been queried).
We call an expert \emph{stale} if it is currently unavailable and has not been queried for more than
\(\Delta_{\max}\) steps, where \(\Delta_{\max}\ge 1\) is a user-chosen staleness horizon:
\begin{equation}
\label{eq:stale_set}
\mathcal{K}^{\mathrm{stale}}_t
\coloneqq
\left\{k\in \mathcal{K}^{\text{work}}_{t-1}\setminus \mathcal{E}_t:\ t-\tau_{\mathrm{last}}(k)>\Delta_{\max}\right\}.
\end{equation}
We prune by updating
\(\mathcal{K}^{\text{work}}_t \leftarrow \mathcal{K}^{\text{work}}_{t-1}\setminus \mathcal{K}^{\mathrm{stale}}_t\).
Operationally, pruning means we stop storing the idiosyncratic filtering marginal(s) associated with
\(\mathbf{u}_{t-1,k}\) (and hence do not propagate it forward) for \(k\in\mathcal{K}^{\mathrm{stale}}_t\).

Crucially, pruning does \emph{not} alter the maintained belief over the retained variables: it is
exact marginalization of dropped coordinates in the filtering distribution.

\begin{restatable}[Pruning does not affect retained experts]{proposition}{invariance}
\label{prop:invariance}
Fix time \(t\) and let \(P_t \subseteq \mathcal{K}^{\text{work}}_{t-1}\) be any set of experts to be pruned.
Let
\(
q_{t-1\mid t-1}\big(\mathbf{g}_{t-1},(\mathbf{u}_{t-1,\ell})_{\ell\in\mathcal{K}^{\text{work}}_{t-1}}\big)
\)
denote the (exact or approximate) filtering belief at the end of round \(t-1\) conditioned on the realized history.
Define the pruned belief by marginalization:
\begin{equation*}
    \begin{aligned}
        q^{\mathrm{pr}(P_t)}_{t-1\mid t-1}\big(\mathbf{g}_{t-1},(\mathbf{u}_{t-1,\ell})_{\ell\in\mathcal{K}^{\text{work}}_{t-1}\setminus P_t}\big)
& \coloneqq \\
\int q_{t-1\mid t-1}\big(\mathbf{g}_{t-1},(\mathbf{u}_{t-1,\ell})_{\ell\in\mathcal{K}^{\text{work}}_{t-1}}\big)\,
\prod_{k\in P_t} d\mathbf{u}_{t-1,k}.
    \end{aligned}
\end{equation*}
Then \(q^{\mathrm{pr}(P_t)}_{t-1\mid t-1}\) equals the marginal of \(q_{t-1\mid t-1}\) on the retained variables.
Consequently, after applying the standard SLDS time update to obtain the predictive belief at round \(t\),
the predictive distribution of \(\boldsymbol\alpha_{t,\ell}\) and the one-step predictive law of
\(e_{t,\ell}^{\mathrm{pred}}\) are identical before and after pruning, for every retained \(\ell\notin P_t\).
\end{restatable}

We defer the proof to Appendix~\ref{app:invariance}. If a pruned expert later reappears, we treat it
as a re-entry and reinitialize its idiosyncratic state; \(\Delta_{\max}\) controls the resulting
memory--accuracy trade-off.

\paragraph{Birth and re-entry (informed initialization).}
Let
\(\mathcal{E}^{\mathrm{init}}_t\coloneqq \mathcal{E}_t\setminus \mathcal{K}^{\text{work}}_{t-1}\)
denote experts that \emph{enter} the working registry at time \(t\) (either newly observed or
re-entering after pruning). For each \(j\in\mathcal{E}^{\mathrm{init}}_t\), we instantiate an
idiosyncratic state at the predictive time. When no side information is available, we use the
population prior:
\begin{equation}
\label{eq:birth_prior}
\mathbf{u}_{t,j}\mid(z_t=m)\sim \mathcal{N}\!\big(\mu^{(m)}_{\mathrm{pop}},\Sigma^{(m)}_{\mathrm{pop}}\big),
\qquad m\in[M].
\end{equation}
A convenient choice consistent with an ``initialize-then-time-update'' view is
\(
\mu^{(m)}_{\mathrm{pop}}=\mathbf{A}^{(u)}_m\mu_{\mathrm{pop}}
\)
and
\(
\Sigma^{(m)}_{\mathrm{pop}}=\mathbf{A}^{(u)}_m\Sigma_{\mathrm{pop}}\mathbf{A}^{(u)\top}_m+\mathbf{Q}^{(u)}_m,
\)
where \((\mu_{\mathrm{pop}},\Sigma_{\mathrm{pop}})\) are global hyperparameters.
This adds experts online without gradient-based retraining.

If side information is available for a re-entering expert \(j\), we can set a more specific Gaussian
prior \((\mu^{(m)}_{\mathrm{init},j},\Sigma^{(m)}_{\mathrm{init},j})\) that reflects that knowledge (e.g.,
historical performance or metadata). When no such information is available, we default to the
population prior \eqref{eq:birth_prior}. To make the expert immediately \emph{context-aware}, we assume its loading matrix
\(\mathbf B_j\) is available at entry (e.g., provided by metadata or an embedding model, or initialized
from a population prior); thus the shared factor \(\mathbf g_t\) inferred from past queried experts
couples into \(\boldsymbol\alpha_{t,j}=\mathbf B_j\mathbf g_t+\mathbf u_{t,j}\) from the first round of
availability.

\begin{restatable}[Coupling at birth through the shared factor]{proposition}{transfer}
\label{prop:transfer}
Fix time \(t\) and condition on \((\mathcal{F}_t,z_t=m)\). Under the Factorized SLDS one-step predictive
belief (i.e., with \(\mathrm{Cov}(\mathbf{g}_t,\mathbf{u}_{t,k}\mid\cdot)=\mathbf{0}\) and
\(\mathrm{Cov}(\mathbf{u}_{t,i},\mathbf{u}_{t,j}\mid\cdot)=\mathbf{0}\) for \(i\neq j\)), for any experts \(j\neq k\),
\[
\mathrm{Cov}\!\left(\boldsymbol\alpha_{t,j},\boldsymbol\alpha_{t,k}\mid \mathcal{F}_t,z_t=m\right)
=
\mathbf{B}_j\,\Sigma^{(m)}_{g,t\mid t-1}\,\mathbf{B}_k^\top,
\]
where \(\Sigma^{(m)}_{g,t\mid t-1}\) is the regime-\(m\) one-step predictive covariance of \(\mathbf{g}_t\).
In particular, if the joint predictive law is Gaussian and
\(\mathbf{B}_j\,\Sigma^{(m)}_{g,t\mid t-1}\,\mathbf{B}_k^\top\neq \mathbf{0}\),
then \(\boldsymbol\alpha_{t,j}\) and \(\boldsymbol\alpha_{t,k}\) are not independent and hence
\(\mathcal{I}(\boldsymbol\alpha_{t,j};\boldsymbol\alpha_{t,k}\mid \mathcal{F}_t,z_t=m)>0\).
\end{restatable}

We give the proof in Appendix~\ref{app:transfer}.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[
    node distance=1.5cm and 3cm,
    >=latex,
    thick,
    latent_node/.style={latent, minimum size=1cm},
    obs_node/.style={obs, minimum size=1cm},
    rcurinfo_node/.style={latent, minimum size=1cm},
    action_node/.style={draw, rectangle, minimum size=0.9cm},
    param_edge/.style={->, dashed, color=gray!80}
]

% --- NODES ---
\node[latent_node] (zt_prev) {$z_{t-1}$};
\node[latent_node, right=of zt_prev] (zt) {$z_t$};
\node[latent_node, right=of zt] (zt_next) {$z_{t+1}$};

\node[obs_node, above=1.2cm of zt] (xt) {$\mathbf{x}_t$};

\node[latent_node, below=1.8cm of zt_prev] (gt_prev) {$\mathbf{g}_{t-1}$};
\node[latent_node, right=of gt_prev] (gt) {$\mathbf{g}_t$};

\node[latent_node, below=1.8cm of gt_prev] (ut_prev) {$\mathbf{u}_{t-1,j}$};
\node[latent_node, right=of ut_prev] (ut) {$\mathbf{u}_{t,j}$};

\node[obs_node, below=1.5cm of ut] (lt) {$e_{t,j}$};

% Plate over maintained registry (working set)
\plate {experts} {(ut_prev)(ut)(lt)} {$j \in \mathcal{K}^{\text{work}}_t$};

% Availability and action
\node[obs_node, right=4cm of ut] (Kt) {$\mc{E}_t$};
\node[action_node, below=1.5cm of Kt] (rt) {$I_t$};

% --- EDGES ---
\edge {zt_prev} {zt};
\edge {zt} {zt_next};
\edge {gt_prev} {gt};
\edge {ut_prev} {ut};

\draw[param_edge] (zt) to [bend right=20] node[pos=0.3, midway, right, font=\tiny] {$A^{(g)}_{z_t},Q^{(g)}_{z_t}$} (gt);
\draw[param_edge] (zt.south east) to [bend left=45] node[pos=0.7, right, font=\tiny, xshift=2pt] {$A^{(u)}_{z_t},Q^{(u)}_{z_t}$} (ut.north east);

\draw[->] (gt) to [bend left=45] (lt);
\edge {ut} {lt};

\draw[param_edge]
  (xt)
  to
  node[pos=0.5, right, font=\tiny, xshift=2pt]
  {$\Pi_\theta(\cdot,\cdot\mid \mathbf{x}_t)$}
  (zt);

\draw[->] (xt.east) to [out=0, in=0, looseness=1] node[midway, right, font=\small] {$\phi(\mathbf{x}_t)$} (lt.east);

\edge {Kt} {rt};

% Selection affects what is observed (not what is generated)
\draw[->, dotted] (rt) -- (lt) node[midway, above, font=\tiny] {reveals};

\end{tikzpicture}
    \caption{Factorized SLDS with bandit feedback and \emph{context-dependent} regime switching:
\(p(z_t\mid z_{t-1},\mathbf{x}_t)\). The plate \(j\in\mathcal{K}^{\text{work}}_t\) indexes experts whose
idiosyncratic states are stored. Each \(e_{t,j}\) is a \emph{potential} residual, but only \(e_{t,I_t}\)
is revealed at round \(t\).}
    \label{fig:slds_pgm_final}
\end{figure}



\section{Predictive Resource Allocation}
\label{sec:scheduling}

In many operational settings, experts are scarce resources (e.g., human analysts, licensed APIs, or specialized compute nodes). Keeping an expert \emph{on-call} incurs an opportunity cost, as that resource could otherwise be deployed to background tasks. Releasing experts that are unlikely to be queried frees capacity but risks missing a good prediction. We therefore treat future demand probabilistically, rather than trying to identify a single ``exact'' future expert.

We formalize a \textbf{predictive scheduling} layer that operates above the instance-level router. At decision
time $t$, the scheduler selects an \emph{active-set trajectory}
\(
\mathcal{S}_{t:t+H} \coloneqq (\mathcal{S}_{t,h})_{h=1}^H
\)
with $\mathcal{S}_{t,h}\subseteq \mathcal{K}^{\text{work}}_t$, where $\mathcal{S}_{t,h}$ is the set of experts
kept on-call for time $t+h$. Our goal is to choose $\mathcal{S}_{t:t+H}$ so that, for each $h$, the
planner's predicted choice at time $t+h$ (defined below) falls in $\mathcal{S}_{t,h}$ with probability at
least $1-\delta$. We achieve this by estimating the planner's time-marginal demand and taking a
quantile-based coverage set at each lookahead. We distinguish the ideal schedule
$\mathcal{S}_{t:t+H}(\delta)$ (built from the true marginals) from its Monte Carlo estimate
$\widehat{\mathcal{S}}_{t:t+H}(\delta)$.

\subsection{Stochastic Demand Formulation}
\label{sec:demand}

Scheduling requires forecasting which experts will be needed over the next $H$ rounds under
uncertainty in future contexts and latent dynamics. We model future contexts by a (possibly
degenerate) forecast distribution $p(\mathbf{X}_{t+1:t+H}\mid \mathcal{F}_t)$, where
$\mathcal{F}_t=\sigma(\mathcal{H}_{t-1},\mathbf{x}_t,\mathcal{E}_t)$ is the decision-time information.

A key constraint is that \emph{planning itself must not trigger extra expert queries}. Querying experts
purely to evaluate their future performance would (i) immediately incur the query fees $\beta_k$ and
(ii) keep the experts busy, contradicting the purpose of scheduling (freeing scarce resources).



\paragraph{Known availability constraints.}
At time $t$, some future feasibility information may be known (e.g., calendars, rate limits, or
maintenance windows). We encode it by an $\mathcal{F}_t$-measurable family
$(\mathcal{A}_{t+h})_{h=1}^{H}$ with $\mathcal{A}_{t+h}\subseteq \mathcal{K}$, where
$\mathcal{A}_{t+h}$ is the set of experts known at time $t$ to be feasible at future time $t+h$.
Since the planner requires a predictive belief for each candidate expert, we restrict to those with
maintained idiosyncratic marginals at time $t$, i.e. the working registry $\mathcal{K}^{\text{work}}_t$:
\begin{equation}
\label{eq:feas_set}
\mathcal{E}^{\mathrm{feas}}_{t+h \mid t}
\coloneqq
\mathcal{K}^{\text{work}}_t \cap \mathcal{A}_{t+h},
\qquad h=1,\dots,H.
\end{equation}
If expert $j$ is known at time $t$ to be unavailable on Mondays, then for any
$h\in\{1,\dots,H\}$ such that $t+h$ falls on a Monday we set $j\notin\mathcal{A}_{t+h}$, hence
$j\notin\mathcal{E}^{\mathrm{feas}}_{t+h \mid t}$, and $j$ cannot be selected by planning at that time.

\paragraph{Model-predictive optimal path (planning at time $t$).}
Fix a candidate future context trajectory $\mathbf{x}_{t+1:t+H}$.
For each lookahead step $h\in\{1,\dots,H\}$ and $k\in\mathcal{E}^{\mathrm{feas}}_{t+h \mid t}$, the SLDS
induces an \emph{open-loop} predictive law for the potential residual, obtained by propagating the
time-$t$ belief forward without measurement updates and conditioning on the context prefix
$\mathbf{X}_{t+1:t+h}=\mathbf{x}_{t+1:t+h}$. Let $e_{t+h,k}^{\mathrm{pred}}$ denote the resulting
pre-query residual random variable. We score expert $k$ by its \emph{planning-time predicted cost}
\begin{align}
\label{eq:pred_cost}
C_{t+h,k}^{\dagger}(\mathbf{x}_{t+1:t+h})
&\coloneqq
\mathbb{E}\!\left[\psi\!\left(e_{t+h,k}^{\mathrm{pred}}\right)+\beta_k\ \middle|\ \mathcal{F}_t,\ \mathbf{X}_{t+1:t+h}=\mathbf{x}_{t+1:t+h}\right].
\end{align}
Equivalently, \(C_{t+h,k}^{\dagger}\) is the conditional expectation of
\(C_{t+h,k}^{\mathrm{pred}}\) from \eqref{eq:virtual_cost_def}.
Crucially, \eqref{eq:pred_cost} is computed \emph{without} querying experts during planning (no
$\widehat{y}_{t+h,k}$ is revealed) and \emph{without} observing the future target $y_{t+h}$; it depends
only on the time-$t$ SLDS belief and the hypothesized context prefix.

The \emph{predictive} choice at time $t+h$ is
\begin{equation}
\label{eq:pred_choice}
\widehat{I}_{t+h}(\mathbf{x}_{t+1:t+h})
\in
\arg\min_{k\in\mathcal{E}^{\mathrm{feas}}_{t+h \mid t}} C_{t+h,k}^{\dagger}(\mathbf{x}_{t+1:t+h}),
\end{equation}
with a fixed deterministic tie-break, and the corresponding planned path is
\begin{equation}
\label{eq:pred_path}
\widehat{I}_{t+1:t+H}(\mathbf{x}_{t+1:t+H})
\coloneqq
\bigl(\widehat{I}_{t+h}(\mathbf{x}_{t+1:t+h})\bigr)_{h=1}^{H}.
\end{equation}

\paragraph{Demand induced by context uncertainty.}
For a \emph{fixed} candidate future context trajectory $\mathbf{x}_{t+1:t+H}$, the planning rule in
\eqref{eq:pred_choice}--\eqref{eq:pred_path} is deterministic: it maps the context path to a single
planned expert sequence $\widehat{I}_{t+1:t+H}(\mathbf{x}_{t+1:t+H})$. Consequently, the only source
of randomness in the planned sequence is uncertainty about future contexts. Under the forecast model,
$\mathbf{X}_{t+1:t+H}\sim p(\cdot\mid \mathcal{F}_t)$, this induces a distribution over planned expert
paths. Formally, for any feasible index sequence
$\mathbf{i}\in\prod_{h=1}^{H}\mathcal{E}^{\mathrm{feas}}_{t+h \mid t}$,
\begin{equation}
\label{eq:path_dist}
\mathbb{P}\!\left(\widehat{I}_{t+1:t+H}(\mathbf{X}_{t+1:t+H})=\mathbf{i} \ \middle|\ \mathcal{F}_t\right).
\end{equation}
In practice, scheduling rarely needs the full path distribution. A convenient summary is the set of
time-marginal selection probabilities: for each lookahead step $h\in\{1,\dots,H\}$ and expert
$k\in\mathcal{K}^{\text{work}}_t$,
\begin{equation}
\label{eq:time_marginals}
\rho_{t,h}(k)
\coloneqq
\mathbb{P}\!\left(\widehat{I}_{t+h}=k \ \middle|\ \mathcal{F}_t\right).
\end{equation}
The collection $\bigl(\rho_{t,h}(k)\bigr)_{h,k}$ forms an
$H\times|\mathcal{K}^{\text{work}}_t|$ demand matrix; each row is supported on the feasible set
$\mathcal{E}^{\mathrm{feas}}_{t+h\mid t}$. When the forecast is sharp, a row concentrates on one
expert; when futures are ambiguous, mass spreads across multiple experts, motivating on-call sets that
retain several candidates at the same lookahead.

\subsection{Monte Carlo Planning for Predictive Scheduling}
\label{sec:mc_planning}

The demand objects in Section~\ref{sec:demand} are rarely available in closed form. Even for a fixed
lookahead step $h$, the planner's choice rule $\widehat{I}_{t+h}(\cdot)$ is defined by an
$\arg\min$ over predictive conditional expectations \eqref{eq:pred_cost}, and the resulting path
distribution \eqref{eq:path_dist} and time-marginals \eqref{eq:time_marginals} inherit this
nonlinearity through the discrete minimization. We therefore approximate these quantities by forward
Monte Carlo.

\paragraph{Sampling future context scenarios.}
We assume access to a conditional forecast law $p(\mathbf{X}_{t+1:t+H}\mid \mathcal{F}_t)$ and require
i.i.d.\ samples from it. Concretely, let $s_t$ denote a realized $\mathcal{F}_t$-measurable
information state (e.g., $s_t=(\mathcal{H}_{t-1},\mathbf{x}_t,\mathcal{E}_t)$). We assume a (possibly
learned) scenario generator $\mathcal{G}$ and i.i.d.\ noise $\epsilon^{(n)}\sim p_\epsilon$ with $n\leq N$ such that
$\mathcal{G}(s_t,\epsilon)\sim p(\cdot\mid \mathcal{F}_t)$. We then generate
\begin{equation}
\label{eq:mc_contexts}
\mathbf{X}^{(n)}_{t+1:t+H}
=
\mathcal{G}(s_t,\epsilon^{(n)}),
\qquad
\epsilon^{(n)}\stackrel{\text{i.i.d.}}{\sim}p_\epsilon
\end{equation}
In practice, $\mathcal{G}$ can be instantiated as a lightweight autoregressive model or a non-parametric sampler from historical trajectories, ensuring that the computational cost of planning does not exceed the cost of the experts we aim to prune.

\paragraph{Scenario-wise planned paths.}
Fix a sampled scenario $n$ with context trajectory $\mathbf{X}^{(n)}_{t+1:t+H}$. For each lookahead
step $h\in\{1,\dots,H\}$, we restrict attention to experts that are (i) in the working registry at
time $t$ and (ii) known feasible at time $t+h$, i.e.,
$k\in\mathcal{E}^{\mathrm{feas}}_{t+h \mid t}$ from \eqref{eq:feas_set}. Using the scenario prefix
$\mathbf{X}^{(n)}_{t+1:t+h}$, we evaluate the planning-time predicted costs
$C_{t+h,k}^{\dagger}(\mathbf{X}^{(n)}_{t+1:t+h})$ via \eqref{eq:pred_cost} for all
$k\in\mathcal{E}^{\mathrm{feas}}_{t+h \mid t}$, and then apply the deterministic decision rule for
$h \in \{1,\dots,H\}$,
\begin{equation}
\label{eq:mc_choice}
\widehat{I}^{(n)}_{t+h}
\in
\arg\min_{k\in\mathcal{E}^{\mathrm{feas}}_{t+h \mid t}}
C_{t+h,k}^{\dagger}\!\left(\mathbf{X}^{(n)}_{t+1:t+h}\right)
\end{equation}
using the same fixed tie-breaking as in \eqref{eq:pred_choice}. Importantly, this is \emph{pure
planning}: it does not query any expert during the lookahead (so no $\widehat{y}_{t+h,k}$ is
revealed), does not observe the future target $y_{t+h}$, and performs no measurement updates inside
the horizon. Collecting these per-time choices yields the scenario-wise planned expert path
\begin{equation}
\label{eq:mc_path}
\widehat{I}^{(n)}_{t+1:t+H}
\coloneqq
\bigl(\widehat{I}^{(n)}_{t+h}\bigr)_{h=1}^{H}
\in
\prod_{h=1}^{H}\mathcal{E}^{\mathrm{feas}}_{t+h \mid t}.
\end{equation}

\paragraph{Estimating time-marginal demand.}
We estimate the time-marginal demand profile by empirical frequencies. For each $h\in\{1,\dots,H\}$
and $k\in\mathcal{K}^{\text{work}}_t$,
\begin{equation}
\label{eq:mc_time_marginals}
\hat{\rho}_{t,h}(k)
\coloneqq
\frac{1}{N}\sum_{n=1}^N \mathbf{1}\!\left\{\widehat{I}^{(n)}_{t+h}=k\right\}.
\end{equation}
For fixed $h$, the vector $\bigl(\hat{\rho}_{t,h}(k)\bigr)_{k\in\mathcal{K}^{\text{work}}_t}$ is
a pmf supported on $\mathcal{E}^{\mathrm{feas}}_{t+h\mid t}$: it assigns non-zero mass to multiple
experts exactly when different scenarios yield different planned winners at time $t+h$. Conditioned
on $\mathcal{F}_t$, the conditional law of large numbers gives
$\hat{\rho}_{t,h}(k)\to \rho_{t,h}(k)$ almost surely as $N\to\infty$
for each $(h,k)$, under the context-sampling distribution actually used.

\paragraph{From time-marginals to an active-set trajectory.}
The time-marginals $\rho_{t,h}(\cdot)$ describe which expert the planner would select at time $t+h$ under
the forecast. To translate these marginals into an on-call decision, we construct for each $h$ a
(possibly multi-expert) \emph{coverage set} $\mathcal{S}_{t,h}(\delta)$ that contains the
highest-probability experts summing to at least $1-\delta$.

Fix a tolerance $\delta\in[0,1)$. For each $h\in\{1,\dots,H\}$, let $k_{(1)}, k_{(2)}, \dots$ denote experts
sorted so that $\rho_{t,h}(k_{(1)})\ge \rho_{t,h}(k_{(2)})\ge \cdots$. Define the minimum set size
\begin{equation}
K_h \coloneqq \min \left\{ K : \sum_{i=1}^K \rho_{t,h}(k_{(i)}) \ge 1-\delta \right\},
\end{equation}
and set $\mathcal{S}_{t,h}(\delta) \coloneqq \{ k_{(1)}, \dots, k_{(K_h)} \}$. The resulting
\emph{active-set trajectory} is
\begin{equation}
\label{eq:active_schedule}
\mathcal{S}_{t:t+H}(\delta)
\coloneqq
\bigl(\mathcal{S}_{t,h}(\delta)\bigr)_{h=1}^H,
\end{equation}
which prescribes the on-call set for each future time $t+h$.

By construction, $\sum_{k\in\mathcal{S}_{t,h}(\delta)} \rho_{t,h}(k)\ge 1-\delta$ for each $h$, so
\[
\mathbb{P}\!\left(\widehat{I}_{t+h}\in \mathcal{S}_{t,h}(\delta)\ \middle|\ \mathcal{F}_t\right)\ge 1-\delta,
\qquad h=1,\dots,H.
\]
Moreover, $\mathcal{S}_{t,h}(\delta)\subseteq \mathcal{E}^{\mathrm{feas}}_{t+h\mid t}$ for each $h$,
so each scheduled set is feasible and contained in $\mathcal{K}^{\text{work}}_t$.

\paragraph{Monte Carlo implementation.}
In practice we replace $\rho_{t,h}$ by its Monte Carlo estimate $\hat{\rho}_{t,h}$ in the same construction.
The resulting plug-in sets are denoted $\widehat{\mathcal{S}}_{t,h}(\delta)$, and the output schedule is
\(
\widehat{\mathcal{S}}_{t:t+H}(\delta)\coloneqq (\widehat{\mathcal{S}}_{t,h}(\delta))_{h=1}^H
\).
By the conditional law of large numbers, $\hat{\rho}_{t,h}(k)\to\rho_{t,h}(k)$ almost surely for each $(h,k)$,
so $\widehat{\mathcal{S}}_{t,h}(\delta)$ converges to $\mathcal{S}_{t,h}(\delta)$ except at ties.
For example, with $H=3$ and experts \(\{1,2,3\}\), a realized schedule might be
\(\widehat{\mathcal{S}}_{t:t+3}(\delta)=(\{2\},\{2,3\},\{1\})\), meaning that time $t+1$ calls for expert
2, time $t+2$ keeps \(\{2,3\}\) on-call, and time $t+3$ calls for expert 1. If we also include the
current round, we can prepend the singleton \(\{I_t\}\) at $h=0$. If a single static pool is required,
one can form $\widehat{\mathcal{S}}^{\cup}_t(\delta)\coloneqq\bigcup_{h=1}^H \widehat{\mathcal{S}}_{t,h}(\delta)$.
When uncertainty is high, the marginal mass can be spread across several experts and
$\widehat{\mathcal{S}}_{t,h}(\delta)$ may include multiple candidates; when demand concentrates, it typically
reduces to a single dominant expert.

\begin{algorithm}[H]
\caption{Predictive Scheduling via Monte Carlo}
\label{alg:predictive_schedule}
\begin{algorithmic}[1]
\STATE {\bfseries Input:} horizon $H$; scenarios $N$; tolerance $\delta$; working registry $\mathcal{K}^{\text{work}}_t$; feasibility sets $(\mathcal{A}_{t+h})_{h=1}^{H}$; forecast generator $\mathcal{G}$; time-$t$ SLDS belief.
\STATE Sample scenarios $\mathbf{X}^{(n)}_{t+1:t+H}$ via \eqref{eq:mc_contexts} for $n=1,\dots,N$.
\FOR{$n=1$ to $N$}
    \FOR{$h=1$ to $H$}
        \STATE $\mathcal{E}^{\mathrm{feas}}_{t+h\mid t} \leftarrow \mathcal{K}^{\text{work}}_t \cap \mathcal{A}_{t+h}$
        \STATE Compute $C_{t+h,k}^{\dagger}(\mathbf{X}^{(n)}_{t+1:t+h})$ for all $k\in\mathcal{E}^{\mathrm{feas}}_{t+h\mid t}$ via \eqref{eq:pred_cost}
        \STATE $\widehat{I}^{(n)}_{t+h} \in \arg\min_{k\in\mathcal{E}^{\mathrm{feas}}_{t+h\mid t}} C_{t+h,k}^{\dagger}(\mathbf{X}^{(n)}_{t+1:t+h})$
    \ENDFOR
\ENDFOR
\STATE Compute $\hat{\rho}_{t,h}(k)$ via \eqref{eq:mc_time_marginals} for all $h,k$.
\FOR{$h=1$ to $H$}
    \STATE Sort experts by $\hat{\rho}_{t,h}(k)$ and choose the smallest $K_h$ with cumulative mass $\ge 1-\delta$
    \STATE $\widehat{\mathcal{S}}_{t,h}(\delta) \leftarrow \{k_{(1)},\dots,k_{(K_h)}\}$
\ENDFOR
\STATE {\bfseries Return:} $\widehat{\mathcal{S}}_{t:t+H}(\delta)=(\widehat{\mathcal{S}}_{t,h}(\delta))_{h=1}^H$.
\end{algorithmic}
\end{algorithm}

\appendix
\onecolumn
\section{Algorithms} \label{algo}

% =========================
% Algorithm 0: Parameter Learning
% =========================
\begin{algorithm}[H]
\caption{\textsc{LearnParameter\_EM}: Monte Carlo EM for the Factorized SLDS (offline initialization or windowed batch; includes $\theta$ for context-dependent transitions)}
\label{alg:trainmodel_em}
\begin{algorithmic}[1]
\STATE {\bfseries Input:} index set $\mathcal{T}=\{t_a,\dots,t_b\}$ (contiguous window); contexts $(\mathbf{x}_t)_{t\in\mathcal{T}}$; available sets $(\mathcal{E}_t)_{t\in\mathcal{T}}$; actions $(I_t)_{t\in\mathcal{T}}$; observed residuals $(e_t)_{t\in\mathcal{T}}$ with $e_t=\widehat{y}_{t,I_t}-y_t$; feature map $\phi$; EM iterations $N_{\mathrm{EM}}$; posterior samples $N_{\mathrm{samp}}$; burn-in $N_{\mathrm{burn}}$; occupancy floor $\epsilon_N>0$; MAP priors $(M^{(g)}_A,\lambda^{(g)}_A,\Psi^{(g)},\nu^{(g)})$, $(M^{(u)}_A,\lambda^{(u)}_A,\Psi^{(u)},\nu^{(u)})$, $(M_B,\lambda_B)$, $(a_R,b_R)$, and $\lambda_\theta$ for the attention-based transition parameters $\theta$ in $\Pi_\theta(\mathbf{x}_t)$.
\STATE $\mathcal{K}\leftarrow \bigcup_{t\in\mathcal{T}} \mathcal{E}_t$
\STATE {\bfseries Initialize:} parameters $\Theta^{(0)}$; initial state priors for $z_{t_a},\mathbf{g}_{t_a},\{\mathbf{u}_{t_a,k}\}_{k\in\mathcal{K}}$.
\FOR{iteration $i = 1$ to $N_{\mathrm{EM}}$}
    \STATE \textbf{// E-STEP: Monte Carlo posterior}
    \STATE Initialize $(z_{t_a:t_b},\mathbf{g}_{t_a:t_b},\{\mathbf{u}_{t_a:t_b,k}\}_{k\in\mathcal{K}})$ from previous samples.
    \FOR{sample $s=1$ to $N_{\mathrm{burn}}+N_{\mathrm{samp}}$}
        \STATE Sample $z_{t_a:t_b}^{(s)}$ via FFBS using $\Pi_\theta(\mathbf{x}_t)$ and the emission likelihood of $e_t$ conditional on $(\mathbf{g}^{(s-1)}_{t_a:t_b},\{\mathbf{u}^{(s-1)}_{t_a:t_b,k}\}_k)$.
        \STATE Sample $\mathbf{g}_{t_a:t_b}^{(s)}$ via Kalman smoothing given $z_{t_a:t_b}^{(s)}$, $\{\mathbf{u}^{(s-1)}_{t_a:t_b,k}\}_k$, and the bandit observations $\{e_t\}$ (missing for $k\neq I_t$).
        \FOR{each expert $k\in\mathcal{K}$}
            \STATE Sample $\mathbf{u}_{t_a:t_b,k}^{(s)}$ via Kalman smoothing given $z_{t_a:t_b}^{(s)}$, $\mathbf{g}^{(s)}_{t_a:t_b}$, and the observations $\{e_t: I_t=k\}$ (missing otherwise).
        \ENDFOR
        \IF{$s>N_{\mathrm{burn}}$}
            \STATE Accumulate sufficient statistics for $(z_t,\mathbf{g}_t,\mathbf{u}_{t,k})$.
        \ENDIF
    \ENDFOR
    \STATE Form Monte Carlo responsibilities $\gamma_t^{(m)}\coloneqq \frac{1}{N_{\mathrm{samp}}}\sum_{s=1}^{N_{\mathrm{samp}}}\mathbf{1}\{z_t^{(s)}=m\}$ and sample-averaged moments; use $\mathbb{E}[\cdot]$ and $\mathbb{E}_m[\cdot]$ for unconditional and mode-conditional Monte Carlo averages.
    \STATE \textbf{// M-STEP: closed-form updates}
    \STATE For each mode $m$, compute $g^-\!=\!g_{t-1}$,
    $S^{(m)}_{g g^-}\!=\!\sum_{t\in\mathcal{T}\setminus\{t_a\}}\gamma_t^{(m)}\mathbb{E}[\mathbf{g}_t\mathbf{g}_{t-1}^\top]$,
    $S^{(m)}_{g^- g^-}\!=\!\sum_{t\in\mathcal{T}\setminus\{t_a\}}\gamma_t^{(m)}\mathbb{E}[\mathbf{g}_{t-1}\mathbf{g}_{t-1}^\top]$,
    and $N_m=\sum_{t\in\mathcal{T}\setminus\{t_a\}}\gamma_t^{(m)}$.
    \IF{$N_m \le \epsilon_N$}
        \STATE Keep $A_m^{(g)}$ and $Q_m^{(g)}$ unchanged.
    \ELSE
        \STATE $A_m^{(g)}\leftarrow\left(S^{(m)}_{g g^-}+\lambda^{(g)}_A M^{(g)}_A\right)\left(S^{(m)}_{g^- g^-}+\lambda^{(g)}_A I\right)^{-1}$.
        \STATE $S^{(m)}_{g\mid g^-}\leftarrow\sum_{t\in\mathcal{T}\setminus\{t_a\}}\gamma_t^{(m)}\mathbb{E}[(\mathbf{g}_t-A_m^{(g)}\mathbf{g}_{t-1})(\mathbf{g}_t-A_m^{(g)}\mathbf{g}_{t-1})^\top]$.
        \STATE $Q_m^{(g)}\leftarrow\frac{\Psi^{(g)}+S^{(m)}_{g\mid g^-}}{\nu^{(g)}+N_m+d_g+1}$.
    \ENDIF
    \STATE For each mode $m$, compute $u^-\!=\!u_{t-1,k}$,
    $S^{(m)}_{u u^-}\!=\!\sum_{k\in\mathcal{K}}\sum_{t\in\mathcal{T}\setminus\{t_a\}}\gamma_t^{(m)}\mathbb{E}[\mathbf{u}_{t,k}\mathbf{u}_{t-1,k}^\top]$,
    $S^{(m)}_{u^- u^-}\!=\!\sum_{k\in\mathcal{K}}\sum_{t\in\mathcal{T}\setminus\{t_a\}}\gamma_t^{(m)}\mathbb{E}[\mathbf{u}_{t-1,k}\mathbf{u}_{t-1,k}^\top]$,
    and $N^{(u)}_m=\sum_{k\in\mathcal{K}}\sum_{t\in\mathcal{T}\setminus\{t_a\}}\gamma_t^{(m)}$.
    \IF{$N^{(u)}_m \le \epsilon_N$}
        \STATE Keep $A_m^{(u)}$ and $Q_m^{(u)}$ unchanged.
    \ELSE
        \STATE $A_m^{(u)}\leftarrow\left(S^{(m)}_{u u^-}+\lambda^{(u)}_A M^{(u)}_A\right)\left(S^{(m)}_{u^- u^-}+\lambda^{(u)}_A I\right)^{-1}$.
        \STATE $S^{(m)}_{u\mid u^-}\leftarrow\sum_{k\in\mathcal{K}}\sum_{t\in\mathcal{T}\setminus\{t_a\}}\gamma_t^{(m)}\mathbb{E}[(\mathbf{u}_{t,k}-A_m^{(u)}\mathbf{u}_{t-1,k})(\mathbf{u}_{t,k}-A_m^{(u)}\mathbf{u}_{t-1,k})^\top]$.
        \STATE $Q_m^{(u)}\leftarrow\frac{\Psi^{(u)}+S^{(m)}_{u\mid u^-}}{\nu^{(u)}+N^{(u)}_m+d_u+1}$.
    \ENDIF
    \STATE For each expert $k$, compute $W_k\coloneqq \sum_{t\in\mathcal{T}:I_t=k}\sum_{m=1}^M \gamma_t^{(m)}$.
    \IF{$W_k \le \epsilon_N$}
        \STATE Keep $\mathbf{B}_k$ unchanged.
    \ELSE
        \STATE Define $x_t^{(m)}=\mathbb{E}_m[\mathbf{g}_t]\otimes\phi(\mathbf{x}_t)$,
        $y_t^{(m)}=e_t-\phi(\mathbf{x}_t)^\top\mathbb{E}_m[\mathbf{u}_{t,k}]$,
        and $w_t^{(m)}=\gamma_t^{(m)}/R_{m,k}$.
        \STATE
        \[
        \mathrm{vec}(\mathbf{B}_k)\leftarrow\left(\sum_{t\in\mathcal{T}:I_t=k}\sum_{m=1}^M w_t^{(m)} x_t^{(m)} x_t^{(m)\top}+\lambda_B I\right)^{-1}\left(\sum_{t\in\mathcal{T}:I_t=k}\sum_{m=1}^M w_t^{(m)} x_t^{(m)} y_t^{(m)}+\lambda_B\,\mathrm{vec}(M_B)\right).
        \]
    \ENDIF
    \STATE For each mode $m$ and expert $k$, compute $N_{m,k}\coloneqq\sum_{t\in\mathcal{T}:I_t=k}\gamma_t^{(m)}$.
    \IF{$N_{m,k} \le \epsilon_N$}
        \STATE Keep $R_{m,k}$ unchanged.
    \ELSE
        \STATE Assume prior $p(R)\propto R^{-(a_R+1)}e^{-b_R/R}$.
        \STATE
        \[
        R_{m,k}\leftarrow\frac{b_R+\tfrac12\sum_{t\in\mathcal{T}:I_t=k}\gamma_t^{(m)}\,r_{t,k}^{(m)2}}{a_R+\tfrac12 N_{m,k}+1},
        \qquad
        r_{t,k}^{(m)}\coloneqq e_t-\phi(\mathbf{x}_t)^\top(\mathbf{B}_k\mathbb{E}_m[\mathbf{g}_t]+\mathbb{E}_m[\mathbf{u}_{t,k}]),
        \]
    \ENDIF
    \STATE Update transition parameters $\theta$ (the attention logits in $\Pi_\theta(\mathbf{x}_t)$) by maximizing
    $\sum_{t\in\mathcal{T}\setminus\{t_a\}}\sum_{\ell,m}\xi_{t-1}^{(\ell,m)}\log \Pi_{\ell\to m}(\mathbf{x}_t)-\frac{\lambda_\theta}{2}\lVert\theta\rVert_2^2$
    with gradient ascent, where
    $\xi_{t-1}^{(\ell,m)}\coloneqq \frac{1}{N_{\mathrm{samp}}}\sum_{s=1}^{N_{\mathrm{samp}}}\mathbf{1}\{z_{t-1}^{(s)}=\ell,z_t^{(s)}=m\}$.
\ENDFOR
\STATE {\bfseries Return:} $\Theta^{(N_\mathrm{EM})}$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{\textsc{OnlineUpdate}: Sliding-Window Monte Carlo EM (non-stationary adaptation)}
\label{alg:online_em}
\begin{algorithmic}[1]
\STATE {\bfseries Input:} current time $t$; stream data $(\mathbf{x}_\tau,\mathcal{E}_\tau,I_\tau,e_\tau)_{\tau\le t}$; current parameters $\Theta^{(t-1)}$; window length $W$; update period $K$; small EM iterations $N_{\mathrm{EM}}^{\mathrm{win}}$; posterior samples $N_{\mathrm{samp}}$; burn-in $N_{\mathrm{burn}}$; occupancy floor $\epsilon_N$; priors and $\lambda_\theta$ as in Algorithm~\ref{alg:trainmodel_em}.
\STATE Define window $\mathcal{T}_t \leftarrow \{\max(1,t-W+1),\dots,t\}$.
\IF{$t < W$ \textbf{or} $t \bmod K \neq 0$}
    \STATE $\Theta^{(t)} \leftarrow \Theta^{(t-1)}$ and \textbf{return}.
\ENDIF
\STATE Initialize state priors at $\tau=\min(\mathcal{T}_t)$ from the current filter posteriors (or population priors if unavailable).
\STATE Run Algorithm~\ref{alg:trainmodel_em} on the window $\mathcal{T}_t$ with initialization $\Theta^{(t-1)}$, occupancy floor $\epsilon_N$, and $N_{\mathrm{EM}}^{\mathrm{win}}$ iterations.
\STATE {\bfseries Return:} updated parameters $\Theta^{(t)}$.
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Context-Aware Router (Factorized SLDS + IMM)}
\label{alg:router_main}
\begin{algorithmic}[1]
\STATE {\bfseries Input:} horizon $T$; parameters $\Theta$ (or an initial window $\mathcal{T}_0=\{1,\dots,t_0\}$ and EM settings for Algorithm~\ref{alg:trainmodel_em}); feature map $\phi$; contexts $(\mathbf{x}_t)_{t=1}^T$; expert sets $(\mathcal{E}_t)_{t=1}^T$; loss $\psi$; fees $(\beta_k)_k$; side information for entering experts (optional); online EM settings $(W,K,N_{\mathrm{EM}}^{\mathrm{win}},N_{\mathrm{samp}},N_{\mathrm{burn}},\epsilon_N)$ (optional).
\STATE {\bfseries Initialize:} mode weights $w_0^m\!=\!\mathbb{P}(z_0\!=\!m)$; global posteriors $(\mu^{(m)}_{g,0|0},\Sigma^{(m)}_{g,0|0})$ for $m\!\in\![M]$; working registry $\mathcal{K}^{\text{work}}_0\!\leftarrow\!\emptyset$; $\tau_{\mathrm{last}}(k)\!\leftarrow\!0$.
\STATE {\bfseries Hyperparameters:} population priors $(\mu^{(m)}_{\mathrm{pop}},\Sigma^{(m)}_{\mathrm{pop}})_{m=1}^M$; staleness $\Delta_{\max}$; mixing floor $\epsilon_w$.
\STATE {\bfseries Optional init:} if $\Theta$ is not provided, run Algorithm~\ref{alg:trainmodel_em} on $\mathcal{T}_0$ to obtain $\Theta$.
\STATE {\bfseries Optional init:} if $\mathcal{T}_0$ is used, run a forward filtering pass over $\mathcal{T}_0$ with $\Theta$ (using the IMM/Kalman updates) to set $(w_{t_0},\{\mu^{(m)}_{g,t_0|t_0},\Sigma^{(m)}_{g,t_0|t_0}\},\{\mu^{(m)}_{u,k,t_0|t_0},\Sigma^{(m)}_{u,k,t_0|t_0}\},\mathcal{K}^{\text{work}}_{t_0},\tau_{\mathrm{last}})$; set $t_{\mathrm{start}}\leftarrow t_0+1$. Otherwise set $t_{\mathrm{start}}\leftarrow 1$.
\FOR{$t=t_{\mathrm{start}}$ to $T$}
    \STATE Observe $(\mathbf{x}_t,\mathcal{E}_t)$.
    \STATE $(\mathcal{K}^{\text{work}}_t,\mathcal{E}^{\mathrm{init}}_t) \leftarrow \textsc{ManageRegistry}(\mathcal{K}^{\text{work}}_{t-1},\mathcal{E}_t,\tau_{\mathrm{last}},\Delta_{\max})$
    \STATE $\mathcal{K}^{\text{keep}}_t \leftarrow \mathcal{K}^{\text{work}}_t \setminus \mathcal{E}^{\mathrm{init}}_t$
    \STATE For each $k\in\mathcal{E}^{\mathrm{init}}_t$, set an entering prior $(\mu^{(m)}_{\mathrm{init},k},\Sigma^{(m)}_{\mathrm{init},k})_{m=1}^M$ from side information, defaulting to $(\mu^{(m)}_{\mathrm{pop}},\Sigma^{(m)}_{\mathrm{pop}})$.
    \STATE $(\bar w_t,\{\bar\mu^{(m)}_{g,t-1},\bar\Sigma^{(m)}_{g,t-1}\},\{\bar\mu^{(m)}_{u,k,t-1},\bar\Sigma^{(m)}_{u,k,t-1}\}_{k\in\mathcal{K}^{\text{keep}}_t}) \leftarrow \textsc{IMM}(\mathbf{x}_t,w_{t-1},\{\mu^{(m)}_{g,t-1|t-1},\Sigma^{(m)}_{g,t-1|t-1}\},\{\mu^{(m)}_{u,k,t-1|t-1},\Sigma^{(m)}_{u,k,t-1|t-1}\}_{k\in\mathcal{K}^{\text{keep}}_t},\epsilon_w)$
    \STATE $(\{\mu^{(m)}_{g,t|t-1},\Sigma^{(m)}_{g,t|t-1}\},\{\mu^{(m)}_{u,k,t|t-1},\Sigma^{(m)}_{u,k,t|t-1}\},\{\bar{C}_{t,k}^{\mathrm{pred},(m)}\}_{m\in[M],\,k\in\mathcal{E}_t}) \leftarrow \textsc{PredictAndScore}(\mathbf{x}_t,\mathcal{E}_t,\mathcal{K}^{\text{work}}_t,\mathcal{E}^{\mathrm{init}}_t,\{(\mu^{(m)}_{\mathrm{init},k},\Sigma^{(m)}_{\mathrm{init},k})\}_{k\in\mathcal{E}^{\mathrm{init}}_t},\{\bar\mu,\bar\Sigma\},\{\mu^{(m)}_{\mathrm{pop}},\Sigma^{(m)}_{\mathrm{pop}}\}_{m=1}^M,\Theta,\psi,\beta)$
    \FOR{each $k\in\mathcal{E}_t$}
        \STATE $\bar{C}_{t,k}^{\mathrm{pred}} \leftarrow \sum_{m=1}^M \bar w_t^{(m)}\,\bar{C}_{t,k}^{\mathrm{pred},(m)}$
    \ENDFOR
    \STATE $k_t^\star \in \arg\min_{k\in\mathcal{E}_t}\bar{C}_{t,k}^{\mathrm{pred}}$
    \FOR{each $k\in\mathcal{E}_t$}
        \STATE $\Delta_t(k)\leftarrow \bar{C}_{t,k}^{\mathrm{pred}} - \bar{C}_{t,k_t^\star}^{\mathrm{pred}}$
    \ENDFOR
    \FOR{each $k\in\mathcal{E}_t$}
        \STATE $\mathrm{IG}_t(k) \leftarrow 0$
    \ENDFOR
    \FOR{$m=1$ to $M$}
        \FOR{each $k\in\mathcal{E}_t$}
            \STATE $s^{(m)}_{t,k} \leftarrow \phi(\mathbf{x}_t)^\top \Sigma^{(m)}_{u,k,t\mid t-1}\phi(\mathbf{x}_t) + R_{m,k}$
            \STATE $\mathrm{IG}_t(k) \leftarrow \mathrm{IG}_t(k) + \bar{w}_t^{(m)}\,\frac12\log\!\left(1+ \phi(\mathbf{x}_t)^\top \mathbf{B}_k \Sigma^{(m)}_{g,t\mid t-1} \mathbf{B}_k^\top \phi(\mathbf{x}_t) / s^{(m)}_{t,k}\right)$
        \ENDFOR
    \ENDFOR
    \STATE {\bfseries Convention:} if $\mathrm{IG}_t(k)=0$, interpret $\Delta_t(k)^2/\mathrm{IG}_t(k)=+\infty$ unless $\Delta_t(k)=0$.
    \STATE Choose $I_t \in \arg\min_{k\in\mathcal{E}_t}\Delta_t(k)^2 / \mathrm{IG}_t(k)$
    \STATE Observe $(\widehat y_{t,I_t},y_t)$ and set $e_t\leftarrow \widehat y_{t,I_t}-y_t$; $\tau_{\mathrm{last}}(I_t)\leftarrow t$
    \STATE $(w_t,\{\mu^{(m)}_{g,t|t},\Sigma^{(m)}_{g,t|t}\},\{\mu^{(m)}_{u,k,t|t},\Sigma^{(m)}_{u,k,t|t}\}) \leftarrow \textsc{Correct}(\mathbf{x}_t,e_t,I_t,\bar w_t,\{\mu^{(m)}_{g,t|t-1},\Sigma^{(m)}_{g,t|t-1}\},\{\mu^{(m)}_{u,k,t|t-1},\Sigma^{(m)}_{u,k,t|t-1}\},\Theta,\mathcal{K}^{\text{work}}_t)$
    \IF{online adaptation is enabled}
        \STATE $\Theta \leftarrow \textsc{OnlineUpdate}(t,\{(\mathbf{x}_\tau,\mathcal{E}_\tau,I_\tau,e_\tau)\}_{\tau\le t},\Theta,W,K,N_{\mathrm{EM}}^{\mathrm{win}},N_{\mathrm{samp}},N_{\mathrm{burn}},\epsilon_N)$
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{\textsc{IMM}: Context-Dependent IMM Mixing (Moment Matching)}
\label{alg:imm_interact}
\begin{algorithmic}[1]
\STATE {\bfseries Input:} context $\mathbf{x}_t$; mode weights $w_{t-1}$; global posteriors $(\mu^{(i)}_{g,t-1|t-1},\Sigma^{(i)}_{g,t-1|t-1})$; idiosyncratic posteriors $(\mu^{(i)}_{u,k,t-1|t-1},\Sigma^{(i)}_{u,k,t-1|t-1})$ for $k\in\mathcal{K}^{\text{keep}}_t$; mixing floor $\epsilon_w>0$.
\STATE Compute $\Pi_{i\to j}(\mathbf{x}_t)=\mathrm{softmax}_j(S_{ij}(\mathbf{x}_t))$ for all $i,j\in[M]$
\STATE Predicted weights: $\bar w_t^j \leftarrow \sum_{i=1}^M \Pi_{i\to j}(\mathbf{x}_t)\, w_{t-1}^i$
\FOR{$j=1$ to $M$}
    \IF{$\bar w_t^j \le \epsilon_w$}
        \FOR{$i=1$ to $M$}
            \STATE $\mu_{t-1}^{i|j} \leftarrow 1/M$
        \ENDFOR
    \ELSE
        \FOR{$i=1$ to $M$}
            \STATE $\mu_{t-1}^{i|j} \leftarrow \Pi_{i\to j}(\mathbf{x}_t)\, w_{t-1}^i / \bar w_t^j$
        \ENDFOR
    \ENDIF
    \STATE $\bar\mu^{(j)}_{g,t-1} \leftarrow \sum_{i=1}^M \mu_{t-1}^{i|j}\mu^{(i)}_{g,t-1|t-1}$
    \STATE $\bar\Sigma^{(j)}_{g,t-1} \leftarrow \sum_{i=1}^M \mu_{t-1}^{i|j}\!\left(\Sigma^{(i)}_{g,t-1|t-1}+(\mu^{(i)}_{g,t-1|t-1}-\bar\mu^{(j)}_{g,t-1})(\mu^{(i)}_{g,t-1|t-1}-\bar\mu^{(j)}_{g,t-1})^\top\right)$
\FOR{each $k\in\mathcal{K}^{\text{keep}}_t$}
        \STATE $\bar\mu^{(j)}_{u,k,t-1} \leftarrow \sum_{i=1}^M \mu_{t-1}^{i|j}\mu^{(i)}_{u,k,t-1|t-1}$
        \STATE $\bar\Sigma^{(j)}_{u,k,t-1} \leftarrow \sum_{i=1}^M \mu_{t-1}^{i|j}\!\left(\Sigma^{(i)}_{u,k,t-1|t-1}+(\mu^{(i)}_{u,k,t-1|t-1}-\bar\mu^{(j)}_{u,k,t-1})(\mu^{(i)}_{u,k,t-1|t-1}-\bar\mu^{(j)}_{u,k,t-1})^\top\right)$
    \ENDFOR
\ENDFOR
\STATE {\bfseries Return:} $(\bar w_t,\{\bar\mu^{(m)}_{g,t-1},\bar\Sigma^{(m)}_{g,t-1}\},\{\bar\mu^{(m)}_{u,k,t-1},\bar\Sigma^{(m)}_{u,k,t-1}\}_{k\in\mathcal{K}^{\text{keep}}_t})$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{\textsc{PredictAndScore}: Mode-Wise Prediction and Myopic Costing}
\label{alg:predict_score}
\begin{algorithmic}[1]
\STATE {\bfseries Input:} $\mathbf{x}_t$, $\mathcal{E}_t$, $\mathcal{K}^{\text{work}}_t$, $\mathcal{E}^{\mathrm{init}}_t$, entering priors $\{(\mu^{(m)}_{\mathrm{init},k},\Sigma^{(m)}_{\mathrm{init},k})\}_{k\in\mathcal{E}^{\mathrm{init}}_t}$; mixed states $\{\bar\mu,\bar\Sigma\}$; population priors $(\mu^{(m)}_{\mathrm{pop}},\Sigma^{(m)}_{\mathrm{pop}})_{m=1}^M$; parameters $\Theta$ (including $A_m^{(g)},Q_m^{(g)},A_m^{(u)},Q_m^{(u)},\mathbf{B}_k,R_{m,k}$); loss $\psi$; fees $\beta$.
\STATE $\mathcal{K}^{\text{keep}}_t \leftarrow \mathcal{K}^{\text{work}}_t \setminus \mathcal{E}^{\mathrm{init}}_t$
\FOR{$m=1$ to $M$}
    \STATE $\mu^{(m)}_{g,t|t-1}\leftarrow A_m^{(g)}\bar\mu^{(m)}_{g,t-1}$,\;
    $\Sigma^{(m)}_{g,t|t-1}\leftarrow A_m^{(g)}\bar\Sigma^{(m)}_{g,t-1}(A_m^{(g)})^\top+Q_m^{(g)}$
    \FOR{each $k\in\mathcal{K}^{\text{keep}}_t$}
        \STATE $\mu^{(m)}_{u,k,t|t-1}\leftarrow A_m^{(u)}\bar\mu^{(m)}_{u,k,t-1}$,\;
        $\Sigma^{(m)}_{u,k,t|t-1}\leftarrow A_m^{(u)}\bar\Sigma^{(m)}_{u,k,t-1}(A_m^{(u)})^\top+Q_m^{(u)}$
    \ENDFOR
    \FOR{each $k\in\mathcal{E}^{\mathrm{init}}_t$}
        \STATE $\mu^{(m)}_{u,k,t|t-1}\leftarrow \mu^{(m)}_{\mathrm{init},k}$,\;
        $\Sigma^{(m)}_{u,k,t|t-1}\leftarrow \Sigma^{(m)}_{\mathrm{init},k}$
    \ENDFOR
    \FOR{each $k\in\mathcal{E}_t$}
        \STATE $\bar e_{t,k}^{\mathrm{pred},(m)} \leftarrow \mathbb{E}\!\left[e_{t,k}^{\mathrm{pred}} \mid \mathcal{F}_t, z_t=m\right]
        = \phi(\mathbf{x}_t)^\top(\mathbf{B}_k\mu^{(m)}_{g,t|t-1}+\mu^{(m)}_{u,k,t|t-1})$
        \STATE $S_{t,k}^{\mathrm{pred},(m)} \leftarrow \mathrm{Var}\!\left(e_{t,k}^{\mathrm{pred}} \mid \mathcal{F}_t, z_t=m\right)
        = \phi(\mathbf{x}_t)^\top(\mathbf{B}_k\Sigma^{(m)}_{g,t|t-1}\mathbf{B}_k^\top+\Sigma^{(m)}_{u,k,t|t-1})\phi(\mathbf{x}_t)+R_{m,k}$
        \STATE $\bar{C}_{t,k}^{\mathrm{pred},(m)}\leftarrow \mathbb{E}\!\left[C_{t,k}^{\mathrm{pred}} \mid \mathcal{F}_t, z_t=m\right]
        = \mathbb{E}_{e\sim\mathcal{N}(\bar e_{t,k}^{\mathrm{pred},(m)},S_{t,k}^{\mathrm{pred},(m)})}[\psi(e)]+\beta_k$
    \ENDFOR
\ENDFOR
\STATE {\bfseries Return:} $\{\mu^{(m)}_{g,t|t-1},\Sigma^{(m)}_{g,t|t-1}\}$, $\{\mu^{(m)}_{u,k,t|t-1},\Sigma^{(m)}_{u,k,t|t-1}\}$, and $\{\bar{C}_{t,k}^{\mathrm{pred},(m)}\}_{m\in[M],\,k\in\mathcal{E}_t}$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{\textsc{Correct}: Queried Kalman Update and Mode Posterior}
\label{alg:correct_reweight}
\begin{algorithmic}[1]
\STATE {\bfseries Input:} $\mathbf{x}_t$, queried residual $e_t$, queried expert $I_t$; predicted weights $\bar w_t$; predicted states $\{\mu^{(m)}_{g,t|t-1},\Sigma^{(m)}_{g,t|t-1}\}$ and $\{\mu^{(m)}_{u,k,t|t-1},\Sigma^{(m)}_{u,k,t|t-1}\}$; parameters (including $\mathbf{B}_{I_t},R_{m,I_t}$); registry $\mathcal{K}^{\text{work}}_t$.
\FOR{$m=1$ to $M$}
    \STATE $H_t \leftarrow [\phi(\mathbf{x}_t)^\top \mathbf{B}_{I_t}\;\;\phi(\mathbf{x}_t)^\top]$
    \STATE $\mu^{(m)}_{s,t|t-1} \leftarrow [(\mu^{(m)}_{g,t|t-1})^\top\;(\mu^{(m)}_{u,I_t,t|t-1})^\top]^\top$
    \STATE $\Sigma^{(m)}_{s,t|t-1} \leftarrow \mathrm{diag}(\Sigma^{(m)}_{g,t|t-1},\Sigma^{(m)}_{u,I_t,t|t-1})$
    \STATE $\bar e_{t,I_t}^{\mathrm{pred},(m)} \leftarrow H_t\mu^{(m)}_{s,t|t-1}$,\;
    $S_{t,I_t}^{\mathrm{pred},(m)} \leftarrow H_t\Sigma^{(m)}_{s,t|t-1}H_t^\top + R_{m,I_t}$
    \STATE $K_t^{(m)} \leftarrow \Sigma^{(m)}_{s,t|t-1}H_t^\top(S_{t,I_t}^{\mathrm{pred},(m)})^{-1}$
    \STATE $\mu^{(m)}_{s,t|t} \leftarrow \mu^{(m)}_{s,t|t-1} + K_t^{(m)}(e_t-\bar e_{t,I_t}^{\mathrm{pred},(m)})$
    \STATE $\Sigma^{(m)}_{s,t|t} \leftarrow \Sigma^{(m)}_{s,t|t-1} - K_t^{(m)}S_{t,I_t}^{\mathrm{pred},(m)}(K_t^{(m)})^\top$
    \STATE Extract $(\mu^{(m)}_{g,t|t},\Sigma^{(m)}_{g,t|t})$ and $(\mu^{(m)}_{u,I_t,t|t},\Sigma^{(m)}_{u,I_t,t|t})$ from $(\mu^{(m)}_{s,t|t},\Sigma^{(m)}_{s,t|t})$
    \FOR{each $k\in\mathcal{K}^{\text{work}}_t\setminus\{I_t\}$}
        \STATE $(\mu^{(m)}_{u,k,t|t},\Sigma^{(m)}_{u,k,t|t}) \leftarrow (\mu^{(m)}_{u,k,t|t-1},\Sigma^{(m)}_{u,k,t|t-1})$
    \ENDFOR
    \STATE Likelihood $\mathcal{L}_t^{(m)} \leftarrow \mathcal{N}(e_t;\bar e_{t,I_t}^{\mathrm{pred},(m)},S_{t,I_t}^{\mathrm{pred},(m)})$
\ENDFOR
\STATE Updated weights: $w_t^m \leftarrow \frac{\mathcal{L}_t^{(m)}\bar w_t^m}{\sum_{\ell=1}^M \mathcal{L}_t^{(\ell)}\bar w_t^\ell}$
\STATE {\bfseries Return:} $w_t$ and updated posteriors $\{\mu^{(m)}_{g,t|t},\Sigma^{(m)}_{g,t|t}\}$, $\{\mu^{(m)}_{u,k,t|t},\Sigma^{(m)}_{u,k,t|t}\}$
\end{algorithmic}
\end{algorithm}

% =========================
% Algorithm 5: Registry Management
% =========================
\begin{algorithm}[H]
\caption{\textsc{ManageRegistry}: Entering/Stale Experts}
\label{alg:registry}
\begin{algorithmic}[1]
\STATE {\bfseries Input:} $\mathcal{K}^{\text{work}}_{t-1}$, $\mathcal{E}_t$, $\tau_{\mathrm{last}}(\cdot)$, $\Delta_{\max}$.
\STATE $\mathcal{E}^{\mathrm{init}}_t \leftarrow \mathcal{E}_t \setminus \mathcal{K}^{\text{work}}_{t-1}$
\STATE $\mathcal{K}^{\mathrm{stale}}_t \leftarrow \{k\in\mathcal{K}^{\text{work}}_{t-1}\setminus\mathcal{E}_t:\ t-\tau_{\mathrm{last}}(k)>\Delta_{\max}\}$
\STATE $\mathcal{K}^{\text{work}}_t \leftarrow (\mathcal{K}^{\text{work}}_{t-1}\setminus\mathcal{K}^{\mathrm{stale}}_t)\cup \mathcal{E}^{\mathrm{init}}_t$
\STATE {\bfseries Side effect:} delete all stored states for $k\in\mathcal{K}^{\mathrm{stale}}_t$.
\STATE {\bfseries Return:} $\mathcal{K}^{\text{work}}_t$, $\mathcal{E}^{\mathrm{init}}_t$
\end{algorithmic}
\end{algorithm}

% \section{Time Complexity Analysis}

% Our router operates in real time with per-round cost linear in the number of active experts. Let $M$ denote the number of latent regimes, $d_g$ and $d_u$ the dimensions of the global and idiosyncratic latent states, and $|{\mathcal{K}}_t|$ the size of the working expert registry at time $t$. Assuming $d_g, d_u = O(1)$, the computational cost of each step in Algorithm~\ref{alg:router_main} is:

% \begin{itemize}
%     \item \textbf{Registry management (Alg.~\ref{alg:registry})}: $O(|{\mathcal{K}}_t|)$ for pruning and birth.
%     \item \textbf{IMM mixing (Alg.~\ref{alg:imm_interact})}: $O(M^2 |{\mathcal{K}}_t|)$ to compute mixing weights and moment-matched priors.
%     \item \textbf{Predict and score (Alg.~\ref{alg:predict_score})}: $O(M \max\{|\mathcal{E}_t|,|{\mathcal{K}}_t|\})$ to evaluate predictive costs for available experts.
%     \item \textbf{Correct (Alg.~\ref{alg:correct_reweight})}: $O(M|{\mathcal{K}}_t|)$ for Kalman update (only the selected expert $I_t$ is corrected).
% \end{itemize}

% Summing these, the total per-round complexity is $O(M^2 |{\mathcal{K}}_t|)$. In practice, with a fixed number of regimes ($M = O(1)$), this simplifies to $O(|{\mathcal{K}}_t|)$, which is linear in the working registry size.

\section{Proof of Proposition \ref{prop:cross_update}}
\label{app:proof_cross_update}

\propinfo*

\begin{proof}
Fix $t$ and $m$, and let $\mathcal{G}_t \coloneqq \sigma(\mathcal{F}_t, I_t, z_t=m)$.
By assumption, the one-step-ahead predictive vector
$(e_{t,j}^{\mathrm{pred}}, e_{t,I_t}^{\mathrm{pred}}) \mid \mathcal{G}_t$ is jointly Gaussian.
Let
\[
\mu_j \coloneqq \mathbb{E}[e_{t,j}^{\mathrm{pred}}\mid \mathcal{G}_t], \qquad
\mu_I \coloneqq \mathbb{E}[e_{t,I_t}^{\mathrm{pred}}\mid \mathcal{G}_t],
\]
and define the predictive variance and cross-covariance
\[
\sigma_I^2 \coloneqq \mathrm{Var}(e_{t,I_t}^{\mathrm{pred}}\mid \mathcal{G}_t), \qquad
\sigma_{jI} \coloneqq \mathrm{Cov}(e_{t,j}^{\mathrm{pred}}, e_{t,I_t}^{\mathrm{pred}}\mid \mathcal{G}_t).
\]
Assume $\sigma_I^2>0$ (e.g., due to additive observation noise).
For jointly Gaussian variables, the conditional expectation is given by the standard formula
\[
\mathbb{E}[e_{t,j}^{\mathrm{pred}}\mid e_t, \mathcal{G}_t]
=
\mu_j + \sigma_{jI}\,\sigma_I^{-2}\,\bigl(e_t-\mu_I\bigr).
\]
Therefore,
$\mathbb{E}[e_{t,j}^{\mathrm{pred}}\mid e_t, \mathcal{G}_t]=\mu_j$ for all values of $e_t$
if and only if $\sigma_{jI}=0$, i.e.,
$\mathrm{Cov}(e_{t,j}^{\mathrm{pred}}, e_{t,I_t}^{\mathrm{pred}}\mid \mathcal{F}_t, I_t, z_t=m)=0$.
\end{proof}

\section{Proof of Proposition \ref{prop:invariance}} \label{app:invariance}

\invariance*

\begin{proof}
The statement is a direct consequence of the definition of marginalization.

Write the filtering belief at the end of round $t-1$ (conditioned on the realized history, which we omit from the
notation) as a joint density over the shared factor and all idiosyncratic states:
\[
q_{t-1\mid t-1}\Big(\mathbf g_{t-1},(\mathbf u_{t-1,\ell})_{\ell\in\mathcal K^{\text{work}}_{t-1}}\Big).
\]
Let $\mathcal K' \coloneqq \mathcal K^{\text{work}}_{t-1}\setminus P_t$ denote the retained experts and denote
$\mathbf u_{t-1,\mathcal K'} \coloneqq (\mathbf u_{t-1,\ell})_{\ell\in\mathcal K'}$.
By the definition of a marginal density, the joint marginal of the retained variables under $q_{t-1\mid t-1}$ is
\begin{equation}
q_{t-1\mid t-1}\big(\mathbf g_{t-1},\mathbf u_{t-1,\mathcal K'}\big)
\;=\;
\int q_{t-1\mid t-1}\big(\mathbf g_{t-1},\mathbf u_{t-1,\mathcal K'},(\mathbf u_{t-1,k})_{k\in P_t}\big)\,
\prod_{k\in P_t} d\mathbf u_{t-1,k}.
\label{eq:marg_def}
\end{equation}
On the other hand, the post-pruning belief $q^{\mathrm{pr}(P_t)}_{t-1\mid t-1}$ is \emph{defined} by exactly the same integral:
\[
q^{\mathrm{pr}(P_t)}_{t-1\mid t-1}\big(\mathbf g_{t-1},\mathbf u_{t-1,\mathcal K'}\big)
\;\coloneqq\;
\int q_{t-1\mid t-1}\big(\mathbf g_{t-1},\mathbf u_{t-1,\mathcal K'},(\mathbf u_{t-1,k})_{k\in P_t}\big)\,
\prod_{k\in P_t} d\mathbf u_{t-1,k}.
\]
Comparing with \eqref{eq:marg_def} yields
\[
q^{\mathrm{pr}(P_t)}_{t-1\mid t-1}\big(\mathbf g_{t-1},\mathbf u_{t-1,\mathcal K'}\big)
=
q_{t-1\mid t-1}\big(\mathbf g_{t-1},\mathbf u_{t-1,\mathcal K'}\big),
\]
which proves that pruning $P_t$ leaves the joint belief over all retained variables unchanged.

For the stated consequences, let $\ell\notin P_t$.
The SLDS time update propagates $(\mathbf g_{t-1},\mathbf u_{t-1,\ell})$ to $(\mathbf g_t,\mathbf u_{t,\ell})$
using the same linear-Gaussian transition under both beliefs. Since the retained marginal
$q_{t-1\mid t-1}(\mathbf g_{t-1},\mathbf u_{t-1,\ell})$ is identical before and after pruning, the predictive
distribution of $(\mathbf g_t,\mathbf u_{t,\ell})$ is also identical. Because
$\boldsymbol{\alpha}_{t,\ell}=\mathbf B_\ell \mathbf g_t+\mathbf u_{t,\ell}$ is a measurable function of
$(\mathbf g_t,\mathbf u_{t,\ell})$ and $e_{t,\ell}^{\mathrm{pred}}$ follows the emission model given these
states, the predictive distributions of $\boldsymbol{\alpha}_{t,\ell}$ and $e_{t,\ell}^{\mathrm{pred}}$
are unchanged by pruning.
\end{proof}

\section{Proof of Proposition \ref{prop:transfer}} \label{app:transfer}

\transfer*

\begin{proof}
Fix $t$ and condition on $(\mathcal F_t,z_t=m)$.
Under the factorized one-step predictive belief,
\[
q(\mathbf g_t,(\mathbf u_{t,\ell})_\ell \mid \mathcal F_t,z_t=m)
=
q(\mathbf g_t\mid \mathcal F_t,z_t=m)\prod_{\ell} q(\mathbf u_{t,\ell}\mid \mathcal F_t,z_t=m),
\]
so $\mathbf g_t \perp\!\!\!\perp \mathbf u_{t,\ell}$ for all $\ell$ and
$\mathbf u_{t,j}\perp\!\!\!\perp \mathbf u_{t,k}$ for $j\neq k$.
Recalling $\boldsymbol\alpha_{t,\ell}=\mathbf B_\ell \mathbf g_t+\mathbf u_{t,\ell}$ and using bilinearity of covariance,
\begin{align*}
\mathrm{Cov}(\boldsymbol\alpha_{t,j},\boldsymbol\alpha_{t,k}\mid \mathcal F_t,z_t=m)
&=\mathrm{Cov}(\mathbf B_j\mathbf g_t+\mathbf u_{t,j},\,\mathbf B_k\mathbf g_t+\mathbf u_{t,k}\mid \mathcal F_t,z_t=m)\\
&=\mathrm{Cov}(\mathbf B_j\mathbf g_t,\,\mathbf B_k\mathbf g_t\mid \mathcal F_t,z_t=m)
+\mathrm{Cov}(\mathbf B_j\mathbf g_t,\,\mathbf u_{t,k}\mid \mathcal F_t,z_t=m)\\
&\quad+\mathrm{Cov}(\mathbf u_{t,j},\,\mathbf B_k\mathbf g_t\mid \mathcal F_t,z_t=m)
+\mathrm{Cov}(\mathbf u_{t,j},\,\mathbf u_{t,k}\mid \mathcal F_t,z_t=m)\\
&=\mathbf B_j\,\mathrm{Cov}(\mathbf g_t,\mathbf g_t\mid \mathcal F_t,z_t=m)\,\mathbf B_k^\top\\
&=\mathbf B_j\,\Sigma^{(m)}_{g,t|t-1}\,\mathbf B_k^\top,
\end{align*}
where $\Sigma^{(m)}_{g,t|t-1}\coloneqq \mathrm{Cov}(\mathbf g_t\mid \mathcal F_t,z_t=m)$.
If $\mathbf B_j\Sigma^{(m)}_{g,t|t-1}\mathbf B_k^\top\neq 0$ and the joint predictive law of
$(\boldsymbol\alpha_{t,j},\boldsymbol\alpha_{t,k})$ is Gaussian, then they are not independent, hence
$\mathcal I(\boldsymbol\alpha_{t,j};\boldsymbol\alpha_{t,k}\mid \mathcal F_t,z_t=m)>0$.
\end{proof}


% \section{Proof of Dynamic Structural Inference Properties}
% \label{app:proofs}

% In this section, we provide the formal derivation for Proposition~\ref{prop:plasticity}, confirming that our dynamic registry management (pruning and birth) is mathematically consistent with the underlying probabilistic graphical model.

% \propplasticity*

% \subsection{Proof of Part 1: Invariance to Pruning}
% \label{proof:part1}

% \textbf{Objective:} We must demonstrate that removing the idiosyncratic state $\mathbf{u}_{t,k}$ from the joint belief state does not alter the marginal posterior of the global factor $\mathbf{g}_t$, provided that expert $k$ is not observed at time $t$.

% \textbf{Step 1: Factorization of the Joint Prior.}
% Consider the joint predictive distribution at time $t$ given history $\mathcal{H}_{t-1}$. Due to the structural independence defined in the transition dynamics (Section \ref{sec:generative_model}), the joint prior factorizes conditional on the regime $z_t$. For notational simplicity, we omit the conditioning on $z_t=m$ as the result holds for every component of the mixture.
% The joint predictive density is:
% \begin{equation}
%     p(\mathbf{g}_t, \mathbf{u}_{t, 1}, \dots, \mathbf{u}_{t, K} \mid \mathcal{H}_{t-1}) \;=\; p(\mathbf{g}_t \mid \mathcal{H}_{t-1}) \prod_{i \in \mathcal{K}_t} p(\mathbf{u}_{t, i} \mid \mathcal{H}_{t-1}).
% \end{equation}
% Let $\mathbf{S}_t = [\mathbf{g}_t^\top, \mathbf{u}_{t,1}^\top, \dots, \mathbf{u}_{t,K}^\top]^\top$ denote the full latent state vector. The joint covariance matrix $\boldsymbol{\Sigma}_{t|t-1}$ is block-diagonal between the global factor and the idiosyncratic blocks.

% \textbf{Step 2: Bayesian Update under Partial Observation.}
% At time $t$, the router selects expert $I_t$ and observes the residual $r_{t, I_t}$. A "stale" expert $k \in \mathcal{K}^{\text{stale}}_t$ is, by definition, not in the available set $\mathcal{E}_t$, which implies $k \neq I_t$. Thus, $r_{t, I_t}$ depends only on $\mathbf{g}_t$ and $\mathbf{u}_{t, I_t}$.
% The observation model is:
% \begin{equation}
%     r_{t, I_t} = \mathbf{C}_{I_t} \mathbf{S}_t + v_t,
% \end{equation}
% where the observation matrix $\mathbf{C}_{I_t}$ has zero entries for all columns corresponding to $\mathbf{u}_{t, k}$.

% \textbf{Step 3: Analytical Marginalization.}
% We seek the marginal posterior $p(\mathbf{g}_t \mid \mathcal{H}_t)$. Using the definition of the posterior:
% \begin{align}
%     p(\mathbf{g}_t \mid \mathcal{H}_t) &= \int \dots \int p(\mathbf{S}_t \mid r_{t, I_t}) \, d\mathbf{u}_{t, 1} \dots d\mathbf{u}_{t, K} \\
%     &\propto \int \dots \int p(r_{t, I_t} \mid \mathbf{S}_t) p(\mathbf{S}_t \mid \mathcal{H}_{t-1}) \, d\mathbf{u}_{t, 1} \dots d\mathbf{u}_{t, K}.
% \end{align}
% Since the likelihood $p(r_{t, I_t} \mid \mathbf{S}_t)$ is independent of $\mathbf{u}_{t,k}$, we can isolate the integral with respect to $\mathbf{u}_{t,k}$:
% \begin{equation}
%     \int p(\mathbf{u}_{t,k} \mid \mathcal{H}_{t-1}) \, d\mathbf{u}_{t,k} = 1.
% \end{equation}
% The term associated with expert $k$ integrates to unity and vanishes from the equation. Consequently, the posterior update for $\mathbf{g}_t$ depends exclusively on the prior $p(\mathbf{g}_t \mid \mathcal{H}_{t-1})$, the active expert's prior $p(\mathbf{u}_{t,I_t} \mid \mathcal{H}_{t-1})$, and the likelihood. The presence or absence of $\mathbf{u}_{t,k}$ in the registry matrix has zero analytical impact on the posterior of $\mathbf{g}_t$.
% \hfill $\blacksquare$

% \subsection{Proof of Part 2: Non-Trivial Information Transfer}
% \label{proof:part2}

% \textbf{Objective:} We show that the mutual information $I(\boldsymbol{\alpha}_{t,j} ; \boldsymbol{\alpha}_{t, i} \mid \mathcal{H}_{t-1})$ is strictly positive for a new expert $j$ and an existing expert $i$.

% \textbf{Step 1: Representation of Reliability States.}
% Recall the definition of the reliability state for any expert $k$:
% \begin{equation}
%     \boldsymbol{\alpha}_{t,k} = \mathbf{B}_k \mathbf{g}_t + \mathbf{u}_{t,k}.
% \end{equation}
% Conditioned on history $\mathcal{H}_{t-1}$, $\mathbf{g}_t$ and the idiosyncratic noise terms $\mathbf{u}$ are independent Gaussian vectors. Let $\boldsymbol{\Sigma}_{g, t|t-1}$ denote the predictive covariance of the global factor.

% \textbf{Step 2: Cross-Covariance Derivation.}
% We compute the conditional cross-covariance between the new expert $j$ (initialized with a population prior) and an existing expert $i$:
% \begin{align}
%     \mathrm{Cov}(\boldsymbol{\alpha}_{t,j}, \boldsymbol{\alpha}_{t,i} \mid \mathcal{H}_{t-1}) &= \mathbb{E}\left[ (\mathbf{B}_j \tilde{\mathbf{g}}_t + \tilde{\mathbf{u}}_{t,j})(\mathbf{B}_i \tilde{\mathbf{g}}_t + \tilde{\mathbf{u}}_{t,i})^\top \right] \\
%     &= \mathbf{B}_j \mathbb{E}[\tilde{\mathbf{g}}_t \tilde{\mathbf{g}}_t^\top] \mathbf{B}_i^\top + \underbrace{\mathbb{E}[\tilde{\mathbf{u}}_{t,j} \tilde{\mathbf{g}}_t^\top]}_{0} \mathbf{B}_i^\top + \dots \\
%     &= \mathbf{B}_j \boldsymbol{\Sigma}_{g, t|t-1} \mathbf{B}_i^\top.
% \end{align}
% The cross-terms vanish because idiosyncratic states are independent of the global factor and of each other. Assuming non-trivial loading matrices $\mathbf{B}$ and a positive definite global covariance $\boldsymbol{\Sigma}_{g, t|t-1} \succ 0$, this cross-covariance matrix is non-zero.

% \textbf{Step 3: Connection to Mutual Information.}
% For jointly Gaussian variables $X, Y$, the mutual information is strictly positive if and only if they are correlated (i.e., $\mathrm{Cov}(X, Y) \neq \mathbf{0}$).
% Specifically, the mutual information is given by:
% \begin{equation}
%     I(\boldsymbol{\alpha}_{t,j} ; \boldsymbol{\alpha}_{t, i}) = -\frac{1}{2} \ln \left( \frac{|\boldsymbol{\Sigma}_{\alpha_j \alpha_i}|}{|\boldsymbol{\Sigma}_{\alpha_j}| |\boldsymbol{\Sigma}_{\alpha_i}|} \right) > 0.
% \end{equation}
% \textbf{Interpretation:} Expert $j$ is initialized with a generic prior on $\mathbf{u}_{t,j}$, but its effective reliability $\boldsymbol{\alpha}_{t,j}$ includes the term $\mathbf{B}_j \mathbf{g}_t$. Since the posterior of $\mathbf{g}_t$ has been refined by the history of observations from expert $i$, expert $j$ immediately inherits this information structure.
% \hfill $\blacksquare$
