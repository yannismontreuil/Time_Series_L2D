Learning to Defer For Time Series via Switching State-Space
Models

November 24, 2025

Abstract
We present a theoretical framework for Learning-to-Defer (L2D) in non-stationary time
series environments where querying experts incurs an explicit consultation cost. We formalize
the problem as a sequential decision process under partial observation with a time-varying
set of experts. We model the time-varying reliability of black-box experts (e.g., neural
networks) as latent states in a Switching Linear Dynamical System (SLDS). This structure
captures abrupt environmental shifts (regimes) and gradual performance drift. We derive a
tractable inference algorithm using the Interacting Multiple Model (IMM) filter, providing
full derivations for the variance spread in Gaussian mixtures, and propose a cost-sensitive
myopic selection policy that balances predictive risk, epistemic uncertainty, and operational
costs.

Note: might be interested to predict when the expert will be or not available?

1

Problem Formulation

1.1

Preliminaries

We consider a sequential forecasting problem over a discrete time horizon t = 1, . . . , T . The
system is defined on a probability space (Ω, F, P) equipped with a filtration F = {Ft }t≥0 ,
representing the information available to the router up to (and including) time t.
• Context and Target (Regression Setting). At each time t, nature reveals a context
vector xt ∈ X ⊆ Rd , which is Ft -measurable (known to the router at decision time). The
associated ground-truth target is a real-valued quantity
yt ∈ Y ⊆ R,
which is latent at decision time and becomes observable only through partial feedback
after the decision is made; in particular, yt is not Ft -measurable.
• Dynamic Expert Universe. Let U = {1, . . . , Nmax } denote the universe of all potential
experts (models, human specialists, or external systems). At each time t, only a subset
Kt ⊆ U is actually available for querying. The set Kt is allowed to vary over time, capturing
phenomena such as system downtime, onboarding or removal of models, or doctor shifts.
• Expert Predictions. Each expert j ∈ U is associated with a predictive function
fj : X → Y. At time t, any available expert j ∈ Kt produces a prediction
(j)

ŷt

:= fj (xt ),

based on the current context xt .
1

• Loss Process and Partial Feedback. Predictive performance is evaluated via a loss
function
L : Y × Y → [0, +∞),
e.g., squared error L(ŷ, y) = (ŷ − y)2 or absolute error L(ŷ, y) = |ŷ − y|. The instantaneous
loss of expert j at time t is the random variable
(j)

ℓj,t := L ŷt , yt ∈ [0, +∞).


We assume that the family {ℓj,t : j ∈ U, t ≥ 1} forms a well-defined stochastic process on
(Ω, F, P). Because only the queried expert is evaluated, after choosing an expert index
rt ∈ Kt at time t the router only observes the loss
ℓrt ,t ,
while the losses ℓj,t for j ̸= rt remain unobserved. In particular, ℓrt ,t is not contained in
Ft , but becomes available before the next decision epoch and is included in Ft+1 .
• Information History and Admissible Policies. For each t ≥ 1, define the feedback
random element

Ft := rt , ℓrt ,t ∈ U × [0, +∞).
The interaction history up to, but not including, time t is the finite sequence
It−1 := (xτ , Kτ , Fτ ) 1≤τ ≤t−1 ,


with the convention that I0 is the empty sequence. The information available to the router
at decision time t is summarized by the random element
Ht := It−1 , xt , Kt .


We assume that the observation filtration is generated by this information, i.e.
Ft = σ(Ht ),

t = 1, . . . , T.

A (deterministic) routing policy is a sequence of measurable mappings
πt : supp(Ht ) → U,

t = 1, . . . , T,

such that πt (Ht ) ∈ Kt almost surely. The chosen expert at time t is then
rt := πt (Ht ),
so in particular rt is Ft -measurable with values in Kt .
Running Example (Medical Triage). To ground these definitions, consider an automated
triage system in a hospital Emergency Room.
• The Context xt represents a patient’s initial vitals, laboratory values, and presenting
symptoms.
• The Target yt is a real-valued outcome of interest, e.g., a risk score or a biomarker value
observed after further testing.
• The Experts in Kt are the diagnostic models or human specialists currently available
(e.g., a chest-pain model, a sepsis model, an on-call cardiologist). As doctors start or end
their shifts, or as models are deployed/retired, the set Kt changes over time.
2

• For a given patient t assigned to specialist j, the Loss ℓj,t quantifies the discrepancy
between the prediction fj (xt ) and the realized outcome yt (e.g., squared error). Only the
loss of the chosen specialist j = rt is observed and enters subsequent decisions.
• Non-Stationarity arises from environmental shifts such as the onset of a flu epidemic,
changes in hospital protocols, or gradual degradation of a sensor’s calibration, which can
all alter the relative performance of experts over time.
The router must therefore select, at each time t, which expert rt ∈ Kt to query based solely
on the current patient’s features xt and the historical feedback It−1 , without access to the full
outcome yt at decision time.

1.2

The Decision Process

The system operates as a cost-sensitive router. At each decision epoch t ∈ {1, . . . , T }, after
observing the information Ht (recall that Ht collects the past interaction history together with
the current context xt and availability set Kt ), the router must select a single expert index
rt ∈ Kt to query for the current instance.
• Action Space. At time t, the admissible actions form the finite set Kt ⊆ U. The router
chooses an index
rt ∈ Kt .
• Consultation Cost. Each expert j ∈ U is associated with a deterministic, non-negative
consultation cost
βj ≥ 0,
modeling, for instance, API fees, latency constraints, or computational, energy, and
environmental costs. The vector (βj )j∈U is assumed known to the router.
Realized Cost. At time t, the total realized cost incurred when selecting expert rt ∈ Kt is the
sum of its prediction loss and its consultation cost:
Ct (rt ) :=

+

ℓrt ,t
|{z}

βr

t
|{z}

(1)

.

consultation cost

prediction loss

The quantity ℓrt ,t is a random variable at decision time t (its distribution is determined by the
underlying data-generating process and the chosen expert), whereas βrt is deterministic and
known.
Objective Function. Let Ht denote the state space in which Ht takes values. A (deterministic,
possibly non-stationary) routing policy is a sequence of measurable mappings
π = (πt )Tt=1 ,

πt : Ht → U,

such that πt (Ht ) ∈ Kt almost surely. Given a policy π, the chosen expert at time t is
rt := πt (Ht ),
so rt is Ft -measurable with values in Kt .
The goal is to find a causal policy π that minimizes the expected cumulative cost over the
horizon {1, . . . , T }:
inf EP
π

" T
X
t=1

#

Ct πt (Ht )



= inf EP
π

" T
X

#

ℓπt (Ht ),t + βπt (Ht )



,

(2)

t=1

where the expectation is taken with respect to the joint law of the stochastic process (xt , yt )Tt=1
and the induced loss process (ℓj,t )j,t . Any policy π ∗ attaining the infimum in (2) (if such a policy
exists) is called optimal.
3

Intuition (The “Economy of Diagnosis”).

Revisiting the medical triage example:

• Expert A (Nurse Algorithm): modest accuracy on complex cases, but very cheap (e.g.,
βA ≈ 0);
• Expert B (Specialist): high accuracy, but expensive or slow (e.g., βB ≫ 0).
A purely accuracy-driven scheme (ignoring the consultation costs βj ) might always select Expert B
whenever it yields a smaller expected loss. The cost-sensitive objective (2) instead forces the
router to trade off accuracy against resource usage, effectively asking: Is the expected reduction
in prediction loss from consulting the specialist worth the additional cost βB ? For a patient
whose context xt indicates a simple, low-risk case, the router may select the cheaper Nurse
Algorithm; for an ambiguous or high-risk case, it is preferable to pay the higher cost and defer
to the specialist.
Remark 1 (Bayes action under squared loss). Assume that at time t the router knows the
conditional distribution of yt given Ht , and that the instantaneous cost for choosing expert
j ∈ Kt is
2
Ct (j) := fj (xt ) − yt + βj .
Let mt := E[yt | Ht ] denote the conditional mean. Then any Bayes action at time t is given by
jt⋆ ∈ arg min

j∈Kt

n

fj (xt ) − mt

2

o

+ βj ,

i.e., the expert whose prediction is closest (in squared distance) to mt , up to the consultation
cost βj .

2

Generative Model: Switching Linear Dynamical System

In order to construct a cost-sensitive routing policy, we need, at each decision time t, the
conditional distribution of the loss of each available expert,


P ℓj,t | Ht ,

j ∈ Kt .

A static regression model of the form ℓj,t = gj (xt ) + εj,t would be inadequate here, because (i)
expert performance is typically non-stationary (concept drift over time), and (ii) we only observe
ℓj,t for the selected expert j = rt (bandit-style partial feedback), which breaks standard i.i.d.
assumptions.
We therefore posit that the losses are generated by a latent Switching Linear Dynamical
System (SLDS), in which a discrete regime process controls the parameters of a continuous latent
state describing expert reliability.

2.1

Latent State Dynamics

The core generative assumption is that the observed losses are emissions from a latent process
that exhibits both discrete structural changes and continuous evolution. We model this via a
Switching Linear Dynamical System specified by the tuple
M
Z, (Ak )M
k=1 , (Qk )k=1 , Π .



4

1. Regime Process (Discrete).

Let
zt ∈ Z := {1, . . . , M }

be a discrete latent variable representing the global environmental regime at time t (for instance,
“normal operations” versus “outbreak” in the medical example). The regime evolves according
to a time-homogeneous Markov chain with transition matrix Π ∈ [0, 1]M ×M and some initial
distribution µ0 on Z:
M
X

P(zt = k | zt−1 = i) = Πik ,

Πik = 1,

i = 1, . . . , M,

(3)

k=1

and

P(z1 = k) = µ0k ,

k = 1, . . . , M.

The regime zt will govern the parameters of the continuous dynamics, allowing the system to
switch, for example, between low-variance and high-variance modes.
2. Reliability Process (Continuous). For each expert j ∈ U, we introduce a continuous
latent state
αj,t ∈ Rdα ,
which is intended to capture the instantaneous reliability of expert j at time t (e.g., bias and
context-dependent error coefficients; the precise observation model will be specified in the next
subsection). The state is assumed to evolve even when the expert is not selected or not currently
available, reflecting the idea that an expert’s performance can drift over time irrespective of
whether we query it.
Conditional on the current regime zt = k, we posit linear-Gaussian dynamics with shared
regime-specific dynamics across experts:
αj,t = Ak αj,t−1 + wj,t ,



wj,t ∼ N 0, Qk ,

(4)

for all t ≥ 2 and all j ∈ U, where
• Ak ∈ Rdα ×dα is the state transition matrix in regime k (e.g., encoding mean-reversion or
persistence);
α
• Qk ∈ Sd++
is the corresponding process noise covariance, governing the rate at which the
state diffuses over time in regime k.

One may additionally specify a prior distribution for the initial states, e.g. αj,1 ∼ N (m0,j , P0,j ),
independent across j and independent of (zt )t≥1 .
For clarity, we stress that, conditional on the regime sequence (zt )t≥1 , the processes (αj,t )t≥1
are assumed independent across experts j and each follows the linear-Gaussian dynamics (4).
Intuition: Drift versus Shift. This hybrid (discrete/continuous) structure separates two
qualitatively different forms of non-stationarity:
• Gradual Drift (via αj,t ). The continuous state captures slow, continuous changes in
expert reliability, such as a sensor gradually losing calibration or a clinician’s performance
changing over a shift. In a single-regime linear-Gaussian model, this is handled by the
process noise covariance Qk , as in a standard Kalman filter.
• Abrupt Shifts (via zt ). The discrete regime process accounts for sudden, discontinuous
changes, such as the emergence of a new virus strain, a sudden change in patient mix, or
a hardware failure. These events are modeled by switches in the underlying parameters
(Ak , Qk ) and in the prior over αj,t .
5

A standard (single-regime) Kalman filter tends to be sluggish in the presence of abrupt shifts: it
interprets a large shock as an unusually large noise realization and may require many observations
to adapt its state estimate. By contrast, an SLDS can assign high posterior probability to a
“volatile” regime k when confronted with an apparent shock, thereby switching to dynamics
with, say, larger Qk and different Ak . This allows rapid adaptation to new conditions while still
retaining information accumulated during previous, more stable regimes.

2.2

Dynamic Expert Availability

Our model must accommodate changes in the available expert set Kt over time. Recall that
the latent state process (αj,t )t≥1 is defined for every expert j ∈ U, independently of whether j
is currently available. Dynamic availability therefore affects only the observation updates (i.e.,
when we do or do not receive a loss for expert j), not the underlying state dynamics (4).
At each time t, for each regime k ∈ Z and each expert j ∈ U, the SLDS filter maintains a
Gaussian approximation
(k)
(k) 
αj,t {zt = k, Ht } ≈ N mj,t , Pj,t ,
(k)

(k)

α
where mj,t ∈ Rdα and Pj,t ∈ Sd++
are the regime-conditional mean and covariance. The effect
of dynamic availability can then be described as follows.

• Expert Removal (Downtime). Fix t ≥ 2 and an expert j ∈ U. Suppose j is available
at time t − 1 but not at time t, i.e.
and

j ∈ Kt−1

j∈
/ Kt .

The latent state αj,t is still defined and evolves according to the regime-dependent dynamics
(4). In the Kalman/IMM recursion, this means that for expert j at time t we perform only
the prediction (time-update) step
(k)

(k)

(k)

(k)

(mj,t−1 , Pj,t−1 ) 7−→ (mj,t|t−1 , Pj,t|t−1 ),
for each k ∈ Z, and we skip the measurement (correction) step, since no loss ℓj,t is observed
when j ∈
/ Kt . Thus the belief on αj,t is updated purely by passive drift. The expert
remains in the global universe U and can be seamlessly re-integrated when it reappears in
Kt′ at some later time t′ > t.
• Expert Addition (New Expert Joins). Now suppose j ∈ U satisfies
and

j∈
/ Kt−1

j ∈ Kt ,

i.e., t is the first time at which expert j becomes available (a new model is deployed, or
a previously absent server comes online). Since no past loss information is available for
j, its state must be initialized from a population prior, common across experts with no
individual history.
Formally, for each regime k ∈ Z we set
αj,t {zt = k} ∼ N µpop , Σpop ,


(5)

α
independently of Ht and of the other experts, where µpop ∈ Rdα and Σpop ∈ Sd++
are fixed
hyperparameters. Equivalently, in terms of the filter parameters, we set

(k)

mj,t = µpop ,

(k)

Pj,t = Σpop ,

k ∈ Z.

Mathematical role of Σpop . The choice of Σpop encodes our epistemic uncertainty about
the performance of a newly-arrived expert:
6

(a) High Epistemic Uncertainty. Taking Σpop to be a large (typically diagonal) positive
definite matrix corresponds to a highly diffuse prior on αj,t , representing substantial
prior uncertainty about the expert’s reliability.
(b) Influence on the Routing Policy. Under any routing policy that makes use of the
predictive variance of the loss (for example, UCB/LCB-type rules or mean–variance
2 := Var(ℓ
2
bj,t
bj,t
scores based on σ
j,t | Ht )), a large Σpop induces a large initial value of σ
for the new expert. This, in turn, has the following qualitative effects:
2 tends to
bj,t
– For a risk-averse, variance-penalizing policy, high initial uncertainty σ
discourage the selection of j until informative feedback has been gathered.
– For an exploration-oriented policy (e.g., one that is optimistic in the face of
uncertainty, as in UCB, or that uses Thompson sampling), a large Σpop increases
the probability that j will be selected early, thereby accelerating the reduction of
uncertainty about its performance.

(c) Regime Alignment. Initializing the expert from the same population prior in each
regime k ensures that the SLDS can consistently track the expert’s performance as
the environment switches between regimes. If desired, one may also specify regime(k)
(k)
dependent population priors (µpop , Σpop ) to reflect prior knowledge that, say, certain
experts are expected to perform differently across regimes.

2.3

Observation Model (Partial Feedback)

To relate the latent reliability states (αj,t )j,t to the observed losses, we introduce a fixed feature
map
ϕ : X → Rdα ,
ϕt := ϕ(xt ).
Conditionally on the regime zt = k and the state αj,t , we model the loss of expert j at time t as
a linear-Gaussian emission:
ℓj,t = ϕ⊤
t αj,t + vj,t ,

vj,t ∼ N 0, Rk,j ,


(6)

where Rk,j > 0 denotes the observation noise variance for expert j in regime k. In other words,
1×dα to the latent
for fixed (t, k, j) the emission is obtained by applying the row vector ϕ⊤
t ∈R
state αj,t and adding Gaussian noise.
Partial Feedback and Censoring. At decision time t, the router chooses a single expert
index rt ∈ Kt based on the information Ht . Subsequently, the system observes the corresponding
loss
ℓrt ,t ,
while the losses ℓj,t for j =
̸ rt remain unobserved. Thus, although the generative model (6)
specifies a (latent) loss for every expert j ∈ U at each time t, the data available at time t consist
only of the single scalar observation ℓrt ,t .
From the filtering viewpoint, for each regime k ∈ Z and expert j ∈ U we maintain Gaussian
approximations of the form
(k)

(k) 

αj,t {zt = k, Ht } ≈ N mj,t|t , Pj,t|t ,
(k)

(k)

with (mj,t|t , Pj,t|t ) obtained recursively from the dynamics (4). Given the prediction (time-update)
step
(k)
(k)
(k)
(k)
(mj,t−1|t−1 , Pj,t−1|t−1 ) 7−→ (mj,t|t−1 , Pj,t|t−1 )

7

induced by (4), the observation model (6) yields the standard linear-Gaussian measurement
update for the selected expert j = rt :
(k)

(k)

(k)

(k)

(mrt ,t|t−1 , Prt ,t|t−1 ) 7−→ (mrt ,t|t , Prt ,t|t )
using the scalar observation ℓrt ,t . For all unselected experts j ̸= rt , no measurement is available
at time t, so the posterior equals the prior,
(k)

(k)

(k)

mj,t|t = mj,t|t−1 ,

(k)

Pj,t|t = Pj,t|t−1 .
(k)

Consequently, for experts that are rarely queried, the uncertainty encoded in Pj,t|t increases over
time according to the process noise covariance Qk in the state dynamics (4), reflecting growing
epistemic uncertainty about their current reliability.

3

Inference: Interacting Multiple Model (IMM) Filter

Exact Bayesian filtering in a switching linear dynamical system is intractable in general, since
the number of regime histories (z1 , . . . , zt ) grows exponentially in t (on the order of M t for M
regimes). We therefore resort to the Interacting Multiple Model (IMM) approximation, which
maintains, at each time t, a finite set of M Gaussian filters, one per regime k ∈ Z, and performs
a moment-matching “interaction” between them at every step.
For filtering it is convenient to condition on the full interaction history up to time t, including
past decisions and observed losses:
It := (xτ , Kτ , Fτ ) 1≤τ ≤t ,


3.1

Fτ = (rτ , ℓrτ ,τ ).

Belief State Representation

At the end of time t (after having observed ℓrt ,t ), the IMM filter represents the joint posterior
over (zt , αj,t ) in the following approximate form:
1. Regime Probabilities. The posterior probability of being in regime k:
bt (k) := P(zt = k | It ),

k ∈ Z.

2. Regime-Conditional Expert States. For each expert j ∈ U and each regime k ∈ Z, a
Gaussian approximation to the conditional distribution of αj,t :
(k)

(k) 

αj,t {zt = k, It } ≈ N mj,t|t , Pj,t|t ,
(k)

(k)

α
where mj,t|t ∈ Rdα and Pj,t|t ∈ Sd++
denote, respectively, the regime-conditional mean and
covariance at time t.

Thus the overall posterior at time t is approximated as a finite mixture
P(αj,t , zt | It ) ≈

M
X

(k)

(k) 

bt (k) N αj,t ; mj,t|t , Pj,t|t ⊗ δk (zt ),

k=1

for each j ∈ U.

3.2

Step 1: Interaction (Mixing of Regime-Conditional Estimates)

The IMM “interaction” step constructs, for each candidate regime k at time t + 1, a mixed
Gaussian initial condition by combining the regime-conditional estimates at time t using suitable
mixing weights.
8

For a fixed k ∈ Z, define the conditional mixing probability

Mixing Weights.

µi|k := P(zt = i | zt+1 = k, It ),

i ∈ Z.

Proposition 1 (Mixing weights). For each k ∈ Z and i ∈ Z,
µi|k =

Πik bt (i)
,
c̄k

c̄k :=

M
X

Πlk bt (l),

(7)

l=1

where Π is the regime transition matrix and c̄k = P(zt+1 = k | It ) is the predicted probability of
regime k at time t + 1.
Proof. By Bayes’ rule and the Markov property of (zt )t≥1 ,
P(zt+1 = k | zt = i, It ) P(zt = i | It )
P(zt+1 = k | It )
Πik bt (i)
= PM
l=1 P(zt+1 = k | zt = l) P(zt = l | It )
Πik bt (i)
,
= PM
l=1 Πlk bt (l)

µi|k =

which is precisely (7).
Moment Matching. Fix an expert j ∈ U and a candidate regime k ∈ Z at time t + 1. The
exact conditional distribution of αj,t given {zt+1 = k, It } is a mixture of the M Gaussians
indexed by i ∈ Z, with weights µi|k . The IMM approximation replaces this mixture by a single
Gaussian whose first two moments match those of the mixture.
1. Mixed mean. Define the mixed mean for expert j and next regime k by
0,(k)
mj,t|t

M
X

:=

(i)

(8)

µi|k mj,t|t .

i=1

2. Mixed covariance. The corresponding mixed covariance incorporates both the withinregime variances and the dispersion of the regime-specific means:
0,(k)

Pj,t|t

:=

M
X

h

(i)

(i)

0,(k) 

µi|k Pj,t|t + mj,t|t − mj,t|t

(i)

0,(k) ⊤

mj,t|t − mj,t|t

i

.

(9)

i=1
0,(k)

0,(k)

By construction, mj,t|t and Pj,t|t are, respectively, the first and second conditional moments
of the exact mixture

(i)
(i)
i µi|k N (·; mj,t|t , Pj,t|t ).

P

(i)

Intuition. If the regime-specific means mj,t|t are widely spread, the “spread-of-means” term in
0,(k)

(9) is large, so the mixed covariance Pj,t|t reflects substantial uncertainty due to ambiguity
about the current regime.

3.3

Step 2: Time Update (Prediction)
0,(k)

0,(k)

Given the mixed initial condition (mj,t|t , Pj,t|t ) for each k ∈ Z, the regime-matched Kalman
filter performs the linear-Gaussian prediction from t to t + 1 using the dynamics (4). For each
expert j ∈ U and regime k ∈ Z,
(k)

0,(k)

(10)

(k)

0,(k)

(11)

mj,t+1|t = Ak mj,t|t ,
Pj,t+1|t = Ak Pj,t|t A⊤
k + Qk .

This prediction step is performed for all experts j ∈ U, independently of whether expert j was
queried at time t or is available at time t + 1.
9

3.4

Step 3: Measurement Update (Correction under Partial Feedback)

At time t + 1, the router selects a single expert index rt+1 ∈ Kt+1 and subsequently observes the
corresponding loss ℓrt+1 ,t+1 . The observation model (6) gives
ℓrt+1 ,t+1 = ϕ⊤
t+1 αrt+1 ,t+1 + vrt+1 ,t+1 ,

vrt+1 ,t+1 ∼ N 0, Rk,rt+1 ,


conditional on zt+1 = k.
For each regime k ∈ Z we define the predicted observation mean and innovation variance:
(k)

(k)

(k)

(k)

ℓ̂t+1 := ϕ⊤
t+1 mrt+1 ,t+1|t ,
St+1 := ϕ⊤
t+1 Prt+1 ,t+1|t ϕt+1 + Rk,rt+1 .
The innovation (residual) is
(k)

(k)

et+1 := ℓrt+1 ,t+1 − ℓ̂t+1 .
The scalar-output Kalman update for expert rt+1 and regime k is then
Kalman gain:
Updated mean:
Updated covariance:

(k)

(k) −1

(k)

Kt+1 = Prt+1 ,t+1|t ϕt+1 St+1

(k)

(k)

,

(k) (k)

(12)

mrt+1 ,t+1|t+1 = mrt+1 ,t+1|t + Kt+1 et+1 ,

(13)

 (k)
(k)
(k)
Prt+1 ,t+1|t+1 = I − Kt+1 ϕ⊤
t+1 Prt+1 ,t+1|t .

(14)

For all unselected experts j ̸= rt+1 , no observation is available at time t + 1, so the measurement
update is skipped:
(k)
(k)
(k)
(k)
Pj,t+1|t+1 = Pj,t+1|t .
mj,t+1|t+1 = mj,t+1|t ,

3.5

Step 4: Regime Probability Update

Finally, we update the regime probabilities using the likelihood of the observed loss under each
regime hypothesis. For k ∈ Z, define the (scalar) Gaussian likelihood
(k)

(k)

(k) 

Λt+1 := N ℓrt+1 ,t+1 ; ℓ̂t+1 , St+1 ,

(15)

(k)

(k)

i.e., the density of a normal distribution with mean ℓ̂t+1 and variance St+1 evaluated at ℓrt+1 ,t+1 .
We have already defined the predicted regime probabilities
c̄k = P(zt+1 = k | It ) =

M
X

Πlk bt (l).

l=1

Bayes’ rule then yields the updated regime probabilities at time t + 1:
(k)

bt+1 (k) := P(zt+1 = k | It+1 ) = P

Λt+1 c̄k

(l)
M
l=1 Λt+1 c̄l

,

k ∈ Z,

(16)

where It+1 is the σ-field generated by the extended history It+1 .
(k)

Intuition. If the observed loss ℓrt+1 ,t+1 is highly improbable under the regime-k filter (i.e., Λt+1
is small), then bt+1 (k) is decreased relative to its prediction c̄k . Conversely, regimes under which
(k)
the observation is well explained (large Λt+1 ) gain posterior probability.

10

4

Selection Policy

At the end of time t, after running the IMM filter and updating the belief state with the observed
loss ℓrt ,t , the router must choose an expert rt+1 ∈ Kt+1 for the next instance at time t + 1. Recall
that the one-step total cost for expert j is
Cj,t+1 := ℓj,t+1 + βj .
A purely risk-neutral, myopic objective would be to choose rt+1 so as to minimize the conditional
expectation


E Crt+1 ,t+1 | It .
In practice, we will consider a one-step risk-sensitive criterion that also depends on the conditional
variance of Cj,t+1 given It , defined below.

4.1

MMSE Forecast (Predictive Mean)

Fix an expert j ∈ U. Conditioned on It , the latent regime zt+1 is distributed according to the
one-step-ahead regime probabilities
bt+1|t (k) := P(zt+1 = k | It ),

k ∈ Z,

which, in the IMM recursion, coincide with the predicted regime probabilities c̄k from Step 4 of
the filter.
For each regime k ∈ Z, the IMM prediction step provides
(k)

(k)



(k)

(k)



αj,t+1 {zt+1 = k, It } ≈ N mj,t+1|t , Pj,t+1|t ,
and the observation model (6) yields
ℓj,t+1 {zt+1 = k, It } ∼ N µj,t+1|t , Sj,t+1|t ,
with
(k)

(k)

(k)

(k)

µj,t+1|t := ϕ⊤
t+1 mj,t+1|t ,
Sj,t+1|t := ϕ⊤
t+1 Pj,t+1|t ϕt+1 + Rk,j .
The (approximate) predictive distribution of ℓj,t+1 given It is therefore a finite mixture of
Gaussians:
P(ℓj,t+1 | It ) ≈

M
X

(k)

(k)

bt+1|t (k) N ·; µj,t+1|t , Sj,t+1|t .


k=1

Under squared loss for predicting ℓj,t+1 , the optimal point predictor is the conditional
expectation (MMSE forecast)
b
R
j,t+1|t := E ℓj,t+1 | It =




M
X

(k)

bt+1|t (k) µj,t+1|t .

k=1

b
Interpretation. The quantity R
j,t+1|t is the regime-averaged predictive loss of expert j at time
(k)

t + 1: it combines the predicted reliability state (the regime-conditional means mj,t+1|t ) with
the probability bt+1|t (k) of each regime k being active at time t + 1.

11

4.2

Predictive Variance and Epistemic Uncertainty

We now quantify the overall uncertainty associated with expert j’s predicted loss via the
conditional variance

2
:= Var ℓj,t+1 | It .
σ̂j,t+1|t
This variance aggregates both aleatoric uncertainty (observation noise and within-regime state
uncertainty) and epistemic uncertainty due to ambiguity over the regime. Since βj is deterministic,
we have
2
Var(Cj,t+1 | It ) = Var(ℓj,t+1 | It ) = σ̂j,t+1|t
.
Proposition 2 (Predictive variance decomposition). For each expert j ∈ U and time t,
2
σ̂j,t+1|t
=

M
X

(k)

bt+1|t (k) Sj,t+1|t +

k=1

M
X

2

(k)

b
bt+1|t (k) µj,t+1|t − R
j,t+1|t .

(17)

k=1

Proof. Apply the law of total variance with Y = ℓj,t+1 and Z = zt+1 :
Var(Y | It ) = E Var(Y | Z, It ) | It + Var E[Y | Z, It ] | It .






Conditionally on {zt+1 = k, It } we have
(k)

(k)

E[ℓj,t+1 | zt+1 = k, It ] = µj,t+1|t ,

Var(ℓj,t+1 | zt+1 = k, It ) = Sj,t+1|t ,

and P(zt+1 = k | It ) = bt+1|t (k). Therefore,
E Var(Y | Z, It ) | It =




M
X

(k)

bt+1|t (k) Sj,t+1|t ,

k=1

and
Var E[Y | Z, It ] | It =


M
X

2

(k)

b
bt+1|t (k) µj,t+1|t − R
j,t+1|t ,

k=1

b
since R
j,t+1|t =

(k)
k bt+1|t (k) µj,t+1|t . Summing the two terms yields (17).

P

Interpretation.
• The first term,
M
X

(k)

bt+1|t (k) Sj,t+1|t ,

k=1
(k)

averages the within-regime variances. Each Sj,t+1|t captures both the uncertainty in the
(k)

latent state (through Pj,t+1|t ) and the irreducible observation noise Rk,j .
• The second term,
M
X

(k)

2

b
bt+1|t (k) µj,t+1|t − R
j,t+1|t ,

k=1

measures the dispersion of the regime-specific predictive means around the overall mean.
It quantifies epistemic uncertainty due to regime ambiguity: if different regimes predict
very different losses for expert j, this term is large.

12

4.3

Cost-Sensitive Selection Rule

Using the predictive mean and variance, we define a myopic, risk-adjusted selection rule for the
next decision. For each available expert j ∈ Kt+1 , consider the score
q

2
b
Jj,t+1 := R
j,t+1|t + βj + λ σ̂j,t+1|t ,

where λ ∈ R is a user-specified risk parameter. The router then selects
∗
rt+1
∈ arg min Jj,t+1 .
j∈Kt+1

(18)

Operational interpretation.
b
(i) R
j,t+1|t (expected loss) captures pure exploitation: lower values correspond to experts
that are predicted to incur smaller errors under the current belief.

(ii) βj (consultation cost) accounts for economic or operational constraints (e.g., latency,
API fees, computational cost). An expert with a large βj must offer a correspondingly
lower expected loss to be competitive.
q

2
(risk/uncertainty term) controls how the policy reacts to predictive uncer(iii) λ σ̂j,t+1|t
tainty:

• If λ > 0 (risk-averse regime), the policy penalizes experts with high predictive variance,
favouring stable, well-understood experts.
• If λ < 0 (exploratory or “optimistic” regime), the policy is encouraged to select
experts with high uncertainty, in line with exploration principles: querying such
2
experts yields information that reduces σ̂j,t+1|t
in subsequent steps.
The rule (18) is myopic: it minimizes a one-step, risk-adjusted proxy for the cumulative
objective (2). It is not, in general, globally Bayes-optimal over the horizon {1, . . . , T }, but it is
computationally tractable and directly exploits the IMM-based predictive mean and variance.
Remark 2 (One-step risk-adjusted Bayes action). Fix t and condition on the history It . For
each expert j ∈ Kt+1 ,
Cj,t+1 = ℓj,t+1 + βj
denotes the total next-step cost, with
b
E[Cj,t+1 | It ] = R
j,t+1|t + βj ,

2
Var(Cj,t+1 | It ) = σ̂j,t+1|t
.

Define the local risk functional
q

ρt (Y ) := E[Y | It ] + λ Var(Y | It ),
for some parameter λ ∈ R. Then the selection rule (18) is exactly
∗
rt+1
∈ arg min ρt Cj,t+1 ,



j∈Kt+1

i.e., a Bayes action for the one-step risk functional ρt .

13

5

Parameter Optimization

The parameters of the SLDS model are collected in
Θ :=





Π, (Ak , Qk )M
k=1 , (Rk,j )k=1,...,M ; j∈U , initial priors ,

where:
• Π is the M × M regime transition matrix;
α
are the regime-dependent state transition and process noise
• Ak ∈ Rdα ×dα and Qk ∈ Sd++
matrices;

• Rk,j > 0 is the observation noise variance for expert j in regime k;
• the initial priors comprise the initial regime distribution and the Gaussian priors on the
latent states (e.g. the population prior (µpop , Σpop )).
The feature map ϕ : X → Rdα is treated as fixed and not learned.
We observe a single scalar loss per time step, corresponding to the selected expert. The data
are
D := {(xt , rt , ℓrt ,t , Kt )}Tt=1 ,
and, for fixed Θ, the latent variables are the regime sequence (zt )Tt=1 and the latent reliability
states (αj,t )j∈U , 1≤t≤T .

5.1

Maximum Likelihood Objective

Given that the routing decisions (rt ) and availability sets (Kt ) are treated as exogenous, we
seek a maximum likelihood estimate of Θ based on the conditional marginal likelihood of the
observed losses given the contexts and actions. Writing
It = (xτ , Kτ , Fτ ) 1≤τ ≤t ,


Fτ = (rτ , ℓrτ ,τ ),

the incremental likelihood at time t is
pΘ (ℓrt ,t | It−1 ) = PΘ (ℓrt ,t | It−1 ),
and the (conditional) log-likelihood over the horizon {1, . . . , T } is
L(Θ) :=

T
X

log pΘ (ℓrt ,t | It−1 ).

(19)

t=1

Maximizing (19) over Θ yields a maximum likelihood estimate conditional on the observed
contexts, actions, and availability patterns.
Likelihood term. For each t, the quantity pΘ (ℓrt ,t | It−1 ) can be expressed by marginalizing
over the latent regime zt :
pΘ (ℓrt ,t | It−1 ) =

M
X
k=1

pΘ (ℓrt ,t | zt = k, It−1 ) PΘ (zt = k | It−1 ) .
|

{z

(k)

Λt

}|

{z

c̄t (k)

(20)

}

Here:
• c̄t (k) := PΘ (zt = k | It−1 ) = M
i=1 Πik bt−1 (i) is the predicted probability of regime k at
time t, as in the IMM recursion;
P

14

(k)

• Λt is the scalar Gaussian likelihood of the observation ℓrt ,t under the regime-k Kalman
filter at time t:

(k)
(k)
(k)
Λt = N ℓrt ,t ; µrt ,t|t−1 , Srt ,t|t−1 ,
(k)

(k)

where µrt ,t|t−1 and Srt ,t|t−1 are the predicted mean and variance for expert rt in regime k
at time t (as in the IMM filter).

5.2

EM Algorithm for SLDS

Direct maximization of (19) is intractable due to the latent regime sequence and continuous
states. We therefore employ an Expectation–Maximization (EM) procedure, using the IMM
filter and an associated smoother to approximate the posterior over (zt , αj,t )j,t .
Let Z1:T denote the regime sequence and α1:T the collection of all latent states. The EM
algorithm iteratively maximizes the expectation of the complete-data log-likelihood
log pΘ D, Z1:T , α1:T



with respect to a posterior approximation q(Z1:T , α1:T ) induced by the current parameter iterate.
Because the IMM posterior is itself an approximation, the resulting scheme is a generalized EM
algorithm (it monotonically increases an approximate likelihood surrogate).

15

Algorithm 1 Cost-Sensitive SLDS Training (Generalized EM with IMM)
1: Input: data D = {(xt , ℓrt ,t , rt , Kt )}T
t=1 .

2: Initialize parameters Θ(0) (e.g., Π nearly uniform, Ak close to identity, Qk small, Rk,j

moderate, and reasonable initial priors).
3: repeat
4:
E-step (IMM smoothing). Given Θ(m) , run the IMM forward filter and a backward
smoother to obtain approximate posterior quantities:
• regime marginals:
(k)

γt

≈ PΘ(m) (zt = k | IT ),

1 ≤ t ≤ T, k ∈ Z;

• pairwise regime marginals (for transitions):
(i,k)

ξt

≈ PΘ(m) (zt−1 = i, zt = k | IT ),

2 ≤ t ≤ T, i, k ∈ Z;

• regime-conditional smoothed state statistics for each expert j:
(k)

(k) 

αj,t | {zt = k, IT } ≈ N mj,t|T , Pj,t|T ,
(k)

(k)

(k)⊤

together with cross-covariances Pj,t,t−1|T ≈ E[αj,t α⊤
j,t−1 | zt = k, IT ] − mj,t|T mj,t−1|T when
needed.
5:

M-step (parameter update). Update Θ by maximizing the expected complete-data
log-likelihood under the approximate posterior:
Θ(m+1) ∈ arg max Eq(m) log pΘ (D, Z1:T , α1:T ) ,




Θ

where q (m) denotes the IMM-based posterior approximation from the E-step. This yields
closed-form updates of the following form:
• Regime transitions Π. For each i, k ∈ Z,
PT
(i,k)
(m+1)
t=2 ξt
.
Πik
=P P
(i,k′ )
T
M
t=2
k′ =1 ξt

• State dynamics (Ak , Qk ). For each regime k ∈ Z, estimate Ak and Qk by (approximately)
solving a weighted least-squares problem over the latent transitions αj,t−1 7→ αj,t , pooling
(k)
all experts j ∈ U and times t with weights γt :
(m+1)

Ak

≈ arg min
A

T
XX
(k) 

γt

E ∥αj,t − Aαj,t−1 ∥22 zt = k, IT ,


j∈U t=2

(m+1)

and Qk
as the corresponding weighted empirical covariance of the residuals αj,t −
(m+1)
Ak
αj,t−1 .
• Observation noise Rk,j . For each (k, j), update Rk,j using the observed losses ℓrt ,t
(k)
whenever expert j was selected (rt = j), weighted by the regime posteriors γt :
(m+1)
Rk,j
=

2
(k) 
⊤
t:rt =j γt E ℓj,t − ϕt αj,t
P
(k)
t:rt =j γt

P

zt = k, IT



,

where, under the Gaussian approximation,
(k)

(k)

(k)

(k)⊤ 

2
2
⊤
⊤
E (ℓj,t − ϕ⊤
t αj,t ) | zt = k, IT = (ℓrt ,t ) − 2ℓrt ,t ϕt mj,t|T + ϕt Pj,t|T + mj,t|T mj,t|T ϕt .
16





(For t with rt ̸= j, no contribution appears because ℓj,t is unobserved.)
6: until convergence of L(Θ) or of the parameter iterates.

Intuition.
• E-step (attribution). The E-step approximates, for each time t, how much of the
observed loss ℓrt ,t and the latent state evolution should be attributed to each regime k.
(k)
(i,k)
This produces soft counts for regime occupancy (γt ) and transitions (ξt ), as well as
smoothed trajectories of the latent states under each regime.
• M-step (recalibration). The M-step then recalibrates the parameters (Π, Ak , Qk , Rk,j )
so that, under the current posterior attribution, the linear dynamics and noise statistics
(k)
best explain the observed sequence of losses. For example, periods where γt is large exert
more influence on (Ak , Qk ), so a “volatile” regime will learn larger process noise Qk if the
data exhibit rapid changes during those times.

6

Extension: Joint Subset Selection (Top-K)

In some applications, the router may wish to consult not just a single expert, but a subset
|St | = K,

St ⊂ Kt ,

for the current instance at time t (for instance, to form an ensemble prediction). A naive strategy
would select the K experts with smallest individual scores (e.g. lowest predicted loss plus cost),
treating them as independent. This ignores correlation: if two experts have highly correlated
reliability states, querying both may incur redundant cost without commensurate information
gain.
We describe an extension of the SLDS framework that models such correlations and a greedy
Top-K selection rule that penalizes redundant queries.

6.1

Joint state-space formulation

To capture correlated dynamics, we collect the latent reliability states of all experts into a single
vector. Let dα denote the dimension of the per-expert state, and define
h

αt := α⊤
1,t · · ·

α⊤
N,t

i⊤

∈ RN dα .

For each regime k ∈ Z, we define regime-dependent joint dynamics
(joint)

αt+1 = Ak

(k)

αt + w t ,

(k)

wt

(joint)

∼ N (0, Qk

),

(21)

where:
(joint)

• Ak
∈ RN dα ×N dα is a joint transition matrix. A natural choice consistent with the
single-expert model is
(joint)
Ak
= IN ⊗ Ak ,
i.e. the same Ak acts independently on each expert’s state, but more general couplings are
allowed.
(joint)

dα
• Qk
∈ SN
++ is a full joint process noise covariance. Its block structure encodes cross(joint)
expert correlations: the (i, j) block (Qk
)ij ∈ Rdα ×dα models how shocks affecting
expert i co-vary with shocks affecting expert j under regime k. The independent case
(joint)
corresponds to Qk
block-diagonal.

The regime process (zt ) and its transition matrix Π remain unchanged. In the full SLDS,
we maintain regime-conditional joint Gaussians for αt and combine them via IMM as in the
single-expert case.
17

6.2

Subset observation model

At a fixed time t, the (hypothetical) loss of each expert j ∈ U is
ℓj,t = ϕ⊤
t αj,t + vj,t ,

vj,t ∼ N (0, Rk,j )

conditional on regime zt = k, where ϕt = ϕ(xt ) is the feature vector. In joint form, define the
full loss vector
h
i⊤
ℓt := ℓ1,t · · · ℓN,t ∈ RN .
Then, conditional on zt = k and It−1 ,
ℓt = Ht αt + vt ,

vt ∼ N (0, Rk ),

(22)

where
• Ht ∈ RN ×N dα is block-diagonal with row j equal to
0, . . . , 0, ϕ⊤
t , 0, . . . , 0 ,





i.e. ϕ⊤
t in the block corresponding to expert j and zeros elsewhere;
• Rk = diag(Rk,1 , . . . , Rk,N ) is the diagonal observation noise covariance.
If, at time t, we select a subset St ⊂ Kt of size K, we observe only the corresponding entries
of ℓt . Let PSt ∈ {0, 1}K×N denote the row selection matrix that picks coordinates in St , and
define
ySt ,t := PSt ℓt ∈ RK .
Then

ySt ,t = HSt ,t αt + vSt ,t ,

where HSt ,t := PSt Ht and vSt ,t ∼ N (0, RSt ,k ) with RSt ,k := PSt Rk PS⊤t .

6.3

Greedy sequential Top-K selection

At time t, before observing any losses, the IMM filter (with moment-matching over regimes)
yields an approximate Gaussian predictive distribution for the full loss vector
c
ℓt | It−1 ≈ N R
t|t−1 , Σt|t−1 ,


where:
N
c
• R
t|t−1 ∈ R collects the one-step-ahead MMSE forecasts

c
b
R
t|t−1 j = Rj,t|t−1 = E[ℓj,t | It−1 ],


obtained as in the single-expert case;
• Σt|t−1 ∈ SN
++ is the predictive covariance matrix
Σt|t−1 = Cov(ℓt | It−1 ),
whose diagonal entries are the predictive variances of each expert’s loss and whose offdiagonal entries encode cross-expert correlations.
Selecting the globally optimal
subset St of size K with respect to a risk-adjusted objective
N
would require evaluating all K
subsets, which is combinatorially infeasible for large N . We
therefore adopt a forward greedy strategy that, at each step, selects the expert that yields the best
marginal improvement when accounting for conditional variances under Gaussian conditioning.
(0)
(0)
Let St := ∅, and let Σt := Σt|t−1 . For k = 1, . . . , K, perform:
18

(k−1)

1. Scoring step. For each candidate j ∈ Kt \ St
(k−1)
Scoret (j | St
)

where

(k−1)

2
σj,t
(St

, define the risk-adjusted score
r

(k−1)
2
b
:= R
),
j,t|t−1 + βj + λ σj,t (St

(k−1)

) := Var ℓj,t | It−1 , {ℓi,t : i ∈ St

}

(23)



is the conditional predictive variance of expert j’s loss given hypothetical observation of
(k−1)
the losses of the experts already in St
. Under the Gaussian approximation,
(k−1)

2
σj,t
(St

(k−1) 

) = Σt

jj

,

i.e. the (j, j) entry of the current conditional covariance matrix.
2. Selection step. Choose
j ∗ ∈ arg
and update the selected set

(k−1)

min

(k−1)
j∈Kt \St

(k)

St

Scoret (j | St

),

:= St(k−1) ∪ {j ∗ }.

3. Conditional covariance update (diversity mechanism). We now update the conditional covariance of the remaining experts’ losses given the hypothetical observation of ℓj ∗ ,t .
(k−1)
c
Since ℓt | It−1 ∼ N (R
), and we condition on observing ℓj ∗ ,t , the conditional
t|t−1 , Σt
covariance of ℓt (before removing the j ∗ -coordinate) is given by the standard Gaussian
conditioning formula
(k−1)
(k−1)
Σt
ej ∗ e⊤
(k)
(k−1)
j ∗ Σt
Σt = Σt
−
,
(24)
(k−1) 
Σt
j∗j∗
where ej ∗ ∈ RN is the j ∗ -th canonical basis vector. Only the submatrix corresponding to
(k)
the remaining indices Kt \ St is relevant for subsequent iterations, and can be extracted
(k)
from Σt .
(K)

At the end of the K iterations, we output St

as the selected subset for time t.

Theoretical insight. The update (24) is the conditional covariance of a jointly Gaussian
vector under observation of one component; it is equivalent to a Kalman update performed
directly in the observation (loss) space. If the loss of a remaining candidate i is highly correlated
(k−1) 
with that of j ∗ , then the off-diagonal covariance Σt
is large in magnitude, and the update
ij ∗
(k) 

will significantly reduce the conditional variance Σt

ii

of ℓi,t , even though i was not selected.
(k)

2 (S
Under an exploratory choice of λ < 0 in the score (23), this reduction in σi,t
t ) makes
expert i less attractive in subsequent greedy steps compared to experts whose losses remain
weakly correlated with j ∗ and therefore retain higher conditional variance. The greedy procedure
thus tends to produce subsets that are diverse in the sense of carrying complementary information
rather than duplicating highly correlated experts.

19

7

Extension: Full-Information Expert Feedback

In the main development, the router observes at each time t only the loss of the selected expert rt ,
leading to a censored (bandit-style) feedback model. In some applications—for instance when all
experts are implemented as neural networks that can be evaluated offline—it is natural to assume
a full-information setting in which, once the outcome yt is revealed, the loss of every available
expert can be computed, regardless of which expert was actually queried for decision-making.
In this section we formalize this full-information feedback model and describe the corresponding changes to the SLDS–IMM inference and to the routing problem.

7.1

Feedback model with full expert losses

We retain the generative model of Sections 2–3: a latent regime process (zt )t≥1 , regime-dependent
linear dynamics for the expert-specific states (αj,t )t≥1, j∈U , and a linear-Gaussian emission model
for the loss. Recall that for each t ≥ 1 we write
ϕt := ϕ(xt ) ∈ Rdα ,
and, conditionally on zt = k,
ℓj,t = ϕ⊤
t αj,t + vj,t ,

vj,t ∼ N (0, Rk,j ),

for every expert j ∈ U.
The change concerns only the feedback observed after each decision. In the full-information
setting we assume that, once yt is revealed, the loss
ℓj,t = L fj (xt ), yt



is available for all experts j ∈ Kt , not just for the chosen expert rt .
Formally, define the feedback random element at time t by
Gt := rt , (ℓj,t )j∈Kt ∈ U × [0, +∞)|Kt | .


The interaction history up to time t is then
Itfull := (xτ , Kτ , Gτ ) 1≤τ ≤t ,


and the information available at decision time t + 1 is summarized by
full
:= Itfull , xt+1 , Kt+1 .
Ht+1



In this extension, the filtration (Ft )t≥0 is redefined as
Ft := σ Htfull ,


t ≥ 0,

so that, at each time t, Ft contains the full panel of losses


ℓj,τ j∈Kτ , 1≤τ ≤t
observed up to time t.
The latent process


zt , (αj,t )j∈U , xt


t≥1

is unchanged and is assumed to evolve independently of the routing decisions (rt )t≥1 (as in the
base model): the router affects only the incurred cost, not the environment.
20

7.2

IMM belief update under full feedback

Under the full-information model, the IMM filter can exploit at each time t the observed losses of
all experts j ∈ Kt rather than only that of the selected expert. For clarity, we keep the standing
assumptions:
• conditional on the regime sequence (zt ), the processes (αj,t )t≥1 are independent across
j ∈ U and follow the regime-dependent linear dynamics;
• conditional on (zt ) and (αj,t ), the observation noises (vj,t ) are independent across experts
and time.
Fix t ≥ 1. For each regime k ∈ Z and expert j ∈ U, the prediction step of the IMM filter
yields the approximate Gaussian law
(k)

(k)



(k)

(k)



full
αj,t {zt = k, It−1
} ≈ N mj,t|t−1 , Pj,t|t−1 ,

and the emission model gives
full
ℓj,t {zt = k, It−1
} ∼ N µj,t|t−1 , Sj,t|t−1 ,

with
(k)

(k)

(k)

(k)

µj,t|t−1 := ϕ⊤
t mj,t|t−1 ,
Sj,t|t−1 := ϕ⊤
t Pj,t|t−1 ϕt + Rk,j .
Measurement update for all experts. Given the observed loss ℓj,t , the regime-conditional
Kalman update for expert j in regime k is
(k)

(k)

(k)

(k)

(k)

(k)

Kj,t := Pj,t|t−1 ϕt Sj,t|t−1

−1

,
(k)

mj,t|t = mj,t|t−1 + Kj,t ℓj,t − µj,t|t−1 ,
(k)

(k)



 (k)

Pj,t|t = I − Kj,t ϕ⊤
t Pj,t|t−1 .
In the full-information setting, this update is performed for every expert j ∈ Kt (and may be
skipped for j ∈
/ Kt if such experts are considered dormant). Thus all experts benefit from each
newly revealed outcome yt , irrespective of whether they were selected for routing at time t.
Regime probability update.
time t − 1, and set

full ) denote the regime beliefs at
Let bt−1 (i) = P(zt−1 = i | It−1

full
c̄t (k) := P(zt = k | It−1
)=

M
X

Πik bt−1 (i),

k ∈ Z,

i=1

for the one-step regime predictions. Under the Gaussian approximations above and the conditional
independence across experts, the joint predictive density of the loss vector
full
(ℓj,t )j∈Kt {zt = k, It−1
}

factorizes as

Y

(k)

(k)

N ℓj,t ; µj,t|t−1 , Sj,t|t−1 .

j∈Kt

21



Define the regime-specific likelihood
(k)

Λt

:=

Y

(k)

(k)

N ℓj,t ; µj,t|t−1 , Sj,t|t−1 .


j∈Kt

Bayes’ rule then yields the updated regime beliefs
(k)
Λt c̄t (k)
full
bt (k) = P(zt = k | It ) = P
,
(ℓ)
M
ℓ=1 Λt c̄t (ℓ)

k ∈ Z.

Interpretation. Compared to the censored-feedback case, each time step now contributes
information about the relative plausibility of the regimes through the entire vector of expert
losses (ℓj,t )j∈Kt . Regimes under which the observed pattern of losses is unlikely (for many experts
simultaneously) receive smaller posterior weight, while regimes consistent with the observed
multi-expert performance are reinforced.

7.3

Consequences for the routing problem

Under full-information feedback, the IMM-based predictive mean and variance for each expert j
at time t + 1,
full
2
b
R
σ̂j,t+1|t
= Var(ℓj,t+1 | Itfull ),
j,t+1|t = E[ℓj,t+1 | It ],
are defined exactly as in Section 4, replacing It by Itfull . The IMM machinery and the variance
decomposition remain unchanged; only the underlying filter now exploits all past losses, not just
those of the selected experts, yielding more informative and less selection-biased estimates of
the latent dynamics.
Crucially, in this full-information regime the router’s decisions (rt )t≥1 no longer affect the
quality of future information: at each time t, the full panel of losses (ℓj,t )j∈Kt is observed
regardless of which expert was used for prediction. Combined with the assumption that the
latent process


zt , (αj,t )j∈U , xt
t≥1

evolves independently of (rt )t≥1 , this implies that the dynamic optimization problem (2) decomposes into a sequence of one-step problems.
Proposition 3 (Optimality of the myopic Bayes rule under full feedback). Assume the fullinformation feedback model described above and suppose that the latent process and context
sequence


zt , (αj,t )j∈U , xt
t≥1

do not depend on the routing decisions (rt )t≥1 . Then any policy π that, at each time t, chooses
full
b
rt⋆ ∈ arg min E Ct (j) | It−1
= arg min R
j,t|t−1 + βj







j∈Kt

j∈Kt

is globally optimal for the cumulative objective (2). In particular, the one-step Bayes rule with
squared loss (i.e., λ = 0 in the risk-adjusted score) minimizes
T
hX

E

Ct (rt )

t=1

among all admissible policies.

22

i

Proof. Fix an admissible policy π. By assumption, the joint law of the latent process and
contexts

T
zt , (αj,t )j∈U , xt
t=1

is independent of π. In the full-information model, for each t the vector of losses (ℓj,t )j∈Kt is
observed and added to the history, regardless of the choice rt . Thus the conditional distribution
of the next-step costs


Ct (j) j∈Kt = ℓj,t + βj j∈Kt
full does not depend on the past actions (r , . . . , r
given It−1
1
t−1 ).
For any such policy,

Eπ

T
hX

i

Ct (rt ) =

t=1

T
X

Eπ Ct (rt ) =




t=1

T
h 
X

i

full
E E Ct (rt ) | It−1
,

t=1

where Eπ denotes expectation under the process induced by π and the fixed environment. At
full , the inner term
each time t, conditional on It−1
full
E Ct (rt ) | It−1



is minimized by choosing



full
rt⋆ ∈ arg min E Ct (j) | It−1
.





j∈Kt

Since the sum over t is a sum of such conditional expectations, selecting rt⋆ at each time t yields
a policy that minimizes the total expected cost. Using
full
b
E Ct (j) | It−1
=R
j,t|t−1 + βj





gives the stated form.
Interpretation. In the censored (bandit) setting, the router faces an exploration–exploitation
trade-off: selecting an expert both incurs cost and determines which loss is observed, influencing
what is learned about future performance. Under full expert feedback, the learning process
is entirely decoupled from the routing decisions: the SLDS–IMM model is updated using a
complete panel of expert losses at each time step, independently of which experts were used for
prediction. As a consequence, the optimal routing strategy becomes purely cost-sensitive: it
b
suffices to select, at each time t, the expert minimizing the conditional expected cost R
j,t|t−1 + βj .
2
The variance term σ̂j,t+1|t can still be used to encode risk preferences (via a parameter λ as in
Section 4), but it no longer plays a role in driving active exploration.

8

Extension: Model-Based Horizon Planning via Expert-Driven
Context Updates

In the core formulation, the context process (xt )t≥1 is exogenous: at each time t the router
observes xt , selects an expert rt ∈ Kt , incurs cost Ct (rt ), and the environment then produces
the next context xt+1 . The SLDS/IMM machinery models the evolution of the experts’ losses,
not of the contexts themselves.
For horizon-H planning and workload forecasting, it is often useful to construct a surrogate
future in which the router’s decisions and the experts’ forecasts shape an internal notion of
“future context”. In this section we describe such a model-based extension. It is important to
stress that this extension defines an internal planning model: it does not alter the exogenous
generative assumptions of Sections ??–??.
23

8.1

Expert-Driven Context Update Map

We recall that the context space is X ⊆ Rd , and that at each time t the router observes a context
vector xt ∈ X and a feature vector
ϕt := ϕ(xt ) ∈ Rm .
Each expert j ∈ U implements a forecasting function
fj : X → Y ⊆ R,
so that fj (xt ) can be interpreted as the expert’s prediction for the next value of the underlying
time series (or some derived quantity) based on the current context.
To propagate surrogate contexts over a planning horizon, we introduce a deterministic context
update map
Ψ : X × Y → X.
Intuitively, Ψ(x, ŷ) specifies how we would construct the next context if we were to treat
the forecast ŷ as the next observation of the underlying series. For example, in a standard
autoregressive setting where the context aggregates the last p lags,
xt = yt , yt−1 , . . . , yt−p+1 ,


a natural choice is

Ψ(xt , ŷt+1 ) := ŷt+1 , yt , . . . , yt−p+2 ,


i.e., shift the lag window and append the forecast in place of the unknown yt+1 .

8.2

Surrogate Context Trajectories under a Planned Schedule

Fix a current decision time t and a planning horizon H ∈ N. For a given starting context xt and
a candidate schedule
s := (j1 , . . . , jH ) ∈ U H ,
we define a deterministic surrogate context trajectory
(s) H

x̃t+h h=0 ⊂ X
by the recursion
(s)

x̃t

:= xt ,

(25)


(s)
(s)
ŷt+h := fjh x̃t+h−1 ,
h = 1, . . . , H,


(s)
(s)
(s)
x̃t+h := Ψ x̃t+h−1 , ŷt+h ,
h = 1, . . . , H.

(26)
(27)

(s)

Thus x̃t+h is the context we would obtain at (pseudo-)time t + h if, starting from the current xt ,
we were to route to expert j1 at t + 1, treat fj1 (xt ) as the next observation when constructing
the context, then route to j2 , and so on.
For each h, we also define the associated feature vector
(s)

(s) 

ϕ̃t+h := ϕ x̃t+h ∈ Rm .
(s) H

Remark 3 (Internal planning model). The trajectory x̃t+h h=0 is a deterministic function of

(xt , s) and the expert maps (fj )j∈U . It is not the actual future context trajectory xt+h h≥1 under
the true data-generating process, but an internal surrogate used for planning and scheduling.
The SLDS/IMM model for expert losses remains exogenous and unchanged.
24

8.3

Predictive Loss along a Planned Schedule
(k)

(k)

Let (bt (k))k∈Z and (mj,t|t , Pj,t|t ) denote the IMM posterior at time t:
(k)

bt (k) = P(zt = k | It ),

(k) 

αj,t | {zt = k, It } ≈ N mj,t|t , Pj,t|t .

For each h ≥ 1, we can propagate the SLDS dynamics forward without incorporating future
observations to obtain the h-step-ahead predicted reliability states and regime probabilities:
(k)

(k)

for each j ∈ U, k ∈ Z,

mj,t+h|t , Pj,t+h|t

bt+h|t (k) := P(zt+h = k | It ),
by iterating the IMM time-update recursions h times. These quantities depend only on the
dynamics (Π, Ak,j , Qk,j ) and the current belief at t; they do not depend on the planned schedule
s.
Given a schedule s and its surrogate contexts (27), the predictive distribution of the loss
incurred at pseudo-time t + h by selecting expert jh is approximated by
M
X



P ℓjh ,t+h | It , s ≈

(k,s)

(k,s)

bt+h|t (k) N µjh ,t+h|t , Sjh ,t+h|t ,


k=1

where, under regime k,
(k,s)

(s) ⊤

(k)

(k,s)

(s) ⊤

(k)

µjh ,t+h|t := ϕ̃t+h mjh ,t+h|t ,
(s)

Sjh ,t+h|t := ϕ̃t+h Pjh ,t+h|t ϕ̃t+h + Rk,jh .
The corresponding predictive mean and variance of ℓjh ,t+h under schedule s are
b (s)
R

:= E ℓjh ,t+h | It , s =
j ,t+h|t




M
X

h

(k,s)

bt+h|t (k) µjh ,t+h|t ,

k=1

2,(s)
σ̂jh ,t+h|t := Var ℓjh ,t+h | It , s
M
M
X
X
2
(k,s)
(k,s)
b (s)
=
bt+h|t (k) Sjh ,t+h|t +
bt+h|t (k) µjh ,t+h|t − R
jh ,t+h|t ,
k=1
k=1

by the same variance decomposition as in Proposition ??.
We define the risk-adjusted one-step planning cost for the h-th element of the schedule s by
r
(s)
(s)
2,(s)
e
b
Ct+h := Rjh ,t+h|t + βjh + λ σ̂jh ,t+h|t ,

(28)

where λ ∈ R is the same risk parameter as in the one-step selection rule.

8.4

Horizon-H Planning Objective and Scheduling Forecast

For a fixed schedule s = (j1 , . . . , jH ) we can aggregate the risk-adjusted planning costs over the
horizon, e.g. without discounting,
Jplan (s) :=

H
X

(s)

Cet+h ,

h=1

or with a discount factor γ ∈ (0, 1],
γ
Jplan
(s) :=

H
X
h=1

25

(s)

γ h−1 Cet+h .

(29)

In principle, one could define an open-loop horizon-H planning problem by minimizing (29)
over s ∈ U H . This is combinatorial in H and |U| and primarily of conceptual interest.
A more practical use of the surrogate model is scheduling forecast: instead of optimizing over
schedules, we fix a router policy (typically the myopic one-step rule of Section ??) and simulate
its behaviour on the surrogate environment defined by (27). Concretely:
(s)

• At simulation step h = 1, starting from x̃t
candidate surrogate contexts
(j)

= xt and the belief at time t, we construct

x̃t+1 := Ψ xt , fj (xt ) ,


j ∈ Kt+1 ,
(j)

2,(j)

b
compute the corresponding predictive means and variances (R
j,t+1|t , σ̂j,t+1|t ), and select

r̃t+1 ∈ arg min

j∈Kt+1

(r̃

n

b (j)
R

r

2,(j)

o

j,t+1|t + βj + λ σ̂j,t+1|t .

)

t+1
We then set x̃t+1 = x̃t+1
.

• At simulation step h = 2, we repeat the construction starting from x̃t+1 , obtain r̃t+2 and
x̃t+2 , and so on up to horizon H.
This yields a simulated sequence (r̃t+1 , . . . , r̃t+H ) of expert indices which approximates
the future routing decisions that would be induced by the one-step policy under the internal,
expert-driven context dynamics. Repeating this procedure under different modelling choices for
Ψ or different initial contexts xt provides:
• Workload forecasts for each expert over the next H steps (e.g., expected number of routed
queries), useful for capacity planning and SLA management;
• A way to compare alternative routing policies (different λ, different cost vectors (βj )) in
terms of their long-run impact on expert usage, without modifying the core SLDS-L2D
problem.
Remark 4 (Interpretation and limitations). The horizon-H planning framework described in
(s)
this section is explicitly model-based. The surrogate contexts x̃t+h and the induced routing
sequence (r̃t+1 , . . . , r̃t+H ) live in an internal closed-loop model where expert forecasts are treated
as future observations via the update map Ψ. They need not coincide with the true future
contexts and routing decisions under the exogenous data-generating process. As such, this
extension is best viewed as a principled scheduling and capacity-planning heuristic built on top
of the SLDS/IMM L2D framework, rather than as a modification of the underlying probabilistic
model.

9

Conclusion

We presented a theoretically grounded framework for Time-Series L2D. By leveraging Switching
State-Space models, we explicitly account for non-stationarity and cost. The formulation supports
dynamic expert sets and handles partial feedback via rigorous Bayesian filtering, with extensions
for diversity-aware ensemble selection.

26

